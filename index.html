<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-09T00:00:00Z">2024-10-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">140</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge
  Conflicts for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do better language models have crisper vision? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Initialization to Rule them All: Fine-tuning via Explained Variance
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned on a downstream task for a specific application. The most successful
and most commonly used fine-tuning method is to update the pre-trained weights
via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usually initialized at random with a uniform rank distribution across model
weights. Recent works focus on weight-driven initialization or learning of
adaptive ranks during training. Both approaches have only been investigated in
isolation, resulting in slow convergence or a uniform rank distribution, in
turn leading to sub-optimal performance. We propose to enhance LoRA by
initializing the new weights in a data-driven manner by computing singular
value decomposition on minibatches of activation vectors. Then, we initialize
the LoRA matrices with the obtained right-singular vectors and re-distribute
ranks among all weight matrices to explain the maximal amount of variance and
continue the standard LoRA fine-tuning procedure. This results in our new
method Explained Variance Adaptation (EVA). We apply EVA to a variety of
fine-tuning tasks ranging from language generation and understanding to image
classification and reinforcement learning. EVA exhibits faster convergence than
competitors and attains the highest average score across a multitude of tasks
per domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + references and appendix, code available at
  https://github.com/ml-jku/EVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering Cross-Modal Alignment in Large Vision-Language Models with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sylber: Syllabic Embedding Representation of Speech from Raw Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllables are compositional units of spoken language that play a crucial role
in human speech perception and production. However, current neural speech
representations lack structure, resulting in dense token sequences that are
costly to process. To bridge this gap, we propose a new model, Sylber, that
produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised model that regresses features on
syllabic segments distilled from a teacher model which is an exponential moving
average of the model in training. This results in a highly structured
representation of speech features, offering three key benefits: 1) a fast,
linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization
with an average of 4.27 tokens per second, and 3) syllabic units better suited
for lexical and syntactic understanding. We also train token-to-speech
generative models with our syllabic units and show that fully intelligible
speech can be reconstructed from these tokens. Lastly, we observe that
categorical perception, a linguistic phenomenon of speech perception, emerges
naturally in our model, making the embedding space more categorical and sparse
than previous self-supervised learning approaches. Together, we present a novel
self-supervised approach for representing speech as syllables, with significant
potential for efficient speech tokenization and spoken language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to evaluate Large Language Models (LLMs) for embodied decision making.
While a significant body of work has been leveraging LLMs for decision making
in embodied environments, we still lack a systematic understanding of their
performance because they are usually applied in different domains, for
different purposes, and built based on different inputs and outputs.
Furthermore, existing evaluations tend to rely solely on a final success rate,
making it difficult to pinpoint what ability is missing in LLMs and where the
problem lies, which in turn blocks embodied agents from leveraging LLMs
effectively and selectively. To address these limitations, we propose a
generalized interface (Embodied Agent Interface) that supports the
formalization of various types of tasks and input-output specifications of
LLM-based modules. Specifically, it allows us to unify 1) a broad set of
embodied decision-making tasks involving both state and temporally extended
goals, 2) four commonly-used LLM-based modules for decision making: goal
interpretation, subgoal decomposition, action sequencing, and transition
modeling, and 3) a collection of fine-grained metrics which break down
evaluation into various types of errors, such as hallucination errors,
affordance errors, various types of planning errors, etc. Overall, our
benchmark offers a comprehensive assessment of LLMs' performance for different
subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI
systems, and providing insights for effective and selective use of LLMs in
embodied decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2024 in the Datasets and
  Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplicity Prevails: Rethinking Negative Preference Optimization for LLM
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the problem of large language model (LLM)
unlearning, aiming to remove unwanted data influences and associated model
capabilities (e.g., copyrighted data or harmful content generation) while
preserving essential model utilities, without the need for retraining from
scratch. Despite the growing need for LLM unlearning, a principled optimization
framework remains lacking. To this end, we revisit the state-of-the-art
approach, negative preference optimization (NPO), and identify the issue of
reference model bias, which could undermine NPO's effectiveness, particularly
when unlearning forget data of varying difficulty. Given that, we propose a
simple yet effective unlearning optimization framework, called SimNPO, showing
that 'simplicity' in removing the reliance on a reference model (through the
lens of simple preference optimization) benefits unlearning. We also provide
deeper insights into SimNPO's advantages, supported by analysis using mixtures
of Markov chains. Furthermore, we present extensive experiments validating
SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU
and MUSE, and robustness against relearning attacks. Codes are available at
https://github.com/OPTML-Group/Unlearn-Simple.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructG2I: Synthesizing Images from Multimodal Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taking a turn for the better: Conversation redirection throughout the
  course of mental-health therapy <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivian Nguyen, Sang Min Jung, Lillian Lee, Thomas D. Hull, Cristian Danescu-Niculescu-Mizil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental-health therapy involves a complex conversation flow in which patients
and therapists continuously negotiate what should be talked about next. For
example, therapists might try to shift the conversation's direction to keep the
therapeutic process on track and avoid stagnation, or patients might push the
discussion towards issues they want to focus on.
  How do such patient and therapist redirections relate to the development and
quality of their relationship? To answer this question, we introduce a
probabilistic measure of the extent to which a certain utterance immediately
redirects the flow of the conversation, accounting for both the intention and
the actual realization of such a change. We apply this new measure to
characterize the development of patient-therapist relationships over multiple
sessions in a very large, widely-used online therapy platform. Our analysis
reveals that (1) patient control of the conversation's direction generally
increases relative to that of the therapist as their relationship progresses;
and (2) patients who have less control in the first few sessions are
significantly more likely to eventually express dissatisfaction with their
therapist and terminate the relationship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of EMNLP (Findings) 2024. Code available
  at https://convokit.cornell.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stuffed Mamba: State Collapse and State Capacity of RNN-Based
  Long-Context Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One essential advantage of recurrent neural networks (RNNs) over
transformer-based language models is their linear computational complexity
concerning the sequence length, which makes them much faster in handling long
sequences during inference. However, most publicly available RNNs (e.g., Mamba
and RWKV) are trained on sequences with less than 10K tokens, and their
effectiveness in longer contexts remains largely unsatisfying so far. In this
paper, we study the cause of the inability to process long context for RNNs and
suggest critical mitigations. We examine two practical concerns when applying
state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to
inputs longer than the training length and (2) the upper bound of memory
capacity. Addressing the first concern, we first investigate *state collapse*
(SC), a phenomenon that causes severe performance degradation on sequence
lengths not encountered during training. With controlled experiments, we
attribute this to overfitting due to the recurrent state being
overparameterized for the training length. For the second concern, we train a
series of Mamba-2 models on long documents to empirically estimate the
recurrent state capacity in language modeling and passkey retrieval. Then,
three SC mitigation methods are proposed to improve Mamba-2's length
generalizability, allowing the model to process more than 1M tokens without SC.
We also find that the recurrent state capacity in passkey retrieval scales
exponentially to the state size, and we empirically train a Mamba-2 370M with
near-perfect passkey retrieval accuracy on 256K context length. This suggests a
promising future for RNN-based long-context modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mental Disorders Detection in the Era of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Readiness of Prominent Small Language Models for the
  Democratization of Financial Literacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of small language models (SLMs), herein defined as models with less
than three billion parameters, is increasing across various domains and
applications. Due to their ability to run on more accessible hardware and
preserve user privacy, SLMs possess the potential to democratize access to
language models for individuals of different socioeconomic status and with
different privacy preferences. This study assesses several state-of-the-art
SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama
project) for use in the financial domain to support the development of
financial literacy LMs. Democratizing access to quality financial information
for those who are financially under educated is greatly needed in society,
particularly as new financial markets and products emerge and participation in
financial markets increases due to ease of access. We are the first to examine
the use of open-source SLMs to democratize access to financial question
answering capabilities for individuals and students. To this end, we provide an
analysis of the memory usage, inference time, similarity comparisons to
ground-truth answers, and output readability of prominent SLMs to determine
which models are most accessible and capable of supporting access to financial
information. We analyze zero-shot and few-shot learning variants of the models.
The results suggest that some off-the-shelf SLMs merit further exploration and
fine-tuning to prepare them for individual use, while others may have limits to
their democratization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Want to Break Free! Anti-Social Behavior and Persuasion Ability of
  LLMs in Multi-Agent Settings with Social Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Multi-Hop Reasoning Potential in <span class="highlight-title">Large Language Model</span>s
  through Repetition of Misordered Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-hop reasoning, which requires multi-step reasoning based on the
supporting documents within a given context, remains challenging for large
language models (LLMs). LLMs often struggle to filter out irrelevant documents
within the context, and their performance is sensitive to the position of
supporting documents within that context. In this paper, we identify an
additional challenge: LLMs' performance is also sensitive to the order in which
the supporting documents are presented. We refer to this as the misordered
context problem. To address this issue, we propose a simple yet effective
method called context repetition (CoRe), which involves prompting the model by
repeatedly presenting the context to ensure the supporting documents are
presented in the optimal order for the model. Using CoRe, we improve the F1
score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p
on a synthetic task. Additionally, CoRe helps mitigate the well-known
"lost-in-the-middle" problem in LLMs and can be effectively combined with
retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLE-bench: Evaluating Machine Learning Agents on Machine Learning
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Plus 17 pages appendix. 8 figures. Equal contribution by
  first seven authors. Authors randomized. Work by Neil Chowdhury done while at
  OpenAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Approach for Auto Ge<span class="highlight-title">ner</span>ation of Labeling Functions for Software
  Engineering Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineering (SE) chatbots are increasingly gaining attention for
their role in enhancing development processes. At the core of chatbots are the
Natural Language Understanding platforms (NLUs), which enable them to
comprehend and respond to user queries. Before deploying NLUs, there is a need
to train them with labeled data. However, acquiring such labeled data for SE
chatbots is challenging due to the scarcity of high-quality datasets. This
challenge arises because training SE chatbots requires specialized vocabulary
and phrases not found in typical language datasets. Consequently, chatbot
developers often resort to manually annotating user queries to gather the data
necessary for training effective chatbots, a process that is both
time-consuming and resource-intensive. Previous studies propose approaches to
support chatbot practitioners in annotating users' posed queries. However,
these approaches require human intervention to generate rules, called labeling
functions (LFs), that identify and categorize user queries based on specific
patterns in the data. To address this issue, we propose an approach to
automatically generate LFs by extracting patterns from labeled user queries. We
evaluate the effectiveness of our approach by applying it to the queries of
four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)
and measure the performance improvement gained from training the NLU on the
queries labeled by the generated LFs. We find that the generated LFs
effectively label data with AUC scores of up to 85.3%, and NLU's performance
improvement of up to 27.2% across the studied datasets. Furthermore, our
results show that the number of LFs used to generate LFs affects the labeling
performance. We believe that our approach can save time and resources in
labeling users' queries, allowing practitioners to focus on core chatbot
functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Software Engineering for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stanceformer: Target-Aware <span class="highlight-title">Transformer</span> for Stance Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Garg, Cornelia Caragea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Stance Detection involves discerning the stance expressed in a
text towards a specific subject or target. Prior works have relied on existing
transformer models that lack the capability to prioritize targets effectively.
Consequently, these models yield similar performance regardless of whether we
utilize or disregard target information, undermining the task's significance.
To address this challenge, we introduce Stanceformer, a target-aware
transformer model that incorporates enhanced attention towards the targets
during both training and inference. Specifically, we design a \textit{Target
Awareness} matrix that increases the self-attention scores assigned to the
targets. We demonstrate the efficacy of the Stanceformer with various
BERT-based models, including state-of-the-art models and Large Language Models
(LLMs), and evaluate its performance across three stance detection datasets,
alongside a zero-shot dataset. Our approach Stanceformer not only provides
superior performance but also generalizes even to other domains, such as
Aspect-based Sentiment Analysis. We make the code publicly
available.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 14 tables including Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOOSE-Chem: <span class="highlight-title">Large Language Model</span>s for Rediscovering Unseen Chemistry
  Scientific Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Benchmark are available at
  https://github.com/ZonglinY/MOOSE-Chem.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixtral 12B 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.
Pixtral-12B is trained to understand both natural images and documents,
achieving leading performance on various multimodal benchmarks, surpassing a
number of larger models. Unlike many open-source models, Pixtral is also a
cutting-edge text model for its size, and does not compromise on natural
language performance to excel in multimodal tasks. Pixtral uses a new vision
encoder trained from scratch, which allows it to ingest images at their natural
resolution and aspect ratio. This gives users flexibility on the number of
tokens used to process an image. Pixtral is also able to process any number of
images in its long context window of 128K tokens. Pixtral 12B substanially
outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B).
It also outperforms much larger open models like Llama-3.2 90B while being 7x
smaller. We further contribute an open-source benchmark, MM-MT-Bench, for
evaluating vision-language models in practical scenarios, and provide detailed
analysis and code for standardized evaluation protocols for multimodal LLMs.
Pixtral-12B is released under Apache 2.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReIFE: Re-evaluating Instruction-Following Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic evaluation of instruction following typically involves using
large language models (LLMs) to assess response quality. However, there is a
lack of comprehensive evaluation of these LLM-based evaluators across two
dimensions: the base LLMs and the evaluation protocols. Therefore, we present a
thorough meta-evaluation of instruction following, including 25 base LLMs and
15 recently proposed evaluation protocols, on 4 human-annotated datasets,
assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows
us to identify the best-performing base LLMs and evaluation protocols with a
high degree of robustness. Moreover, our large-scale evaluation reveals: (1)
Base LLM performance ranking remains largely consistent across evaluation
protocols, with less capable LLMs showing greater improvement from protocol
enhancements; (2) Robust evaluation of evaluation protocols requires many base
LLMs with varying capability levels, as protocol effectiveness can depend on
the base LLM used; (3) Evaluation results on different datasets are not always
consistent, so a rigorous evaluation requires multiple datasets with
distinctive features. We release our meta-evaluation suite ReIFE, which
provides the codebase and evaluation result collection for more than 500
LLM-evaluator configurations, to support future research in
instruction-following evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result
  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Selection via Optimal Control for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the selection of high-quality pre-training data from
massive corpora to enhance LMs' capabilities for downstream usage. We formulate
data selection as a generalized Optimal Control problem, which can be solved
theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of
necessary conditions that characterize the relationship between optimal data
selection and LM training dynamics. Based on these theoretical results, we
introduce PMP-based Data Selection (PDS), a framework that approximates optimal
data selection by solving the PMP conditions. In our experiments, we adopt PDS
to select data from CommmonCrawl and show that the PDS-selected corpus
accelerates the learning of LMs and constantly boosts their performance on a
wide range of downstream tasks across various model sizes. Moreover, the
benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by
the extrapolation of the test loss curves according to the Scaling Laws. PDS
also improves data utilization when the pre-training data is limited, by
reducing the data demand by 1.8 times, which mitigates the quick exhaustion of
available web-crawled corpora. Our code, data, and model checkpoints can be
found in https://github.com/microsoft/LMOps/tree/main/data_selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Language Mismatch and Repetition Issues in LLM-based
  Machine Translation via Model Editing <span class="chip">EMNLP'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently revolutionized the NLP field,
while they still fall short in some specific down-stream tasks. In the work, we
focus on utilizing LLMs to perform machine translation, where we observe that
two patterns of errors frequently occur and drastically affect the translation
quality: language mismatch and repetition. The work sets out to explore the
potential for mitigating these two issues by leveraging model editing methods,
e.g., by locating Feed-Forward Network (FFN) neurons or something that are
responsible for the errors and deactivating them in the inference time. We find
that directly applying such methods either limited effect on the targeted
errors or has significant negative side-effect on the general translation
quality, indicating that the located components may also be crucial for
ensuring machine translation with LLMs on the rails. To this end, we propose to
refine the located components by fetching the intersection of the locating
results under different language settings, filtering out the aforementioned
information that is irrelevant to targeted errors. The experiment results
empirically demonstrate that our methods can effectively reduce the language
mismatch and repetition ratios and meanwhile enhance or keep the general
translation quality in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, EMNLP'2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robots in the Middle: Evaluating LLMs in Dispute Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinzhe Tan, Hannes Westermann, Nikhil Reddy Pottanigari, Jaromír Šavelka, Sébastien Meeùs, Mia Godet, Karim Benyekhlef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mediation is a dispute resolution method featuring a neutral third-party
(mediator) who intervenes to help the individuals resolve their dispute. In
this paper, we investigate to which extent large language models (LLMs) are
able to act as mediators. We investigate whether LLMs are able to analyze
dispute conversations, select suitable intervention types, and generate
appropriate intervention messages. Using a novel, manually created dataset of
50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human
annotators across several key metrics. Overall, the LLMs showed strong
performance, even outperforming our human annotators across dimensions.
Specifically, in 62% of the cases, the LLMs chose intervention types that were
rated as better than or equivalent to those chosen by humans. Moreover, in 84%
of the cases, the intervention messages generated by the LLMs were rated as
better than or equal to the intervention messages written by humans. LLMs
likewise performed favourably on metrics such as impartiality, understanding
and contextualization. Our results demonstrate the potential of integrating AI
in online dispute resolution (ODR) platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PositionID: LLMs can Control Lengths, Copy and Paste with Explicit
  Positional Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate impressive capabilities across
various domains, including role-playing, creative writing, mathematical
reasoning, and coding. Despite these advancements, LLMs still encounter
challenges with length control, frequently failing to adhere to specific length
constraints due to their token-level operations and insufficient training on
data with strict length limitations. We identify this issue as stemming from a
lack of positional awareness and propose novel approaches--PositionID Prompting
and PositionID Fine-Tuning--to address it. These methods enhance the model's
ability to continuously monitor and manage text length during generation.
Additionally, we introduce PositionID CP Prompting to enable LLMs to perform
copy and paste operations accurately. Furthermore, we develop two benchmarks
for evaluating length control and copy-paste abilities. Our experiments
demonstrate that our methods significantly improve the model's adherence to
length constraints and copy-paste accuracy without compromising response
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages. CP-Bench and LenCtrl-Bench are available in
  https://huggingface.co/datasets/ZenMoore/CP-Bench and
  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clean Evaluations on Contaminated Visual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Lu, Shujie Miao, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to evaluate large language models (LLMs) cleanly has been established as
an important research era to genuinely report the performance of possibly
contaminated LLMs. Yet, how to cleanly evaluate the visual language models
(VLMs) is an under-studied problem. We propose a novel approach to achieve such
goals through data augmentation methods on the visual input information. We
then craft a new visual clean evaluation benchmark with thousands of data
instances. Through extensive experiments, we found that the traditional visual
data augmentation methods are useful, but they are at risk of being used as a
part of the training data as a workaround. We further propose using BGR
augmentation to switch the colour channel of the visual information. We found
that it is a simple yet effective method for reducing the effect of data
contamination and fortunately, it is also harmful to be used as a data
augmentation method during training. It means that it is hard to integrate such
data augmentation into training by malicious trainers and it could be a
promising technique to cleanly evaluate visual LLMs. Our code, data, and model
weights will be released upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation
  Models Without Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists play a crucial role by translating medical images into medical
reports. However, the field faces staffing shortages and increasing workloads.
While automated approaches using vision-language models (VLMs) show promise as
assistants, they require exceptionally high accuracy. Most current VLMs in
radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the
general domain, additional preference fine-tuning has become standard practice.
The challenge in radiology lies in the prohibitive cost of obtaining
radiologist feedback. We propose a scalable automated preference alignment
technique for VLMs in radiology, focusing on chest X-ray (CXR) report
generation. Our method leverages publicly available datasets with an
LLM-as-a-Judge mechanism, eliminating the need for additional expert
radiologist feedback. We evaluate and benchmark five direct alignment
algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN
scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in
an average across six metrics (domain specific and general), compared to the
SFT baseline. We study reward overoptimization via length exploitation, with
reports lengthening by up to 3.2x. To assess a potential alignment tax, we
benchmark on six additional diverse tasks, finding no significant degradations.
A reader study involving four board-certified radiologists indicates win rates
of up to 0.62 over the SFT baseline, while significantly penalizing verbosity.
Our analysis provides actionable insights for the development of VLMs in
high-stakes fields like radiology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based
  Outline-guided Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The patent domain is gaining attention in natural language processing
research, offering practical applications in streamlining the patenting process
and providing challenging benchmarks for large language models (LLMs). However,
the generation of the description sections of patents, which constitute more
than 90% of the patent document, has not been studied to date. We address this
gap by introducing the task of outline-guided paper-to-patent generation, where
an academic paper provides the technical specification of the invention and an
outline conveys the desired patent structure. We present PAP2PAT, a new
challenging benchmark of 1.8k patent-paper pairs with document outlines,
collected using heuristics that reflect typical research lab practices. Our
experiments with current open-weight LLMs and outline-guided chunk-based
generation show that they can effectively use information from the paper but
struggle with repetitions, likely due to the inherent repetitiveness of patent
language. We release our data and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CursorCore: Assist Programming through Aligning Anything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been successfully applied to programming
assistance tasks, such as code completion, code insertion, and instructional
code editing. However, these applications remain insufficiently automated and
struggle to effectively integrate various types of information during the
programming process, including coding history, current code, and user
instructions. In this work, we propose a new conversational framework that
comprehensively integrates these information sources, collect data to train our
models and evaluate their performance. Firstly, to thoroughly evaluate how well
models align with different types of information and the quality of their
outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to
comprehensively assess the performance of models in programming assistance
tasks. Then, for data collection, we develop a data generation pipeline,
Programming-Instruct, which synthesizes training data from diverse sources,
such as GitHub and online judge platforms. This pipeline can automatically
generate various types of messages throughout the programming process. Finally,
using this pipeline, we generate 219K samples, fine-tune multiple models, and
develop the CursorCore series. We show that CursorCore outperforms other models
of comparable size. This framework unifies applications such as inline chat and
automated editing, contributes to the advancement of coding assistants. Code,
models and data are freely available at
https://github.com/TechxGenus/CursorCore.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personal Intelligence System UniLM: Hybrid On-Device Small Language
  Model and Server-Based <span class="highlight-title">Large Language Model</span> for Malay Nusantara 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azree Nazri, Olalekan Agbolade, Faisal Aziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contexts with limited computational and data resources, high-resource
language models often prove inadequate, particularly when addressing the
specific needs of Malay languages. This paper introduces a Personal
Intelligence System designed to efficiently integrate both on-device and
server-based models. The system incorporates SLiM-34M for on-device processing,
optimized for low memory and power usage, and MANYAK-1.3B for server-based
tasks, allowing for scalable, high-performance language processing. The models
achieve significant results across various tasks, such as machine translation,
question-answering, and translate IndoMMLU. Particularly noteworthy is
SLiM-34M's ability to achieve a high improvement in accuracy compared to other
LLMs while using 2 times fewer pre-training tokens. This work challenges the
prevailing assumption that large-scale computational resources are necessary to
build effective language models, contributing to the development of
resource-efficient models for the Malay language with the unique orchestration
between SLiM-34M and MANYAK-1.3B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Factor Level Preferences to Improve Human-Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhyun Oh, Eunsu Kim, Jiseon Kim, Wenda Xu, Inha Cha, William Yang Wang, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in Large Language Model (LLM) alignment, understanding
the reasons behind LLM preferences remains crucial for bridging the gap between
desired and actual behavior. LLMs often exhibit biases or tendencies that
diverge from human preferences, such as favoring certain writing styles or
producing overly verbose outputs. However, current methods for evaluating
preference alignment often lack explainability, relying on coarse-grained
comparisons. To address this, we introduce PROFILE (PRObing Factors of
InfLuence for Explainability), a novel framework that uncovers and quantifies
the influence of specific factors driving preferences. PROFILE's factor level
analysis explains the 'why' behind human-model alignment and misalignment,
offering insights into the direction of model improvement. We apply PROFILE to
analyze human and LLM preferences across three tasks: summarization, helpful
response generation, and document-based question-answering. Our factor level
analysis reveals a substantial discrepancy between human and LLM preferences in
generation tasks, whereas LLMs show strong alignment with human preferences in
evaluation tasks. We demonstrate how leveraging factor level insights,
including addressing misaligned factors or exploiting the generation-evaluation
gap, can improve alignment with human preferences. This work underscores the
importance of explainable preference analysis and highlights PROFILE's
potential to provide valuable training signals, driving further improvements in
human-model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Boosting <span class="highlight-title">Large Language Model</span>s with Synthetic Preference Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through alignment with human preferences, Large Language Models (LLMs) have
advanced significantly in generating honest, harmless, and helpful responses.
However, collecting high-quality preference data is a resource-intensive and
creativity-demanding process, especially for the continual improvement of LLMs.
We introduce SynPO, a self-boosting paradigm that leverages synthetic
preference data for model alignment. SynPO employs an iterative mechanism
wherein a self-prompt generator creates diverse prompts, and a response
improver refines model responses progressively. This approach trains LLMs to
autonomously learn the generative rewards for their own outputs and eliminates
the need for large-scale annotation of prompts and human preferences. After
four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements
in instruction-following abilities, achieving over 22.1% win rate improvements
on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general
performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score
increase on the well-recognized Open LLM leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent
  Approach <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures. Submitted ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on
  Relatively Free Word Ordered and Morphologically Rich Low Resource Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural dependency parsing has achieved remarkable performance for low
resource morphologically rich languages. It has also been well-studied that
morphologically rich languages exhibit relatively free word order. This prompts
a fundamental investigation: Is there a way to enhance dependency parsing
performance, making the model robust to word order variations utilizing the
relatively free word order nature of morphologically rich languages? In this
work, we examine the robustness of graph-based parsing architectures on 7
relatively free word order languages. We focus on scrutinizing essential
modifications such as data augmentation and the removal of position encoding
required to adapt these architectures accordingly. To this end, we propose a
contrastive self-supervised learning method to make the model robust to word
order variations. Furthermore, our proposed modification demonstrates a
substantial average gain of 3.03/2.95 points in 7 relatively free word order
languages, as measured by the UAS/LAS Score metric when compared to the best
performing baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by first employing a compact model to draft multiple tokens
efficiently and then using the target LLM to verify them in parallel. While
this technique has achieved notable speedups, most existing approaches
necessitate either additional parameters or extensive training to construct
effective draft models, thereby restricting their applicability across
different LLMs and tasks. To address this limitation, we explore a novel
plug-and-play SD solution with layer-skipping, which skips intermediate layers
of the target LLM as the compact draft model. Our analysis reveals that LLMs
exhibit great potential for self-acceleration through layer sparsity and the
task-specific nature of this sparsity. Building on these insights, we introduce
SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively
selects intermediate layers of LLMs to skip during inference. SWIFT does not
require auxiliary models or additional training, making it a plug-and-play
solution for accelerating LLM inference across diverse input data streams. Our
extensive experiments across a wide range of models and downstream tasks
demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving
the original distribution of the generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilize the Flow before Stepping into the Same River Twice: Certainty
  Represented Knowledge Flow for Refusal-Aware Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)
to refuse to answer unknown questions. By modifying responses of unknown
questions in the training data to refusal responses such as "I don't know",
RAIT enhances the reliability of LLMs and reduces their hallucination.
Generally, RAIT modifies training samples based on the correctness of the
initial LLM's response. However, this crude approach can cause LLMs to
excessively refuse answering questions they could have correctly answered, the
problem we call over-refusal. In this paper, we explore two primary causes of
over-refusal: Static conflict emerges when the RAIT data is constructed solely
on correctness criteria, causing similar samples in the LLM's feature space to
be assigned different labels (original vs. modified "I don't know"). Dynamic
conflict occurs due to the changes of LLM's knowledge state during fine-tuning,
which transforms previous unknown questions into knowns, while the training
data, which is constructed based on the initial LLM, remains unchanged. These
conflicts cause the trained LLM to misclassify known questions as unknown,
resulting in over-refusal. To address this issue, we introduce Certainty
Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).
CRaFT centers on two main contributions: First, we additionally incorporate
response certainty to selectively filter and modify data, reducing static
conflicts. Second, we implement preliminary rehearsal training to characterize
changes in the LLM's knowledge state, which helps mitigate dynamic conflicts
during the fine-tuning process. We conducted extensive experiments on
open-ended question answering and multiple-choice question task. Experiment
results show that CRaFT can improve LLM's overall performance during the RAIT
process. Source code and training data will be released at Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding
  author: Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ge<span class="highlight-title">ner</span>ative Model for Less-Resourced Language with 1 billion parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domen Vreš, Martin Božič, Aljaž Potočnik, Tomaž Martinčič, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are a basic infrastructure for modern natural
language processing. Many commercial and open-source LLMs exist for English,
e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on
mostly English texts, their fluency and knowledge of low-resource languages and
societies are superficial. We present the development of large generative
language models for a less-resourced language. GaMS 1B - Generative Model for
Slovene with 1 billion parameters was created by continuing pretraining of the
existing English OPT model. We developed a new tokenizer adapted to Slovene,
Croatian, and English languages and used embedding initialization methods FOCUS
and WECHSEL to transfer the embeddings from the English OPT model. We evaluate
our models on several classification datasets from the Slovene suite of
benchmarks and generative sentence simplification task SENTA. We only used a
few-shot in-context learning of our models, which are not yet
instruction-tuned. For classification tasks, in this mode, the generative
models lag behind the existing Slovene BERT-type models fine-tuned for specific
tasks. On a sentence simplification task, the GaMS models achieve comparable or
better performance than the GPT-3.5-Turbo model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FltLM: An Intergrated Long-Context <span class="highlight-title">Large Language Model</span> for Effective
  Context Filtering and Understanding <span class="chip">ECAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Deng, Zhengyang Shen, Boyang Wang, Lixin Su, Suqi Cheng, Ying Nie, Junfeng Wang, Dawei Yin, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th European Conference on Artificial Intelligence
  (ECAI-2024), this is the full version of the paper including technical
  appendices. This final version features enhanced formatting and corrections
  to errors present in other online versions. We regret any inconvenience this
  may have caused our readers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Fine-tuning and Conversion of <span class="highlight-title">Pretrain</span>ed Speech and Language
  Models towards Linear Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mutian He, Philip N. Garner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architectures such as Linformer and Mamba have recently emerged as
competitive linear time replacements for transformers. However, corresponding
large pretrained models are often unavailable, especially in non-text domains.
To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)
approach that jointly converts a transformer model to a linear time substitute
and fine-tunes it to a target task. We also compare several means to guide the
fine-tuning to optimally retain the desired inference capability from the
original model. The methods differ in their use of the target model and the
trajectory of the parameters. In a series of empirical studies on language
processing, language modeling, and speech processing, we show that CALD can
effectively recover the result of the original model, and that the guiding
strategy contributes to the result. Some reasons for the variation are
suggested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MentalArena: Self-play Training of Language Models for Diagnosis and
  Treatment of Mental Health Disorders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health disorders are one of the most serious diseases in the world.
Most people with such a disease lack access to adequate care, which highlights
the importance of training models for the diagnosis and treatment of mental
health disorders. However, in the mental health domain, privacy concerns limit
the accessibility of personalized treatment data, making it challenging to
build powerful models. In this paper, we introduce MentalArena, a self-play
framework to train language models by generating domain-specific personalized
data, where we obtain a better model capable of making a personalized diagnosis
and treatment (as a therapist) and providing information (as a patient). To
accurately model human-like mental health patients, we devise Symptom Encoder,
which simulates a real patient from both cognition and behavior perspectives.
To address intent bias during patient-therapist interactions, we propose
Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and
dynamically manage the dialogue between patient and therapist according to the
identified deviations. We evaluated MentalArena against 6 benchmarks, including
biomedicalQA and mental health tasks, compared to 6 advanced models. Our
models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform
their counterparts, including GPT-4o. We hope that our work can inspire future
research on personalized care. Code is available in
https://github.com/Scarelette/MentalArena/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zeng, Yuying Shang, Yutao Zhu, Jiawei Chen, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated immense utility across various
industries. However, as LLMs advance, the risk of harmful outputs increases due
to incorrect or malicious instruction prompts. While current methods
effectively address jailbreak risks, they share common limitations: 1) Judging
harmful responses from the prefill-level lacks utilization of the model's
decoding outputs, leading to relatively lower effectiveness and robustness. 2)
Rejecting potentially harmful responses based on a single evaluation can
significantly impair the model's helpfulness.This paper examines the LLMs'
capability to recognize harmful outputs, revealing and quantifying their
proficiency in assessing the danger of previous tokens. Motivated by pilot
experiment results, we design a robust defense mechanism at the decoding level.
Our novel decoder-oriented, step-by-step defense architecture corrects harmful
queries directly rather than rejecting them outright. We introduce speculative
decoding to enhance usability and facilitate deployment to boost secure
decoding speed. Extensive experiments demonstrate that our approach improves
model security without compromising reasoning speed. Notably, our method
leverages the model's ability to discern hazardous information, maintaining its
helpfulness compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seg2Act: Global Context-aware Action Ge<span class="highlight-title">ner</span>ation for Document Logical
  Structuring <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Li, Shaojie He, Meng Liao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Yanxiong Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document logical structuring aims to extract the underlying hierarchical
structure of documents, which is crucial for document intelligence. Traditional
approaches often fall short in handling the complexity and the variability of
lengthy documents. To address these issues, we introduce Seg2Act, an
end-to-end, generation-based method for document logical structuring,
revisiting logical structure extraction as an action generation task.
Specifically, given the text segments of a document, Seg2Act iteratively
generates the action sequence via a global context-aware generative model, and
simultaneously updates its global context and current logical structure based
on the generated actions. Experiments on ChCatExt and HierDoc datasets
demonstrate the superior performance of Seg2Act in both supervised and transfer
learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixels to Tokens: Revisiting Object Hallucinations in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in large vision-language models (LVLMs) are a significant
challenge, i.e., generating objects that are not presented in the visual input,
which impairs their reliability. Recent studies often attribute hallucinations
to a lack of understanding of visual input, yet ignore a more fundamental
issue: the model's inability to effectively extract or decouple visual
features. In this paper, we revisit the hallucinations in LVLMs from an
architectural perspective, investigating whether the primary cause lies in the
visual encoder (feature extraction) or the modal alignment module (feature
decoupling). Motivated by our findings on the preliminary investigation, we
propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.
This plug-and-play method can be integrated into various LVLMs, utilizing
adaptive virtual tokens to extract object features from bounding boxes, thereby
addressing hallucinations caused by insufficient decoupling of visual features.
PATCH achieves state-of-the-art performance on multiple multi-modal
hallucination datasets. We hope this approach provides researchers with deeper
insights into the underlying causes of hallucinations in LVLMs, fostering
further advancements and innovation in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Preserve or To Compress: An In-Depth Study of Connector Selection in
  Multimodal <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have garnered
significant attention from both industry and academia. However, there is still
considerable debate on constructing MLLM architectures, particularly regarding
the selection of appropriate connectors for perception tasks of varying
granularities. This paper systematically investigates the impact of connectors
on MLLM performance. Specifically, we classify connectors into
feature-preserving and feature-compressing types. Utilizing a unified
classification standard, we categorize sub-tasks from three comprehensive
benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained
perception, fine-grained perception, and reasoning, and evaluate the
performance. Our findings reveal that feature-preserving connectors excel in
\emph{fine-grained perception} tasks due to their ability to retain detailed
visual information. In contrast, feature-compressing connectors, while less
effective in fine-grained perception tasks, offer significant speed advantages
and perform comparably in \emph{coarse-grained perception} and \emph{reasoning}
tasks. These insights are crucial for guiding MLLM architecture design and
advancing the optimization of MLLM architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBa: Convergence Balancer for Multitask Finetuning of Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) benefits the fine-tuning of large language models
(LLMs) by providing a single model with improved performance and generalization
ability across tasks, presenting a resource-efficient alternative to developing
separate models for each task. Yet, existing MTL strategies for LLMs often fall
short by either being computationally intensive or failing to ensure
simultaneous task convergence. This paper presents CoBa, a new MTL approach
designed to effectively manage task convergence balance with minimal
computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute
Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically
adjusts task weights during the training process, ensuring that the validation
loss of all tasks progress towards convergence at an even pace while mitigating
the issue of individual task divergence. The results of our experiments
involving three disparate datasets underscore that this approach not only
fosters equilibrium in task improvement but enhances the LLMs' performance by
up to 13% relative to the second-best baselines. Code is open-sourced at
https://github.com/codefuse-ai/MFTCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, main conference of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Programming Language and What Features at <span class="highlight-title">Pre-train</span>ing Stage
  Affect Downstream Logical Inference Performance? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated remarkable
generalization abilities in mathematics and logical reasoning tasks. Prior
research indicates that LLMs pre-trained with programming language data exhibit
high mathematical and reasoning abilities; however, this causal relationship
has not been rigorously tested. Our research aims to verify which programming
languages and features during pre-training affect logical inference
performance. Specifically, we pre-trained decoder-based language models from
scratch using datasets from ten programming languages (e.g., Python, C, Java)
and three natural language datasets (Wikipedia, Fineweb, C4) under identical
conditions. Thereafter, we evaluated the trained models in a few-shot
in-context learning setting on logical reasoning tasks: FLD and bAbi, which do
not require commonsense or world knowledge. The results demonstrate that nearly
all models trained with programming languages consistently outperform those
trained with natural languages, indicating that programming languages contain
factors that elicit logic inference performance. In addition, we found that
models trained with programming languages exhibit a better ability to follow
instructions compared to those trained with natural languages. Further analysis
reveals that the depth of Abstract Syntax Trees representing parsed results of
programs also affects logical reasoning performance. These findings will offer
insights into the essential elements of pre-training for acquiring the
foundational abilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with
  Situation Puzzles <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Chen, Bowen Zhang, Gang Wang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While advancements in NLP have significantly improved the performance of
Large Language Models (LLMs) on tasks requiring vertical thinking, their
lateral thinking capabilities remain under-explored and challenging to measure
due to the complexity of assessing creative thought processes and the scarcity
of relevant data. To address these challenges, we introduce SPLAT, a benchmark
leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
This benchmark, containing 975 graded situation puzzles across three difficulty
levels, employs a new multi-turn player-judge framework instead of the
traditional model-based evaluation, which often necessitates a stronger
evaluation model. This framework simulates an interactive game where the model
(player) asks the evaluation model (judge) questions about an incomplete story
to infer the full scenario. The judge answers based on a detailed reference
scenario or evaluates if the player's predictions align with the reference one.
This approach lessens dependence on more robust evaluation models, enabling the
assessment of state-of-the-art LLMs. The experiments demonstrate that a robust
evaluation model, such as WizardLM-2, closely matches human judgements in both
intermediate question-answering and final scenario accuracy, achieving over 80%
agreement-similar to the agreement levels among humans. Furthermore, applying
data and reasoning processes from our benchmark to other lateral
thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to
performance enhancements. This suggests that our benchmark effectively
evaluates and elicits the lateral thinking abilities of LLMs. Code is available
at: https://github.com/chenqi008/LateralThinking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Mixed quantization in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization of Large Language Models (LLMs) has proven
effective in reducing the computational requirements for running inference on
these models. In this study, we focus on a straightforward question: When
aiming for a specific accuracy or perplexity target for low-precision
quantization, how many high-precision numbers or calculations are required to
preserve as we scale LLMs to larger sizes? We first introduce a critical metric
named the quantization ratio, which compares the number of parameters quantized
to low-precision arithmetic against the total parameter count. Through
extensive and carefully controlled experiments across different model families,
arithmetic types, and quantization granularities (e.g. layer-wise,
matmul-wise), we identify two central phenomenons. 1) The larger the models,
the better they can preserve performance with an increased quantization ratio,
as measured by perplexity in pre-training tasks or accuracy in downstream
tasks. 2) The finer the granularity of mixed-precision quantization (e.g.,
matmul-wise), the more the model can increase the quantization ratio. We
believe these observed phenomena offer valuable insights for future AI hardware
design and the development of advanced Efficient AI algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatMamba: A Matryoshka State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State Space Models (SSMs) like Mamba2 are a promising alternative to
Transformers, with faster theoretical training and inference times --
especially for long context lengths. Recent work on Matryoshka Representation
Learning -- and its application to Transformer backbones in works like
MatFormer -- showed how to introduce nested granularities of smaller submodels
in one universal elastic model. In this work, we present MatMamba: a state
space model which combines Matryoshka-style learning with Mamba2, by modifying
the block to contain nested dimensions to enable joint training and adaptive
inference. MatMamba allows for efficient and adaptive deployment across various
model sizes. We train a single large MatMamba model and are able to get a
number of smaller nested models for free -- while maintaining or improving upon
the performance of a baseline smaller model trained from scratch. We train
language and image models at a variety of parameter sizes from 35M to 1.4B. Our
results on ImageNet and FineWeb show that MatMamba models scale comparably to
Transformers, while having more efficient inference characteristics. This makes
MatMamba a practically viable option for deploying large-scale models in an
elastic way based on the available inference compute. Code and models are open
sourced at \url{https://github.com/ScaledFoundations/MatMamba}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guaranteed Ge<span class="highlight-title">ner</span>ation from <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating Verbalized Probabilities for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, Gyuri Szarvas, Georges Balazs, Pavel Danchenko, Patrick Ernst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating verbalized probabilities presents a novel approach for reliably
assessing and leveraging outputs from black-box Large Language Models (LLMs).
Recent methods have demonstrated improved calibration by applying techniques
like Platt scaling or temperature scaling to the confidence scores generated by
LLMs. In this paper, we explore the calibration of verbalized probability
distributions for discriminative tasks. First, we investigate the capability of
LLMs to generate probability distributions over categorical labels. We
theoretically and empirically identify the issue of re-softmax arising from the
scaling of verbalized probabilities, and propose using the invert softmax trick
to approximate the "logit" by inverting verbalized probabilities. Through
extensive evaluation on three public datasets, we demonstrate: (1) the robust
capability of LLMs in generating class distributions, and (2) the effectiveness
of the invert softmax trick in estimating logits, which, in turn, facilitates
post-calibration adjustments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce PII-Scope, a comprehensive benchmark designed to
evaluate state-of-the-art methodologies for PII extraction attacks targeting
LLMs across diverse threat settings. Our study provides a deeper understanding
of these attacks by uncovering several hyperparameters (e.g., demonstration
selection) crucial to their effectiveness. Building on this understanding, we
extend our study to more realistic attack scenarios, exploring PII attacks that
employ advanced adversarial strategies, including repeated and diverse
querying, and leveraging iterative learning for continual PII extraction.
Through extensive experimentation, our results reveal a notable underestimation
of PII leakage in existing single-query attacks. In fact, we show that with
sophisticated adversarial capabilities and a limited query budget, PII
extraction rates can increase by up to fivefold when targeting the pretrained
model. Moreover, we evaluate PII leakage on finetuned models, showing that they
are more vulnerable to leakage than pretrained models. Overall, our work
establishes a rigorous empirical benchmark for PII extraction attacks in
realistic threat scenarios and provides a strong foundation for developing
effective mitigation strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multimodal LLM for Detailed and Accurate Video Captioning
  using Multi-Round Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zujun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Universality: Studying Mechanistic Similarity Across Language
  Model Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hypothesis of Universality in interpretability suggests that different
neural networks may converge to implement similar algorithms on similar tasks.
In this work, we investigate two mainstream architectures for language
modeling, namely Transformers and Mambas, to explore the extent of their
mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate
interpretable features from these models and show that most features are
similar in these two models. We also validate the correlation between feature
similarity and Universality. We then delve into the circuit-level analysis of
Mamba models and find that the induction circuits in Mamba are structurally
analogous to those in Transformers. We also identify a nuanced difference we
call \emph{Off-by-One motif}: The information of one token is written into the
SSM state in its next position. Whilst interaction between tokens in
Transformers does not exhibit such trend.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Code Executors: An Exploratory Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of Large Language Models (LLMs) have significantly evolved,
extending from natural language processing to complex tasks like code
understanding and generation. We expand the scope of LLMs' capabilities to a
broader context, using LLMs to execute code snippets to obtain the output. This
paper pioneers the exploration of LLMs as code executors, where code snippets
are directly fed to the models for execution, and outputs are returned. We are
the first to comprehensively examine this feasibility across various LLMs,
including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the
o1 model achieved over 90% accuracy in code execution, while others
demonstrated lower accuracy levels. Furthermore, we introduce an Iterative
Instruction Prompting (IIP) technique that processes code snippets line by
line, enhancing the accuracy of weaker models by an average of 7.22% (with the
highest improvement of 18.96%) and an absolute average improvement of 3.86%
against CoT prompting (with the highest improvement of 19.46%). Our study not
only highlights the transformative potential of LLMs in coding but also lays
the groundwork for future advancements in automated programming and the
completion of complex tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subtle Errors Matter: Preference Learning via Error-injected
  Self-editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited strong mathematical reasoning and
computational prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle errors, such
as miscalculations or incorrect substitutions, limit the models' full
mathematical potential. Existing studies to improve mathematical ability
typically involve distilling reasoning skills from stronger LLMs or applying
preference learning to step-wise response pairs. Although these methods
leverage samples of varying granularity to mitigate reasoning errors, they
overlook the frequently occurring subtle errors. A major reason is that sampled
preference pairs involve differences unrelated to the errors, which may
distract the model from focusing on subtle errors. In this work, we propose a
novel preference learning framework called eRror-Injected Self-Editing (RISE),
which injects predefined subtle errors into partial tokens of correct solutions
to construct hard pairs for error mitigation. In detail, RISE uses the model
itself to edit a small number of tokens in the solution, injecting designed
subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective to focus on predefined errors and their tokens, without
requiring fine-grained sampling or preference annotation. Extensive experiments
validate the effectiveness of RISE, with preference learning on
Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on
MATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree of Problems: Improving structured problem solving with
  compositionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armel Zebaze, Benoît Sagot, Rachel Bawden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
multiple tasks through in-context learning. For complex reasoning tasks that
require step-by-step thinking, Chain-of-Thought (CoT) prompting has given
impressive results, especially when combined with self-consistency.
Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree
of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing
the complex problem into paths of subproblems. In this paper, we propose Tree
of Problems (ToP), a simpler version of ToT, which we hypothesise can work
better for complex tasks that can be divided into identical subtasks. Our
empirical results show that our approach outperforms ToT and GoT, and in
addition performs better than CoT on complex reasoning tasks. All code for this
paper is publicly available here:
https://github.com/ArmelRandy/tree-of-problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETA: Evaluating Then Aligning Safety of Vision Language Models at
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Bolian Li, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Evolving Tools for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning enables large language models (LLMs) to interact with external
tools and APIs, greatly expanding the application scope of LLMs. However, due
to the dynamic nature of external environments, these tools and APIs may become
outdated over time, preventing LLMs from correctly invoking tools. Existing
research primarily focuses on static environments and overlooks this issue,
limiting the adaptability of LLMs in real-world applications. In this paper, we
propose ToolEVO, a novel framework designed to enhance the adaptive and
reflective capabilities of LLMs against tool variability. By leveraging Monte
Carlo Tree Search, ToolEVO facilitates active exploration and interaction of
LLMs within dynamic environments, allowing for autonomous self-reflection and
self-updating of tool usage based on environmental feedback. Additionally, we
introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of
tool variability. Extensive experiments demonstrate the effectiveness and
stability of our approach, highlighting the importance of adaptability to tool
variability for effective tool learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoning Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $β$-calibration of Language Model Confidence Scores for Ge<span class="highlight-title">ner</span>ative
  QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putra Manggala, Atalanti Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use generative question-and-answering (QA) systems for decision-making and
in any critical application, these systems need to provide well-calibrated
confidence scores that reflect the correctness of their answers. Existing
calibration methods aim to ensure that the confidence score is on average
indicative of the likelihood that the answer is correct. We argue, however,
that this standard (average-case) notion of calibration is difficult to
interpret for decision-making in generative QA. To address this, we generalize
the standard notion of average calibration and introduce $\beta$-calibration,
which ensures calibration holds across different question-and-answer groups. We
then propose discretized posthoc calibration schemes for achieving
$\beta$-calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Fine-Tuning Unlearning in <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning-based unlearning methods prevail for preventing targeted harmful,
sensitive, or copyrighted information within large language models while
preserving overall capabilities. However, the true effectiveness of these
methods is unclear. In this paper, we delve into the limitations of
fine-tuning-based unlearning through activation patching and parameter
restoration experiments. Our findings reveal that these methods alter the
model's knowledge retrieval process, rather than genuinely erasing the
problematic knowledge embedded in the model parameters. Furthermore, behavioral
tests demonstrate that the unlearning mechanisms inevitably impact the global
behavior of the models, affecting unrelated knowledge or capabilities. Our work
advocates the development of more resilient unlearning techniques for truly
erasing knowledge. Our code is released at
https://github.com/yihuaihong/Dissecting-FT-Unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024 Main (Short paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient
  Attentions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Transformer-based large language models (LLMs) have
set new standards in natural language processing. However, the classical
softmax attention incurs significant computational costs, leading to a $O(T)$
complexity for per-token generation, where $T$ represents the context length.
This work explores reducing LLMs' complexity while maintaining performance by
introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an
innovative data-dependent tempered selection (DDTS) mechanism within a linear
attention-based, purely recurrent framework, achieving significant accuracy
while drastically reducing the memory usage typically associated with recurrent
models. This method exemplifies semantic compression by maintaining essential
input information with fixed-size hidden states. Building on this, Rodimus$+$
combines Rodimus with the innovative Sliding Window Shared-Key Attention
(SW-SKA) in a hybrid approach, effectively leveraging the complementary
semantic, token, and head compression techniques. Our experiments demonstrate
that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior
downstream performance against models trained on more tokens, including
Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the
accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Bias and Enhancing Diagnostic Accuracy in Large Language
  Models for Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pardis Sadat Zahraei, Zahra Shakeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biased AI-generated medical advice and misdiagnoses can jeopardize patient
safety, making the integrity of AI in healthcare more critical than ever. As
Large Language Models (LLMs) take on a growing role in medical decision-making,
addressing their biases and enhancing their accuracy is key to delivering safe,
reliable care. This study addresses these challenges head-on by introducing new
resources designed to promote ethical and precise AI in healthcare. We present
two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate
and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with
32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing
symptom-based diagnostic accuracy. Using these datasets, we developed the
EthiClinician, a fine-tuned model built on the ChatDoctor framework, which
outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing
and correcting hidden biases in existing models for healthcare, our work sets a
new benchmark for safer, more reliable patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ING-VP: MLLMs cannot Play Easy Vision-based Games Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multimodal large language models (MLLMs) continue to demonstrate
increasingly competitive performance across a broad spectrum of tasks, more
intricate and comprehensive benchmarks have been developed to assess these
cutting-edge models. These benchmarks introduce new challenges to core
capabilities such as perception, reasoning, and planning. However, existing
multimodal benchmarks fall short in providing a focused evaluation of
multi-step planning based on spatial relationships in images. To bridge this
gap, we present ING-VP, the first INteractive Game-based Vision Planning
benchmark, specifically designed to evaluate the spatial imagination and
multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,
encompassing 300 levels, each with 6 unique configurations. A single model
engages in over 60,000 rounds of interaction. The benchmark framework allows
for multiple comparison settings, including image-text vs. text-only inputs,
single-step vs. multi-step reasoning, and with-history vs. without-history
conditions, offering valuable insights into the model's capabilities. We
evaluated numerous state-of-the-art MLLMs, with the highest-performing model,
Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the
anticipated standard. This work aims to provide a specialized evaluation
framework to drive advancements in MLLMs' capacity for complex spatial
reasoning and planning. The code is publicly available at
https://github.com/Thisisus7/ING-VP.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield
  Better Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at
[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 27 figures (including 18 in the appendix), submitted to
  EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Cost-Efficiency of LLM-Ge<span class="highlight-title">ner</span>ated Training Data for
  Conversational Semantic Frame Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiho Matta, Yin Jou Huang, Fei Cheng, Hirokazu Kiyomaru, Yugo Murawaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that few-shot learning allows LLMs to
generate training data for supervised models at a low cost. However, the
quality of LLM-generated data may not entirely match that of human-labeled
data. This raises a crucial question: how should one balance the trade-off
between the higher quality but more expensive human data and the lower quality
yet substantially cheaper LLM-generated data? In this paper, we synthesized
training data for conversational semantic frame analysis using GPT-4 and
examined how to allocate budgets optimally to achieve the best performance. Our
experiments, conducted across various budget levels, reveal that optimal
cost-efficiency is achieved by combining both human and LLM-generated data
across a wide range of budget levels. Notably, as the budget decreases, a
higher proportion of LLM-generated data becomes more preferable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages including 4 pages of references and appendix. 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuringQ: Benchmarking AI Comprehension in Theory of Computation <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pardis Sadat Zahraei, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TuringQ, the first benchmark designed to evaluate the reasoning
capabilities of large language models (LLMs) in the theory of computation.
TuringQ consists of 4,006 undergraduate and graduate-level question-answer
pairs, categorized into four difficulty levels and covering seven core
theoretical areas. We evaluate several open-source LLMs, as well as GPT-4,
using Chain of Thought prompting and expert human assessment. Additionally, we
propose an automated LLM-based evaluation system that demonstrates competitive
accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on
TuringQ shows measurable improvements in reasoning ability and out-of-domain
tasks such as algebra. TuringQ serves as both a benchmark and a resource for
enhancing LLM performance in complex computational reasoning tasks. Our
analysis offers insights into LLM capabilities and advances in AI comprehension
of theoretical computer science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chip-Tuning: Classify Before Language Models Say 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwei Zhu, Dian Li, Jiajun Huang, Gang Liu, Hui Wang, Zhifang Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development in the performance of large language models (LLMs) is
accompanied by the escalation of model size, leading to the increasing cost of
model training and inference. Previous research has discovered that certain
layers in LLMs exhibit redundancy, and removing these layers brings only
marginal loss in model performance. In this paper, we adopt the probing
technique to explain the layer redundancy in LLMs and demonstrate that language
models can be effectively pruned with probing classifiers. We propose
chip-tuning, a simple and effective structured pruning framework specialized
for classification problems. Chip-tuning attaches tiny probing classifiers
named chips to different layers of LLMs, and trains chips with the backbone
model frozen. After selecting a chip for classification, all layers subsequent
to the attached layer could be removed with marginal performance loss.
Experimental results on various LLMs and datasets demonstrate that chip-tuning
significantly outperforms previous state-of-the-art baselines in both accuracy
and pruning ratio, achieving a pruning ratio of up to 50%. We also find that
chip-tuning could be applied on multimodal models, and could be combined with
model finetuning, proving its excellent compatibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do great minds think alike? Investigating Human-AI Complementarity in
  Question Answering with CAIMIRA <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maharshi Gor, Hal Daumé III, Tianyi Zhou, Jordan Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements of large language models (LLMs) have led to claims of AI
surpassing humans in natural language processing (NLP) tasks such as textual
understanding and reasoning. This work investigates these assertions by
introducing CAIMIRA, a novel framework rooted in item response theory (IRT)
that enables quantitative assessment and comparison of problem-solving
abilities of question-answering (QA) agents: humans and AI systems. Through
analysis of over 300,000 responses from ~70 AI systems and 155 humans across
thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in
knowledge domains and reasoning skills. Humans outperform AI systems in
knowledge-grounded abductive and conceptual reasoning, while state-of-the-art
LLMs like GPT-4 and LLaMA show superior performance on targeted information
retrieval and fact-based reasoning, particularly when information gaps are
well-defined and addressable through pattern matching or data retrieval. These
findings highlight the need for future QA tasks to focus on questions that
challenge not only higher-order reasoning and scientific thinking, but also
demand nuanced linguistic interpretation and cross-contextual knowledge
application, helping advance AI developments that better emulate or complement
human cognitive abilities in real-world problem-solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel LLM-based Two-stage Summarization Approach for Long Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Jhe Yin, Bo-Yu Chen, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long document summarization poses a significant challenge in natural language
processing due to input lengths that exceed the capacity of most
state-of-the-art pre-trained language models. This study proposes a
hierarchical framework that segments and condenses information from long
documents, subsequently fine-tuning the processed text with an abstractive
summarization model. Unsupervised topic segmentation methods identify
semantically appropriate breakpoints. The condensation stage utilizes an
unsupervised generation model to generate condensed data, and our current
experiments employ ChatGPT(v3.5). The summarization stage fine-tunes the
abstractive summarization model on the condensed data to generate the final
results. This framework enables long documents to be processed on models even
when the document length exceeds the model's maximum input size. The exclusion
of the entire document from the summarization model reduces the time and
computational resources required for training, making the framework suitable
for contexts with constrained local computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEGMENT+: Long Text Processing with Short-Context Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shi, Shuang Li, Kerun Yu, Jinglei Chen, Zujie Liang, Xinhui Wu, Yuxi Qian, Feng Wei, Bo Zheng, Jiaqing Liang, Jiangjie Chen, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in expanding the input capacity of language
models (LMs) across various domains. However, simply increasing the context
window does not guarantee robust performance across diverse long-input
processing tasks, such as understanding extensive documents and extracting
detailed information from lengthy and noisy data. In response, we introduce
SEGMENT+, a general framework that enables LMs to handle extended inputs within
limited context windows efficiently. SEGMENT+ utilizes structured notes and a
filtering module to manage information flow, resulting in a system that is both
controllable and interpretable. Our extensive experiments across various model
sizes, focusing on long-document question-answering and Needle-in-a-Haystack
tasks, demonstrate the effectiveness of SEGMENT+ in improving performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TorchTitan: One-stop PyTorch native solution for production ready LLM
  <span class="highlight-title">pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has been instrumental in
advancing state-of-the-art natural language processing applications. Training
LLMs with billions of parameters and trillions of tokens require sophisticated
distributed systems that enable composing and comparing several
state-of-the-art techniques in order to efficiently scale across thousands of
accelerators. However, existing solutions are complex, scattered across
multiple libraries/repositories, lack interoperability, and are cumbersome to
maintain. Thus, curating and empirically comparing training recipes require
non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed
training system that unifies state-of-the-art techniques, streamlining
integration and reducing overhead. TorchTitan enables 3D parallelism in a
modular manner with elastic scaling, providing comprehensive logging,
checkpointing, and debugging tools for production-ready training. It also
incorporates hardware-software co-designed solutions, leveraging features like
Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan
facilitates custom recipe curation and comparison, allowing us to develop
optimized training recipes for Llama 3.1 and provide guidance on selecting
techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8
billion to 405 billion parameters, and showcase its exceptional performance,
modular composability, and elastic scalability. By stacking training
optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at
the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at
the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at
the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge
  with Curriculum Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique
for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO
have enabled LLMs to distill high-quality behaviors from MCTS, improving their
reasoning performance. However, existing distillation methods underutilize the
rich trajectory information generated by MCTS, limiting the potential for
improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel
pairwise training framework that enables LLMs to self-improve through MCTS
behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via
two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from
child nodes sharing the same parent in the search tree, providing step-level
information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL
introduces curriculum preference learning, dynamically adjusting the training
sequence of trajectory pairs in each offline training epoch to prioritize
critical learning steps and mitigate overfitting. Experimental results on
mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly
outperforms previous MCTS behavior distillation methods, substantially boosting
the reasoning capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Similarity of Circuits across Languages: a Case Study on the
  Subject-verb Agreement Task <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Ferrando, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several algorithms implemented by language models have recently been
successfully reversed-engineered. However, these findings have been
concentrated on specific tasks and models, leaving it unclear how universal
circuits are across different settings. In this paper, we study the circuits
implemented by Gemma 2B for solving the subject-verb agreement task across two
different languages, English and Spanish. We discover that both circuits are
highly consistent, being mainly driven by a particular attention head writing a
`subject number' signal to the last residual stream, which is read by a small
set of neurons in the final MLPs. Notably, this subject number signal is
represented as a direction in the residual stream space, and is
language-independent. We demonstrate that this direction has a causal effect on
the model predictions, effectively flipping the Spanish predicted verb number
by intervening with the direction found in English. Finally, we present
evidence of similar behavior in other models within the Gemma 1 and Gemma 2
families.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Compression with Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rhea Sanjay Sukthanker, Benedikt Staffler, Frank Hutter, Aaron Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing
them to generalize across a wide range of downstream tasks, such as commonsense
reasoning or instruction following. However, as LLMs scale, inference costs
become increasingly prohibitive, accumulating significantly over their life
cycle. This poses the question: Can we compress pre-trained LLMs to meet
diverse size and latency requirements? We leverage Neural Architecture Search
(NAS) to compress LLMs by pruning structural components, such as attention
heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between
performance and efficiency. While NAS already achieved promising results on
small language models in previous work, in this paper we propose various
extensions that allow us to scale to LLMs. Compared to structural pruning
baselines, we show that NAS improves performance up to 3.4% on MMLU with an
on-device latency speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for
  Enhanced Following of Instructions with Multiple Constraints <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction following is a key capability for LLMs. However, recent studies
have shown that LLMs often struggle with instructions containing multiple
constraints (e.g. a request to create a social media post "in a funny tone"
with "no hashtag"). Despite this, most evaluations focus solely on synthetic
data. To address this, we introduce RealInstruct, the first benchmark designed
to evaluate LLMs' ability to follow real-world multi-constrained instructions
by leveraging queries real users asked AI assistants. We also investigate
model-based evaluation as a cost-effective alternative to human annotation for
this task. Our findings reveal that even the proprietary GPT-4 model fails to
meet at least one constraint on over 21% of instructions, highlighting the
limitations of state-of-the-art models. To address the performance gap between
open-source and proprietary models, we propose the Decompose, Critique and
Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to
follow constraints. DeCRIM works by decomposing the original instruction into a
list of constraints and using a Critic model to decide when and where the LLM's
response needs refinement. Our results show that DeCRIM improves Mistral's
performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.
Moreover, we demonstrate that with strong feedback, open-source LLMs with
DeCRIM can outperform GPT-4 on both benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and
  Performance of SGD for Fine-Tuning Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeman Li, Xinwei Zhang, Peilin Zhong, Yuan Deng, Meisam Razaviyayn, Vahab Mirrokni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning language models (LMs) with the Adam optimizer often demands
excessive memory, limiting accessibility. The "in-place" version of Stochastic
Gradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO)
have been proposed to address this. However, IP-SGD still requires substantial
memory, and MeZO suffers from slow convergence and degraded final performance
due to its zeroth-order nature. This paper introduces Addax, a novel method
that improves both memory efficiency and performance of IP-SGD by integrating
it with MeZO. Specifically, Addax computes zeroth- or first-order gradients of
data points in the minibatch based on their memory consumption, combining these
gradient estimates to update directions. By computing zeroth-order gradients
for data points that require more memory and first-order gradients for others,
Addax overcomes the slow convergence of MeZO and the excessive memory
requirement of IP-SGD. Additionally, the zeroth-order gradient acts as a
regularizer for the first-order gradient, further enhancing the model's final
performance. Theoretically, we establish the convergence of Addax under mild
assumptions, demonstrating faster convergence and less restrictive
hyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks
show that Addax consistently outperforms MeZO regarding accuracy and
convergence speed while having a comparable memory footprint. When fine-tuning
OPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1
score by 14% and runs 15x faster while using memory similar to MeZO. In our
experiments on the larger OPT-30B model, on average, Addax outperforms MeZO in
terms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU.
Moreover, Addax surpasses the performance of standard fine-tuning approaches,
such as IP-SGD and Adam, in most tasks with significantly less memory
requirement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish Gupta, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the quality of outputs generated by generative models, such as
large language models and vision language models, presents notable challenges.
Traditional methods for evaluation typically rely on either human assessments,
which are resource-intensive, or automatic metrics that often show a low
correlation with human judgment. Another common approach is to use deep
learning systems, which not only consume a substantial amount of compute and
time but also require extensive training data. In this study, we introduce a
tuning-free framework called ReFeR, designed to evaluate generative outputs,
including both text and images, by leveraging a 2-level hierarchy of LLMs and
VLMs themselves. We rigorously evaluate our framework, ReFeR, across four
diverse evaluation tasks. The framework not only improves the accuracy of these
evaluations, surpassing previous benchmarks but also generates constructive
feedback. Interestingly, the framework is also applicable to reasoning tasks.
Experiments on four reasoning tasks demonstrate superior collective reasoning
abilities of the framework. We present two variants of the framework:
ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a
more cost-effective solution. ReFeR-Lite is $\sim7.7\times$ more efficient
while being comparably accurate to ReFeR-Turbo. We make code, data and PIP
package publicly available. See this PIP URL
https://pypi.org/project/refer-agents/ and this Git URL
https://github.com/yaswanth-iitkgp/ReFeR_Code .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private prediction for large-scale synthetic text ge<span class="highlight-title">ner</span>ation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for generating differentially private synthetic text
using large language models (LLMs), via private prediction. In the private
prediction framework, we only require the output synthetic data to satisfy
differential privacy guarantees. This is in contrast to approaches that train a
generative model on potentially sensitive user-supplied source data and seek to
ensure the model itself is safe to release.
  We prompt a pretrained LLM with source data, but ensure that next-token
predictions are made with differential privacy guarantees. Previous work in
this paradigm reported generating a small number of examples (<10) at
reasonable privacy levels, an amount of data that is useful only for downstream
in-context learning or prompting. In contrast, we make changes that allow us to
generate thousands of high-quality synthetic data points, greatly expanding the
set of potential applications. Our improvements come from an improved privacy
analysis and a better private selection mechanism, which makes use of the
equivalence between the softmax layer for sampling tokens in LLMs and the
exponential mechanism. Furthermore, we introduce a novel use of public
predictions via the sparse vector technique, in which we do not pay privacy
costs for tokens that are predictable without sensitive data; we find this to
be particularly effective for structured data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; updated figure + some new experiments from EMNLP 2024
  findings camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoPAMine: Domain-specific <span class="highlight-title">Pre-train</span>ing Adaptation from seed-guided data
  Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinayak Arannil, Neha Narwal, Sourav Sanjukta Bhabesh, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable ability to generalize
effectively across numerous industry domains while executing a range of tasks.
Many of these competencies are obtained from the data utilized during the
pre-training phase of the Language Models (LMs). However, these models exhibit
limitations when tasked with performing in specialized or low-resource industry
domains. More recent approaches use LLMs for generating domain-specific
synthetic data but most often they lack in truthfulness and complexity.
Alternatively, in cases where domain data is available like healthcare and
finance most of the LMs are proprietary necessitating the need for a scalable
method to curate real world industry specific pre-training data. In this work,
we propose an automated and scalable framework - DoPAMine:Domain-specific
Pre-training Adaptation from seed-guided data Mining, to mine domain specific
training data from a large data corpus for domain adaptation of a LM. The
framework leverages the parametric knowledge of a LLM to generate diverse and
representative seed data tailored to a specific domain which is then used to
mine real world data from a large data corpus like Common Crawl. We evaluated
our framework's performance in the continual pre-training (CPT) setting by
training two domain specific 7B parameter LMs in healthcare and finance with
data mined via DoPAMine. Our experiments show that DoPAMine boosts the
performance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and
5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and
PubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings
respectively on finance tasks from FiQA-SA, FPB and Headlines datasets when
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed
  Embeddings <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embedding is one of the most important components in natural language
processing, but interpreting high-dimensional embeddings remains a challenging
problem. To address this problem, Independent Component Analysis (ICA) is
identified as an effective solution. ICA-transformed word embeddings reveal
interpretable semantic axes; however, the order of these axes are arbitrary. In
this study, we focus on this property and propose a novel method, Axis Tour,
which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional
word embedding method, we aim to improve the clarity of the word embedding
space by maximizing the semantic continuity of the axes. Furthermore, we show
through experiments on downstream tasks that Axis Tour yields better or
comparable low-dimensional embeddings compared to both PCA and ICA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings (short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Contexts Are Equal: Teaching LLMs Credibility-aware Ge<span class="highlight-title">ner</span>ation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and
  models are available at https://github.com/panruotong/CAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactuals As a Means for Evaluating Faithfulness of Attribution
  Methods in Autoregressive Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11252v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11252v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Kamahi, Yadollah Yaghoobzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of autoregressive language models,
explainability evaluation research has predominantly focused on span infilling
and masked language models. Evaluating the faithfulness of an explanation
method -- how accurately it explains the inner workings and decision-making of
the model -- is challenging because it is difficult to separate the model from
its explanation. Most faithfulness evaluation techniques corrupt or remove
input tokens deemed important by a particular attribution (feature importance)
method and observe the resulting change in the model's output. However, for
autoregressive language models, this approach creates out-of-distribution
inputs due to their next-token prediction training objective. In this study, we
propose a technique that leverages counterfactual generation to evaluate the
faithfulness of attribution methods for autoregressive language models. Our
technique generates fluent, in-distribution counterfactuals, making the
evaluation protocol more reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictability maximization and the origins of word order harmony 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16570v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16570v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the linguistic problem of the sequential arrangement of a head and
its dependents from an information theoretic perspective. In particular, we
consider the optimal placement of a head that maximizes the predictability of
the sequence. We assume that dependents are statistically independent given a
head, in line with the open-choice principle and the core assumptions of
dependency grammar. We demonstrate the optimality of harmonic order, i.e.,
placing the head last maximizes the predictability of the head whereas placing
the head first maximizes the predictability of dependents. We also show that
postponing the head is the optimal strategy to maximize its predictability
while bringing it forward is the optimal strategy to maximize the
predictability of dependents. We unravel the advantages of the strategy of
maximizing the predictability of the head over maximizing the predictability of
dependents. Our findings shed light on the placements of the head adopted by
real languages or emerging in different kinds of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections; references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vocabulary Transfer for Medical Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Working within specific NLP subdomains presents significant challenges,
primarily due to a persistent deficit of data. Stringent privacy concerns and
limited data accessibility often drive this shortage. Additionally, the medical
domain demands high accuracy, where even marginal improvements in model
performance can have profound impacts. In this study, we investigate the
potential of vocabulary transfer to enhance model performance in biomedical NLP
tasks. Specifically, we focus on vocabulary extension, a technique that
involves expanding the target vocabulary to incorporate domain-specific
biomedical terms. Our findings demonstrate that vocabulary extension, leads to
measurable improvements in both downstream model performance and inference
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies
  for Zero-shot Knowledge-based VQA <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12746v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12746v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaoyu Li, Haoxin Li, Zilin Du, Boyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based Visual Question-answering (K-VQA) often requires the use of
background knowledge beyond the image. However, we discover that a single
knowledge generation strategy is often insufficient for all K-VQA questions. To
this end, we propose Diversification, Evidence Truncation, and Combination for
Knowledge-based Elucidation (DietCoke), which utilizes a bundle of
complementary question-answering tactics and aggregates their answers using
textual rationales. DietCoke comprises of three stages: diversification,
rationalization, and ensemble. The diversification stage generates three
distinctive decision contexts, each leading to its own answer candidate. The
rationalization stage generates two rationales, the automatic rationale and the
mechanistic rationale, for each answer candidate using decorrelated techniques.
Finally, in the ensemble stage, an LLM informed by the rationales selects one
answer from the three candidates. Experiments show that DietCoke significantly
outperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on
A-OKVOA and that the strategies in the ensembles are highly complementary. Code
is available at: https://github.com/limiaoyu/DietCoke
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of
its default system prompt. Despite current practices of adding personas to
system prompts, it remains unclear how different personas affect a model's
performance on objective tasks. In this study, we present a systematic
evaluation of personas in system prompts. We curate a list of 162 roles
covering 6 types of interpersonal relationships and 8 domains of expertise.
Through extensive analysis of 4 popular families of LLMs and 2,410 factual
questions, we demonstrate that adding personas in system prompts does not
improve model performance across a range of questions compared to the control
setting where no persona is added. Nevertheless, further analysis suggests that
the gender, type, and domain of the persona can all influence the resulting
prediction accuracies. We further experimented with a list of persona search
strategies and found that, while aggregating results from the best persona for
each question significantly improves prediction accuracy, automatically
identifying the best persona is challenging, with predictions often performing
no better than random selection. Overall, our findings suggest that while
adding a persona may lead to performance gains in certain settings, the effect
of each persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeteoRA: Multiple-tasks Embedded LoRA for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Xu, Junyu Lai, Yunpeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain+fine-tune paradigm is foundational for deploying large language
models (LLMs) across various downstream applications. Within this framework,
Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning
(PEFT), producing numerous reusable task-specific LoRA adapters. However, this
approach requires explicit task intention selection, posing challenges for
autonomous task sensing and switching during inference with multiple existing
LoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA
(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses
multiple task-specific LoRA adapters into the base LLM via a full-mode
Mixture-of-Experts (MoE) architecture. This framework also includes novel MoE
forward acceleration strategies to address the efficiency challenges of
traditional MoE implementations. Our evaluation, using the LlaMA2-13B and
LlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,
demonstrates equivalent performance with the traditional PEFT method. Moreover,
the LLM equipped with MeteoRA achieves superior performance in handling
composite tasks, effectively solving ten sequential problems in a single
inference pass, thereby demonstrating the framework's enhanced capability for
timely adapter switching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Structure from a Bottleneck on Sequential Information
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Futrell, Michael Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language is a unique form of communication in the natural world,
distinguished by its structured nature. Most fundamentally, it is systematic,
meaning that signals can be broken down into component parts that are
individually meaningful -- roughly, words -- which are combined in a regular
way to form sentences. Furthermore, the way in which these parts are combined
maintains a kind of locality: words are usually concatenated together, and they
form contiguous phrases, keeping related parts of sentences close to each
other. We address the challenge of understanding how these basic properties of
language arise from broader principles of efficient communication under
information processing constraints. Here we show that natural-language-like
systematicity arises in codes that are constrained by predictive information, a
measure of the amount of information that must be extracted from the past of a
sequence in order to predict its future. In simulations, we show that such
codes approximately factorize their source distributions, and then express the
resulting factors systematically and locally. Next, in a series of
cross-linguistic corpus studies, we show that human languages are structured to
have low predictive information at the levels of phonology, morphology, syntax,
and semantics. Our result suggests that human language performs a sequential,
discrete form of Independent Components Analysis on the statistical
distribution over meanings that need to be expressed. It establishes a link
between the statistical and algebraic structure of human language, and
reinforces the idea that the structure of human language is shaped by
communication under cognitive constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4V Cannot Ge<span class="highlight-title">ner</span>ate Radiology Reports Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, code:
  https://github.com/YuyangJ0/GPT-4V-evaluation-radiology-report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Higher-Order Correlations Among Semantic Components in
  Embeddings <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Independent Component Analysis (ICA) offers interpretable semantic components
of embeddings. While ICA theory assumes that embeddings can be linearly
decomposed into independent components, real-world data often do not satisfy
this assumption. Consequently, non-independencies remain between the estimated
components, which ICA cannot eliminate. We quantified these non-independencies
using higher-order correlations and demonstrated that when the higher-order
correlation between two components is large, it indicates a strong semantic
association between them, along with many words sharing common meanings with
both components. The entire structure of non-independencies was visualized
using a maximum spanning tree of semantic components. These findings provide
deeper insights into embeddings through ICA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn while Unlearn: An Iterative Unlearning Framework for Ge<span class="highlight-title">ner</span>ative
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Tang, Ye Liu, Xukai Liu, Kai Zhang, Yanghai Zhang, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in machine learning, particularly in Natural Language
Processing (NLP), have led to the development of sophisticated models trained
on extensive datasets, yet raising concerns about the potential leakage of
sensitive information. In response, regulatory measures such as the European
Union's General Data Protection Regulation (GDPR) have driven increasing
interest in Machine Unlearning techniques, which enable models to selectively
forget specific data entries. Early approaches primarily relied on
pre-processing methods, while more recent research has shifted towards
training-based unlearning techniques. Despite their effectiveness, most
existing methods require access to the original training data, which is often
inaccessible. Additionally, directly applying unlearning techniques bear the
cost of undermining the model's expressive capabilities. To address these
challenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,
which consists of three core components: A Knowledge Unlearning Induction
module designed to remove specific knowledge through an unlearning loss; A
Contrastive Learning Enhancement module to preserve the model's expressive
capabilities against the pure unlearning goal; And an Iterative Unlearning
Refinement module that dynamically assess the unlearning extent on specific
data pieces and make iterative update. Experimental results demonstrate the
efficacy of our ICU method in unlearning sensitive information while
maintaining the model's overall performance, offering a promising solution for
privacy-conscious machine learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scope-enhanced Compositional Semantic Parsing for DRT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulin Yang, Jonas Groschwitz, Alexander Koller, Johan Bos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discourse Representation Theory (DRT) distinguishes itself from other
semantic representation frameworks by its ability to model complex semantic and
discourse phenomena through structural nesting and variable binding. While
seq2seq models hold the state of the art on DRT parsing, their accuracy
degrades with the complexity of the sentence, and they sometimes struggle to
produce well-formed DRT representations. We introduce the AMS parser, a
compositional, neurosymbolic semantic parser for DRT. It rests on a novel
mechanism for predicting quantifier scope. We show that the AMS parser reliably
produces well-formed outputs and performs well on DRT parsing, especially on
complex sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking
  Based on LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher M. Ackerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, I extend the idea of
active steering with vectors that represent a behavioral direction of interest
to tuning those vectors directly into the model, obviating the need for online
control. First, I identify activation vectors related to honesty in an
open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can
be made more or less honest by adding positive or negative multiples of these
vectors to residual stream activations during generation. Then, I show that a
similar effect can be achieved by fine-tuning the vectors directly into the
model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss ("representation tuning"). Finally, I compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning; tuned
models are available at https://huggingface.co/collections/cackerman/
representation-tuning-66da1e5ab41cd1b824687d9f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Separation based on Contrastive Learning and Deep Modularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10652v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10652v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Ochieng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current monaural state of the art tools for speech separation relies on
supervised learning. This means that they must deal with permutation problem,
they are impacted by the mismatch on the number of speakers used in training
and inference. Moreover, their performance heavily relies on the presence of
high-quality labelled data. These problems can be effectively addressed by
employing a fully unsupervised technique for speech separation. In this paper,
we use contrastive learning to establish the representations of frames then use
the learned representations in the downstream deep modularization task.
Concretely, we demonstrate experimentally that in speech separation, different
frames of a speaker can be viewed as augmentations of a given hidden standard
frame of that speaker. The frames of a speaker contain enough prosodic
information overlap which is key in speech separation. Based on this, we
implement a self-supervised learning to learn to minimize the distance between
frames belonging to a given speaker. The learned representations are used in a
downstream deep modularization task to cluster frames based on speaker
identity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix
shows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively
in WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7
respectively in WSJ0-2mix. Its greatest strength being that as the number of
speakers increase, its performance does not degrade significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2212.00369</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE V2: Process Attention Score as Feature Map for Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. arXiv admin note: text overlap with arXiv:2405.14722</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Noise Robustness of In-Context Learning for Text Ge<span class="highlight-title">ner</span>ation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive performance on downstream
tasks by in-context learning (ICL), which heavily relies on the quality of
demonstrations selected from a large set of annotated examples. Recent works
claim that in-context learning is robust to noisy demonstrations in text
classification. In this work, we show that, on text generation tasks, noisy
annotations significantly hurt the performance of in-context learning. To
circumvent the issue, we propose a simple and effective approach called Local
Perplexity Ranking (LPR), which replaces the "noisy" candidates with their
nearest neighbors that are more likely to be clean. Our method is motivated by
analyzing the perplexity deviation caused by noisy labels and decomposing
perplexity into inherent perplexity and matching perplexity. Our key idea
behind LPR is thus to decouple the matching perplexity by performing the
ranking among the neighbors in semantic space. Our approach can prevent the
selected demonstrations from including mismatched input-label pairs while
preserving the effectiveness of the original selection methods. Extensive
experiments demonstrate the effectiveness of LPR, improving the EM score by up
to 18.75 on common benchmarks with noisy annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with
  ASCII Art to Mask Profanity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18708v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18708v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Berezin, Reza Farahbakhsh, Noel Crespi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel family of adversarial attacks that exploit the inability
of language models to interpret ASCII art. To evaluate these attacks, we
propose the ToxASCII benchmark and develop two custom ASCII art fonts: one
leveraging special tokens and another using text-filled letter shapes. Our
attacks achieve a perfect 1.0 Attack Success Rate across ten models, including
OpenAI's o1-preview and LLaMA 3.1.
  Warning: this paper contains examples of toxic language used for research
purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Script Barrier in Multilingual <span class="highlight-title">Pre-Train</span>ed Language Models
  with Transliteration-Based Post-Training Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orgest Xhelili, Yihong Liu, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual pre-trained models (mPLMs) have shown impressive performance on
cross-lingual transfer tasks. However, the transfer performance is often
hindered when a low-resource target language is written in a different script
than the high-resource source language, even though the two languages may be
related or share parts of their vocabularies. Inspired by recent work that uses
transliteration to address this problem, our paper proposes a
transliteration-based post-pretraining alignment (PPA) method aiming to improve
the cross-lingual alignment between languages using diverse scripts. We select
two areal language groups, $\textbf{Mediterranean-Amharic-Farsi}$ and
$\textbf{South+East Asian Languages}$, wherein the languages are mutually
influenced but use different scripts. We apply our method to these language
groups and conduct extensive experiments on a spectrum of downstream tasks. The
results show that after PPA, models consistently outperform the original model
(up to 50% for some tasks) in English-centric transfer. In addition, when we
use languages other than English as sources in transfer, our method obtains
even larger improvements. We will make our code and models publicly available
at \url{https://github.com/cisnlp/Transliteration-PPA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quest: Query-centric Data Synthesis Approach for Long-context Scaling of
  <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19846v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19846v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have highlighted the
importance of extending context lengths for handling complex tasks. While
traditional methods for training on long contexts often use filtered long
documents, these approaches lead to domain imbalances, limiting model
performance. To address this, techniques like random document concatenation
(Standard) and similarity-based methods (KNN, ICLM) have been developed.
However, they either sacrifice semantic coherence or diversity. To balance both
aspects, we introduce Quest, a query-centric data synthesis method aggregating
semantically relevant yet diverse documents. Quest uses a generative model to
predict potential queries for each document, grouping documents with similar
queries and keywords. Extensive experiments demonstrate Quest's superior
performance on long-context tasks, achieving remarkable results with context
lengths of up to 1M tokens and confirming its scalability across various model
sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Persona to Personalization: A <span class="highlight-title">Survey</span> on Role-Playing Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI
systems designed to simulate assigned personas. By harnessing multiple advanced
abilities of LLMs, including in-context learning, instruction following, and
social intelligence, RPLAs achieve a remarkable sense of human likeness and
vivid role-playing performance. RPLAs can mimic a wide range of personas,
ranging from historical figures and fictional characters to real-life
individuals. Consequently, they have catalyzed numerous AI applications, such
as emotional companions, interactive video games, personalized assistants and
copilots, and digital clones. In this paper, we conduct a comprehensive survey
of this field, illustrating the evolution and recent progress in RPLAs
integrating with cutting-edge LLM technologies. We categorize personas into
three types: 1) Demographic Persona, which leverages statistical stereotypes;
2) Character Persona, focused on well-established figures; and 3)
Individualized Persona, customized through ongoing user interactions for
personalized services. We begin by presenting a comprehensive overview of
current methodologies for RPLAs, followed by the details for each persona type,
covering corresponding data sourcing, agent construction, and evaluation.
Afterward, we discuss the fundamental risks, existing limitations, and future
prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI
applications, which reflects practical user demands that shape and drive RPLA
research. Through this work, we aim to establish a clear taxonomy of RPLA
research and applications, and facilitate future research in this critical and
ever-evolving field, and pave the way for a future where humans and RPLAs
coexist in harmony.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do <span class="highlight-title">Large Language Model</span>s Need for Machine Translation Evaluation? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Orăsan, Tharindu Ranasinghe, Frédéric Blain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for various natural language
processing tasks has led to superlative claims about their performance. For the
evaluation of machine translation (MT), existing research shows that LLMs are
able to achieve results comparable to fine-tuned multilingual pre-trained
language models. In this paper, we explore what translation information, such
as the source, reference, translation errors and annotation guidelines, is
needed for LLMs to evaluate MT quality. In addition, we investigate prompting
techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for
eight language pairs covering high-, medium- and low-resource languages,
leveraging varying LLM variants. Our findings indicate the importance of
reference translations for an LLM-based evaluation. While larger models do not
necessarily fare better, they tend to benefit more from CoT prompting, than
smaller models. We also observe that LLMs do not always provide a numerical
score when generating evaluations, which poses a question on their reliability
for the task. Our work presents a comprehensive analysis for
resource-constrained and training-less LLM-based evaluation of machine
translation. We release the accrued prompt templates, code and data publicly
for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithfulness and the Notion of Adversarial Sensitivity in NLP
  Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Manna, Niladri Sett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness is arguably the most critical metric to assess the reliability
of explainable AI. In NLP, current methods for faithfulness evaluation are
fraught with discrepancies and biases, often failing to capture the true
reasoning of models. We introduce Adversarial Sensitivity as a novel approach
to faithfulness evaluation, focusing on the explainer's response when the model
is under adversarial attack. Our method accounts for the faithfulness of
explainers by capturing sensitivity to adversarial input changes. This work
addresses significant limitations in existing evaluation techniques, and
furthermore, quantifies faithfulness from a crucial yet underexplored paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functionality learning through specification instructions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Luz de Araujo, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. This paper introduces
specification instructions: text descriptions specifying fine-grained
task-specific behaviors. For each functionality in a suite, we generate an
instruction that describes it. We combine the specification instructions to
create specification-augmented prompts, which we feed to language models
pre-trained on natural instruction data.
  We conduct experiments to measure how optimizing for some functionalities may
negatively impact functionalities that are not covered by the specification
set. Our analyses across four tasks and models of diverse sizes and families
show that smaller models struggle to follow specification instructions.
However, larger models (>~3B params.) can benefit from specifications and --
surprisingly -- even generalize certain desirable behaviors across
functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 8 figures. Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Vul<span class="highlight-title">ner</span>abilities in <span class="highlight-title">Large Language Model</span>s: Backdoor Attacks for
  In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05949v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05949v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Despite being widely applied, in-context learning is
vulnerable to malicious attacks. In this work, we raise security concerns
regarding this paradigm. Our studies demonstrate that an attacker can
manipulate the behavior of large language models by poisoning the demonstration
context, without the need for fine-tuning the model. Specifically, we design a
new backdoor attack method, named ICLAttack, to target large language models
based on in-context learning. Our method encompasses two types of attacks:
poisoning demonstration examples and poisoning demonstration prompts, which can
make models behave in alignment with predefined intentions. ICLAttack does not
require additional fine-tuning to implant a backdoor, thus preserving the
model's generality. Furthermore, the poisoned examples are correctly labeled,
enhancing the natural stealth of our attack method. Extensive experimental
results across several language models, ranging in size from 1.3B to 180B
parameters, demonstrate the effectiveness of our attack method, exemplified by
a high average attack success rate of 95.0% across the three datasets on OPT
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in
  <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zara Siddique, Liam D. Turner, Luis Espinosa-Anke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to propagate and amplify harmful
stereotypes, particularly those that disproportionately affect marginalised
communities. To understand the effect of these stereotypes more
comprehensively, we introduce GlobalBias, a dataset of 876k sentences
incorporating 40 distinct gender-by-ethnicity groups alongside descriptors
typically used in bias literature, which enables us to study a broad set of
stereotypes from around the world. We use GlobalBias to directly probe a suite
of LMs via perplexity, which we use as a proxy to determine how certain
stereotypes are represented in the model's internal representations. Following
this, we generate character profiles based on given names and evaluate the
prevalence of stereotypes in model outputs. We find that the demographic groups
associated with various stereotypes remain consistent across model likelihoods
and model outputs. Furthermore, larger models consistently display higher
levels of stereotypical outputs, even when explicitly instructed not to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Main 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Enhanced Protein Instruction Tuning: Towards Ge<span class="highlight-title">ner</span>al-Purpose
  Protein Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proteins, as essential biomolecules, play a central role in biological
processes, including metabolic reactions and DNA replication. Accurate
prediction of their properties and functions is crucial in biological
applications. Recent development of protein language models (pLMs) with
supervised fine tuning provides a promising solution to this problem. However,
the fine-tuned model is tailored for particular downstream prediction task, and
achieving general-purpose protein understanding remains a challenge. In this
paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)
framework to bridge this gap. Our approach integrates a noval structure-aware
module into pLMs to inform them with structural knowledge, and then connects
these enhanced pLMs to large language models (LLMs) to generate understanding
of proteins. In this framework, we propose a novel two-stage instruction tuning
pipeline that first establishes a basic understanding of proteins through
caption-based instructions and then refines this understanding using a mixture
of experts (MoEs) to learn more complex properties and functional information
with the same amount of activated parameters. Moreover, we construct the
largest and most comprehensive protein instruction dataset to date, which
allows us to train and evaluate the general-purpose protein understanding
model. Extensive experimental results on open-ended generation and closed-set
answer tasks demonstrate the superior performance of SEPIT over both
closed-source general LLMs and open-source LLMs trained with protein knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Subtle Differences between Human and Model Languages Using
  Spectrum of Relative Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and model-generated texts can be distinguished by examining the
magnitude of likelihood in language. However, it is becoming increasingly
difficult as language model's capabilities of generating human-like texts keep
evolving. This study provides a new perspective by using the relative
likelihood values instead of absolute ones, and extracting useful features from
the spectrum-view of likelihood for the human-model text detection task. We
propose a detection procedure with two classification methods, supervised and
heuristic-based, respectively, which results in competitive performances with
previous zero-shot detection methods and a new state-of-the-art on short-text
detection. Our method can also reveal subtle differences between human and
model languages, which find theoretical roots in psycholinguistics studies. Our
code is available at https://github.com/CLCS-SUSTech/FourierGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion
  Cause 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Lijie Hu, Hasti Seifi, Jiayuan Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal emotion recognition in conversation (MERC) and multimodal
emotion-cause pair extraction (MECPE) have recently garnered significant
attention. Emotions are the expression of affect or feelings; responses to
specific events, or situations -- known as emotion causes. Both collectively
explain the causality between human emotion and intents. However, existing
works treat emotion recognition and emotion cause extraction as two individual
problems, ignoring their natural causality. In this paper, we propose a Unified
Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC)
to explore the causality between emotion and emotion cause. Concretely, UniMEEC
reformulates the MERC and MECPE tasks as mask prediction problems and unifies
them with a causal prompt template. To differentiate the modal effects, UniMEEC
proposes a multimodal causal prompt to probe the pre-trained knowledge
specified to modality and implements cross-task and cross-modality interactions
under task-oriented settings. Experiment results on four public benchmark
datasets verify the model performance on MERC and MECPE tasks and achieve
consistent improvements compared with the previous state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling In-Context Learning: A Coordinate System to Understand Its
  Working Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anhao Zhao, Fanghua Ye, Jinlan Fu, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable in-context learning (ICL)
capabilities. However, the underlying working mechanism of ICL remains poorly
understood. Recent research presents two conflicting views on ICL: One
emphasizes the impact of similar examples in the demonstrations, stressing the
need for label correctness and more shots. The other attributes it to LLMs'
inherent ability of task recognition, deeming label correctness and shot
numbers of demonstrations as not crucial. In this work, we provide a
Two-Dimensional Coordinate System that unifies both views into a systematic
framework. The framework explains the behavior of ICL through two orthogonal
variables: whether similar examples are presented in the demonstrations
(perception) and whether LLMs can recognize the task (cognition). We propose
the peak inverse rank metric to detect the task recognition ability of LLMs and
study LLMs' reactions to different definitions of similarity. Based on these,
we conduct extensive experiments to elucidate how ICL functions across each
quadrant on multiple representative classification tasks. Finally, we extend
our analyses to generation tasks, showing that our coordinate system can also
be used to interpret ICL for generation tasks effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrating the Confidence of <span class="highlight-title">Large Language Model</span>s by Eliciting
  Fidelity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models optimized with techniques like RLHF have achieved good
alignment in being helpful and harmless. However, post-alignment, these
language models often exhibit overconfidence, where the expressed confidence
does not accurately calibrate with their correctness rate. In this paper, we
decompose the language model confidence into the \textit{Uncertainty} about the
question and the \textit{Fidelity} to the answer generated by language models.
Then, we propose a plug-and-play method to estimate the confidence of language
models. Our method has shown good calibration performance by conducting
experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two
novel metrics, IPR and CE, to evaluate the calibration of the model, and we
have conducted a detailed discussion on \textit{Truly Well-Calibrated
Confidence}. Our method could serve as a strong baseline, and we hope that this
work will provide some insights into the model confidence calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Academic Skills Assessment with NLP and Ensemble Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpei Cheng, Yingyi Wu, Danyang Zhang, Jiacheng Hu, Yujian Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenges of assessing foundational
academic skills by leveraging advancements in natural language processing
(NLP). Traditional assessment methods often struggle to provide timely and
comprehensive feedback on key cognitive and linguistic aspects, such as
coherence, syntax, and analytical reasoning. Our approach integrates multiple
state-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,
within an ensemble learning framework. These models are combined through
stacking techniques using LightGBM and Ridge regression to enhance predictive
accuracy. The methodology involves detailed data preprocessing, feature
extraction, and pseudo-label learning to optimize model performance. By
incorporating sophisticated NLP techniques and ensemble learning, this study
significantly improves the accuracy and efficiency of assessments, offering a
robust solution that surpasses traditional methods and opens new avenues for
educational technology research focused on enhancing core academic
competencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SBoRA: Low-Rank Adaptation with Regional Weight Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai-Man Po, Yuyang Liu, Haoxuan Wu, Tianqi Zhang, Wing-Yin Yu, Zhuohan Wang, Zeyu Jiang, Kun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Standard Basis LoRA (SBoRA), a novel
parameter-efficient fine-tuning approach for Large Language Models that builds
upon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal
Adaptation. SBoRA reduces the number of trainable parameters by half or doubles
the rank with the similar number of trainable parameters as LoRA, while
improving learning performance. By utilizing orthogonal standard basis vectors
to initialize one of the low-rank matrices (either $\mathbf{A}$ or
$\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient
fine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only
one of the matrices is updated, leading to a sparse update matrix
$\mathrm{\Delta} \mathbf{W}$ with predominantly zero rows or columns.
Consequently, most of the fine-tuned model's weights
$(\mathbf{W}_0+\mathrm{\Delta} \mathbf{W})$ remain unchanged from the
pre-trained weights, akin to the modular organization of the human brain, which
efficiently adapts to new tasks. Our empirical results demonstrate the
superiority of SBoRA-FA over LoRA in various fine-tuning tasks, including
commonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the
effectiveness of QSBoRA on quantized LLaMA models of varying scales,
highlighting its potential for efficient adaptation to new tasks. Code is
available at https://github.com/cityuhkai/SBoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Large Language Model</span>s on Time Series Feature Understanding: A
  Comprehensive Taxonomy and Benchmark <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Fons, Rachneet Kaur, Soham Palande, Zhen Zeng, Tucker Balch, Manuela Veloso, Svitlana Vyetrenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer the potential for automatic time series
analysis and reporting, which is a critical task across many domains, spanning
healthcare, finance, climate, energy, and many more. In this paper, we propose
a framework for rigorously evaluating the capabilities of LLMs on time series
understanding, encompassing both univariate and multivariate forms. We
introduce a comprehensive taxonomy of time series features, a critical
framework that delineates various characteristics inherent in time series data.
Leveraging this taxonomy, we have systematically designed and synthesized a
diverse dataset of time series, embodying the different outlined features, each
accompanied by textual descriptions. This dataset acts as a solid foundation
for assessing the proficiency of LLMs in comprehending time series. Our
experiments shed light on the strengths and limitations of state-of-the-art
LLMs in time series understanding, revealing which features these models
readily comprehend effectively and where they falter. In addition, we uncover
the sensitivity of LLMs to factors including the formatting of the data, the
position of points queried within a series and the overall time series length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressively Label Enhancement for <span class="highlight-title">Large Language Model</span> Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Liu, Ning Xu, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) alignment aims to prevent models from producing
content that misaligns with human expectations, which can lead to ethical and
legal concerns. In the last few years, Reinforcement Learning from Human
Feedback (RLHF) has been the most prominent method for achieving alignment. Due
to challenges in stability and scalability with RLHF stages, which arise from
the complex interactions between multiple models, researchers are exploring
alternative methods to achieve effects comparable to those of RLHF. However,
these methods often rely on large high-quality datasets. Despite some methods
considering the generation of additional data to expand datasets, they often
treat model training and data generation as separate and static processes,
overlooking the fact that these processes are highly interdependent, leading to
inefficient utilization of the generated data. To deal with this problem, we
propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a
framework that dynamically adjusts the model's training process based on the
evolving quality of the generated data. Specifically, we prompt the model to
generate responses for both the original query and the query guided by a set of
carefully designed principles, and then utilize a dynamic threshold to
determine the appropriate training approach for both responses based on their
corresponding reward scores. Experimental results demonstrate the effectiveness
of PLE compared to existing LLM alignment methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Notion of Complexity for Theory of Mind via Discrete World Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        X. Angelo Huang, Emanuele La Malfa, Samuele Marro, Andrea Asperti, Anthony Cohn, Michael Wooldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) can be used to assess the capabilities of Large Language
Models (LLMs) in complex scenarios where social reasoning is required. While
the research community has proposed many ToM benchmarks, their hardness varies
greatly, and their complexity is not well defined. This work proposes a
framework inspired by cognitive load theory to measure the complexity of ToM
tasks. We quantify a problem's complexity as the number of states necessary to
solve it correctly. Our complexity measure also accounts for spurious states of
a ToM problem designed to make it apparently harder. We use our method to
assess the complexity of five widely adopted ToM benchmarks. On top of this
framework, we design a prompting technique that augments the information
available to a model with a description of how the environment changes with the
agents' interactions. We name this technique Discrete World Models (DWM) and
show how it elicits superior performance on ToM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted EMNLP 2024, Website
  https://flecart.github.io/complexity-tom-dwm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Separators Improve Chain-of-Thought <span class="highlight-title">Prompt</span>ing? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonjeong Park, Hyunjin Kim, Chanyeol Choi, Junseong Kim, Jy-yong Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting is a simple and effective method for
improving the reasoning capabilities of Large Language Models (LLMs). The basic
idea of CoT is to let LLMs break down their thought processes step-by-step by
putting exemplars in the input prompt. However, the densely structured prompt
exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human
cognition, we introduce COT-SEP, a method that strategically employs separators
at the end of each exemplar in CoT prompting. These separators are designed to
help the LLMs understand their thought processes better while reasoning.
Interestingly, it turns out that COT-SEP significantly improves the LLMs'
performances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared
with the vanilla CoT, which does not use separators. We also study the effects
of the type and the location of separators tested on multiple LLMs, including
GPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE FLLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoCA: Regaining Safety-awareness of Multimodal <span class="highlight-title">Large Language Model</span>s
  with Constitutional Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of multimodal large language models (MLLMs) has demonstrated
remarkable success in engaging in conversations involving visual inputs, thanks
to the superior power of large language models (LLMs). Those MLLMs are
typically built based on the LLMs, with an image encoder to process images into
the token embedding space of the LLMs. However, the integration of visual
modality has introduced a unique vulnerability: the MLLM becomes susceptible to
malicious visual inputs and prone to generating sensitive or harmful responses,
even though the LLM has been trained on textual dataset to align with human
value. In this paper, we first raise the question: ``Do the MLLMs possess
safety-awareness against malicious image inputs?". We find that after adding a
principle that specifies the safety requirement into the input of the MLLM, the
model's safety awareness becomes boosted. This phenomenon verifies the
existence of MLLM's safety-awareness against image inputs, it is only weakened
by the modality gap. We then introduce a simple yet effective technique termed
CoCA, which amplifies the safety-awareness of the MLLM by calibrating its
output distribution. Our proposed strategy helps the model reclaim its original
safety awareness without losing its original capabilities. We verify the
effectiveness of our approach on both multimodal safety and understanding
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, COLM-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging and Modeling Correlations in Pairwise Data for Direct
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO), a widely adopted offline preference
optimization algorithm, aims to align large language models (LLMs) with
human-desired behaviors using pairwise preference data. However, the winning
response and the losing response within pairwise data are generated isolatedly,
leading to weak correlations between them as well as suboptimal alignment
performance. To address this issue, we propose an effective framework for
Bridging and Modeling Correlations in pairwise data, named BMC. Firstly, we
increase the consistency and informativeness of the pairwise preference signals
through targeted modifications, synthesizing a pseudo-winning response by
improving the losing response with the winning response as a reference.
Secondly, we identify that DPO alone is insufficient to model these
correlations and capture nuanced variations. Therefore, we propose learning
token-level correlations by dynamically leveraging the policy model's
confidence during training. Comprehensive experiments on QA, math, and
instruction-following tasks demonstrate the effectiveness of our approach,
significantly surpassing competitive baselines, including DPO. Additionally,
our in-depth quantitative analysis reveals the reasons behind our method's
superior performance over DPO and showcases its versatility to other DPO
variants. We release our repository at https://github.com/YJiangcm/BMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures, 10 tables, working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMC: Benchmarking <span class="highlight-title">Large Language Model</span> Quantization with a Versatile
  Compression Toolkit <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chengtao Lv, Yunchen Zhang, Xianglong Liu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) are propelling us toward
artificial general intelligence with their remarkable emergent abilities and
reasoning capabilities. However, the substantial computational and memory
requirements limit the widespread adoption. Quantization, a key compression
technique, can effectively mitigate these demands by compressing and
accelerating LLMs, albeit with potential risks to accuracy. Numerous studies
have aimed to minimize the accuracy loss associated with quantization. However,
their quantization configurations vary from each other and cannot be fairly
compared. In this paper, we present LLMC, a plug-and-play compression toolkit,
to fairly and systematically explore the impact of quantization. LLMC
integrates dozens of algorithms, models, and hardwares, offering high
extensibility from integer to floating-point quantization, from LLM to
vision-language (VLM) model, from fixed-bit to mixed precision, and from
quantization to sparsification. Powered by this versatile toolkit, our
benchmark covers three key aspects: calibration data, algorithms (three
strategies), and data formats, providing novel insights and detailed analyses
for further research and practical guidance for users. Our toolkit is available
at https://github.com/ModelTC/llmc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Correctness: Benchmarking Multi-dimensional Code Ge<span class="highlight-title">ner</span>ation for
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, researchers have proposed numerous benchmarks to evaluate
the impressive coding capabilities of large language models (LLMs). However,
current benchmarks primarily assess the accuracy of LLM-generated code, while
neglecting other critical dimensions that also significantly impact code
quality in real-world development. Moreover, relying exclusively on correctness
as the guiding metric renders LLMs susceptible to data contamination.
Therefore, this paper proposes the RACE benchmark, which comprehensively
evaluates the quality of code generated by LLMs across 4 dimensions:
Readability, mAintainability, Correctness, and Efficiency. Specifically,
considering the demand-dependent nature of dimensions beyond correctness, we
design various types of user requirements for each dimension to assess the
model's ability to generate correct code that also meets user demands. We
analyze 28 representative LLMs based on RACE and find that: 1) current
correctness-centric benchmarks fail to capture the multifaceted requirements of
code in real-world scenarios, while RACE provides a comprehensive evaluation
that reveals the defects of LLMs across multiple dimensions; 2) the RACE
benchmark serves as an effective tool for resisting the risk of data
contamination; 3) even the most advanced code LLMs still encounter significant
challenges in customized requirements involving complex instructions; 4) most
LLMs exhibit an inherent preference for specific coding style. These findings
highlight the need for a multidimensional evaluation of code LLMs, emphasizing
metrics beyond correctness for real-world applications. Future efforts should
aim to develop novel learning algorithms to enhance code generation under
varied constraints and improve coverage and usability for diverse user needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release benchmark at https://github.com/jszheng21/RACE and
  leaderboard at https://huggingface.co/spaces/jszheng/RACE_leaderboard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Plan for Retrieval-Augmented <span class="highlight-title">Large Language Model</span>s from
  Knowledge Graphs <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the performance of large language models (LLMs) in complex
question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining
step-wise planning with external retrieval. While effective for advanced models
like GPT-3.5, smaller LLMs face challenges in decomposing complex questions,
necessitating supervised fine-tuning. Previous work has relied on manual
annotation and knowledge distillation from teacher LLMs, which are
time-consuming and not accurate enough. In this paper, we introduce a novel
framework for enhancing LLMs' planning capabilities by using planning data
derived from knowledge graphs (KGs). LLMs fine-tuned with this data have
improved planning capabilities, better equipping them to handle complex QA
tasks that involve retrieval. Evaluations on multiple datasets, including our
newly proposed benchmark, highlight the effectiveness of our framework and the
benefits of KG-derived planning data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can We Trust the Performance Evaluation of Uncertainty Estimation
  Methods in Text Summarization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text summarization, a key natural language generation (NLG) task, is vital in
various domains. However, the high cost of inaccurate summaries in
risk-critical applications, particularly those involving human-in-the-loop
decision-making, raises concerns about the reliability of uncertainty
estimation on text summarization (UE-TS) evaluation methods. This concern stems
from the dependency of uncertainty model metrics on diverse and potentially
conflicting NLG metrics. To address this issue, we introduce a comprehensive
UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The
benchmark evaluates the uncertainty estimation capabilities of two large
language models and one pre-trained language model on three datasets, with
human-annotation analysis incorporated where applicable. We also assess the
performance of 14 common uncertainty estimation methods within this benchmark.
Our findings emphasize the importance of considering multiple uncorrelated NLG
metrics and diverse uncertainty estimation methods to ensure reliable and
efficient evaluation of UE-TS techniques. Our code and data are available
https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 41 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist
  with Tertiary Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyao Lan, Bingrui Jin, Zichen Zhu, Siyuan Chen, Shu Zhang, Kenny Q. Zhu, Mengyue Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health issues, particularly depressive disorders, present significant
challenges in contemporary society, necessitating the development of effective
automated diagnostic methods. This paper introduces the Agent Mental Clinic
(AMC), a self-improving conversational agent system designed to enhance
depression diagnosis through simulated dialogues between patient and
psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we
design a psychiatrist agent consisting of a tertiary memory structure, a
dialogue control and reflect plugin that acts as ``supervisor'' and a memory
sampling module, fully leveraging the skills reflected by the psychiatrist
agent, achieving great accuracy on depression risk and suicide risk diagnosis
via conversation. Experiment results on datasets collected in real-life
scenarios demonstrate that the system, simulating the procedure of training
psychiatrists, can be a promising optimization method for aligning LLMs with
real-life distribution in specific domains without modifying the weights of
LLMs, even when only a few representative labeled cases are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Object-Oriented Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Wang, Ziqian Bi, Keyu Chen, Jiawei Xu, Qian Niu, Junyu Liu, Benji Peng, Ming Li, Sen Zhang, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Caitlyn Heqi Yin, Yizhu Wen, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Oriented Programming (OOP) has become a crucial paradigm for managing
the growing complexity of modern software systems, particularly in fields like
machine learning, deep learning, large language models (LLM), and data
analytics. This work provides a comprehensive introduction to the integration
of OOP techniques within these domains, with a focus on improving code
modularity, maintainability, and scalability. We begin by outlining the
evolution of computing and the rise of OOP, followed by an in-depth discussion
of key OOP principles such as encapsulation, inheritance, polymorphism, and
abstraction. The practical application of these principles is demonstrated
using Python, a widely adopted language in AI and data science. Furthermore, we
examine how design patterns and modular programming can be employed to enhance
the structure and efficiency of machine learning systems. In subsequent
sections, we apply these OOP concepts to real-world AI tasks, including the
encapsulation of preprocessing workflows, machine learning model training, and
evaluation. Detailed examples illustrate how OOP can be used to build reusable,
scalable machine learning systems while maintaining code clarity and reducing
redundancy.This work is intended to serve as a bridge for both beginners and
experienced developers, equipping them with the necessary knowledge to apply
OOP methodologies in AI-driven projects, ultimately fostering the development
of more robust and maintainable systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: High-value Data Selection for Visual Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning is the key to building large vision language
models~(LVLMs), which can greatly improve the task generalization and solving
capabilities by learning a mixture of instruction data from diverse visual
tasks. Previous work mostly collects multiple existing visual instruction
datasets via heuristic ways for training (even more than a million
instructions), which may introduce data redundancy and enlarge the training
cost. To investigate this issue, we conduct a series of empirical studies,
which reveal a significant redundancy within the visual instruction datasets,
and show that greatly reducing the amount of instructions from several tasks
even do not affect the performance. Based on the findings, we propose a
high-value data selection approach TIVE, to eliminate redundancy within the
visual instruction data and reduce the training cost. In TIVE, we first
estimate the instance influence score on its corresponding task, and the task
difficulty score, based on the gradient-based influence functions. Then, we
leverage the two kinds of scores to determine the task proportion within the
selected visual instruction subset, and select high-value instances for each
task, respectively. Experiments on various LVLMs show that our approach using
only about 15% data can achieve comparable average performance to the full-data
fine-tuned model across eight benchmarks, even surpassing it on four of the
benchmarks. Our code and data will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Editing with LLM-based Tool Chaining: An Efficient Distillation
  Approach for Real-Time Applications <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Sultan, Alex Khasin, Guy Shiran, Asnat Greenstein-Messica, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a practical distillation approach to fine-tune LLMs for invoking
tools in real-time applications. We focus on visual editing tasks;
specifically, we modify images and videos by interpreting user stylistic
requests, specified in natural language ("golden hour"), using an LLM to select
the appropriate tools and their parameters to achieve the desired visual
effect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in
this task, but their high cost and latency make them unsuitable for real-time
applications. In our approach, we fine-tune a (smaller) student LLM with
guidance from a (larger) teacher LLM and behavioral signals. We introduce
offline metrics to evaluate student LLMs. Both online and offline experiments
show that our student models manage to match the performance of our teacher
model (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we
show that fine-tuning was improved by 25% in low-data regimes using
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits of <span class="highlight-title">Transformer</span> Language Models on Learning to Compose Algorithms <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05785v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05785v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Thomm, Aleksandar Terzic, Giacomo Camposampiero, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the capabilities of Transformer language models in learning
compositional discrete tasks. To this end, we evaluate training LLaMA models
and prompting GPT-4 and Gemini on four tasks demanding to learn a composition
of several discrete sub-tasks. On both training LLaMA models from scratch and
prompting on GPT-4 and Gemini, we measure how well these models can reuse
primitives observable in the sub-tasks to learn the composition task. Our
results indicate that compositional learning in state-of-the-art Transformer
language models is highly sample inefficient: LLaMA requires more data samples
than relearning all sub-tasks from scratch to learn the compositional task;
in-context prompting with few samples is unreliable and fails at executing the
sub-tasks or correcting the errors in multi-round code generation. Further, by
leveraging complexity theory, we support these findings with a theoretical
analysis focused on the sample inefficiency of gradient descent in memorizing
feedforward models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoHallusion: Automatic Ge<span class="highlight-title">ner</span>ation of Hallucination Benchmarks for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, Tianyi Zhou, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) are prone to hallucinations, where
certain contextual cues in an image can trigger the language module to produce
overconfident and incorrect reasoning about abnormal or hypothetical objects.
While some benchmarks have been developed to investigate LVLM hallucinations,
they often rely on hand-crafted corner cases whose failure patterns may not
generalize well. Additionally, fine-tuning on these examples could undermine
their validity. To address this, we aim to scale up the number of cases through
an automated approach, reducing human bias in crafting such corner cases. This
motivates the development of AutoHallusion, the first automated benchmark
generation approach that employs several key strategies to create a diverse
range of hallucination examples. Our generated visual-question pairs pose
significant challenges to LVLMs, requiring them to overcome contextual biases
and distractions to arrive at correct answers. AutoHallusion enables us to
create new benchmarks at the minimum cost and thus overcomes the fragility of
hand-crafted benchmarks. It also reveals common failure patterns and reasons,
providing key insights to detect, avoid, or control hallucinations.
Comprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro
Vision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of
hallucination induction on synthetic and real-world datasets of AutoHallusion,
paving the way for a long battle against hallucinations. The codebase and data
can be accessed at https://github.com/wuxiyang1996/AutoHallusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rebuilding ROME : Resolving Model Collapse during Sequential Model
  Editing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07175v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07175v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Sidharth Baskaran, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work using Rank-One Model Editing (ROME), a popular model editing
method, has shown that there are certain facts that the algorithm is unable to
edit without breaking the model. Such edits have previously been called
disabling edits. These disabling edits cause immediate model collapse and
limits the use of ROME for sequential editing. In this paper, we show that
disabling edits are an artifact of irregularities in the implementation of
ROME. With this paper, we provide a more stable implementation ROME, which we
call r-ROME and show that model collapse is no longer observed when making
large scale sequential edits with r-ROME, while further improving
generalization and locality of model editing compared to the original
implementation of ROME. We also provide a detailed mathematical explanation of
the reason behind disabling edits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Geometry of Categorical and Hierarchical Concepts in Large Language
  Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The linear representation hypothesis is the informal idea that semantic
concepts are encoded as linear directions in the representation spaces of large
language models (LLMs). Previous work has shown how to make this notion precise
for representing binary concepts that have natural contrasts (e.g., {male,
female}) as directions in representation space. However, many natural concepts
do not have natural contrasts (e.g., whether the output is about an animal). In
this work, we show how to extend the formalization of the linear representation
hypothesis to represent features (e.g., is_animal) as vectors. This allows us
to immediately formalize the representation of categorical concepts as
polytopes in the representation space. Further, we use the formalization to
prove a relationship between the hierarchical structure of concepts and the
geometry of their representations. We validate these theoretical results on the
Gemma and LLaMA-3 large language models, estimating representations for 900+
hierarchically related concepts using data from WordNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best Paper Award at the ICML 2024 Workshop on Mechanistic
  Interpretability. Code is available at
  https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Model Editing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14236v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14236v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ROME and MEMIT are largely believed to be two different model editing
algorithms, with the major difference between them being the ability to perform
batched edits. In this paper, we unify these two algorithms under a single
conceptual umbrella, optimizing for the same goal, which we call the
preservation-memorization objective. ROME uses an equality constraint to
optimize this objective to perform one edit at a time, whereas MEMIT employs a
more flexible least-square constraint that allows for batched edits. We
generalize ROME and enable batched editing with equality constraint in the form
of EMMET - an Equality-constrained Mass Model Editing algorithm for
Transformers, a new batched memory-editing algorithm. EMMET can perform
batched-edits up to a batch-size of 10,000, with very similar performance to
MEMIT across multiple dimensions. With the introduction of EMMET, we truly
unify ROME and MEMIT and show that both algorithms are equivalent in terms of
their optimization objective, their abilities (singular and batched editing),
their model editing performance and their limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encouraging Divergent Thinking in <span class="highlight-title">Large Language Model</span>s through
  Multi-Agent Debate <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19118v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19118v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs) like ChatGPT have shown remarkable
performance on general language tasks but still struggle on complex reasoning
tasks, which drives the research on cognitive behaviors of LLMs to explore
human-like problem-solving strategies. Along this direction, one representative
strategy is self-reflection, which asks an LLM to refine the solution with the
feedback generated by itself iteratively. However, our study shows that such
reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:
once the LLM has established confidence in its solutions, it is unable to
generate novel thoughts later through reflection even if its initial stance is
incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)
framework, in which multiple agents express their arguments in the state of
"tit for tat" and a judge manages the debate process to obtain a final
solution. Clearly, our MAD framework encourages divergent thinking in LLMs
which would be helpful for tasks that require deep levels of contemplation.
Experiment results on two challenging datasets, commonsense machine translation
and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of
our MAD framework. Extensive analyses suggest that the adaptive break of debate
and the modest level of "tit for tat" state are required for MAD to obtain good
performance. Moreover, we find that LLMs might not be a fair judge if different
LLMs are used for agents. Code is available at
https://github.com/Skytliang/Multi-Agents-Debate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decompose and Compare Consistency: Measuring VLMs' Answer Reliability
  via Task-Decomposition Consistency Comparison <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Yang, Weixiang Yan, Aishwarya Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite tremendous advancements, current state-of-the-art Vision-Language
Models (VLMs) are still far from perfect. They tend to hallucinate and may
generate biased responses. In such circumstances, having a way to assess the
reliability of a given response generated by a VLM is quite useful. Existing
methods, such as estimating uncertainty using answer likelihoods or
prompt-based confidence generation, often suffer from overconfidence. Other
methods use self-consistency comparison but are affected by confirmation
biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC)
for reliability measurement. By comparing the consistency between the direct
answer generated using the VLM's internal reasoning process, and the indirect
answers obtained by decomposing the question into sub-questions and reasoning
over the sub-answers produced by the VLM, DeCC measures the reliability of
VLM's direct answer. Experiments across six vision-language tasks with three
VLMs show DeCC's reliability estimation achieves better correlation with task
accuracy compared to the existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simultaneous Masking, Not <span class="highlight-title">Prompt</span>ing Optimization: A Paradigm Shift in
  Fine-tuning LLMs for Simultaneous Translation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10443v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10443v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Raffel, Victor Agostinelli, Lizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved state-of-the-art performance in
various language processing tasks, motivating their adoption in simultaneous
translation. Current fine-tuning methods to adapt LLMs for simultaneous
translation focus on prompting optimization strategies using either data
augmentation or prompt structure modifications. However, these methods suffer
from several issues, such as unnecessarily expanded training sets,
computational inefficiency from dumping the key and value cache, increased
prompt sizes, or restriction to a single decision policy. To eliminate these
issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs
for simultaneous translation. It utilizes a novel attention mask approach that
models simultaneous translation during fine-tuning by masking attention for a
desired decision policy. Applying the proposed SimulMask on a Falcon LLM for
the IWSLT 2017 dataset, we have observed a significant translation quality
improvement compared to state-of-the-art prompting optimization strategies on
five language pairs while reducing the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12327v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12327v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in GPU computational power has outpaced memory capacity
and bandwidth growth, creating bottlenecks in Large Language Model (LLM)
inference. Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but it suffers from significant
performance degradation below 4-bit precision. This paper addresses these
challenges by investigating the pretraining of low-bitwidth models specifically
Ternary Language Models (TriLMs) as an alternative to traditional
floating-point models (FloatLMs) and their post-training quantized versions
(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning
multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M
to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation
demonstrates that TriLMs offer superior scaling behavior in terms of model size
(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs
consistently outperform their QuantLM and FloatLM counterparts for a given bit
size across various benchmarks. Notably, the 3.9B parameter TriLM matches the
performance of the FloatLM 3.9B across all benchmarks, despite having fewer
bits than FloatLM 830M. Overall, this research provides valuable insights into
the feasibility and scalability of low-bitwidth language models, paving the way
for the development of more efficient LLMs.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 21 figures, and 13 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-08T00:00:00Z">2024-10-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stress Detection on Code-Mixed Texts in Dravidian Languages using
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L. Ramos, M. Shahiki-Tash, Z. Ahani, A. Eponon, O. Kolesnikova, H. Calvo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stress is a common feeling in daily life, but it can affect mental well-being
in some situations, the development of robust detection models is imperative.
This study introduces a methodical approach to the stress identification in
code-mixed texts for Dravidian languages. The challenge encompassed two
datasets, targeting Tamil and Telugu languages respectively. This proposal
underscores the importance of using uncleaned text as a benchmark to refine
future classification methodologies, incorporating diverse preprocessing
techniques. Random Forest algorithm was used, featuring three textual
representations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams
of characters. The approach achieved a good performance for both linguistic
categories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,
overpassing results achieved with different complex techniques such as FastText
and Transformer models. The results underscore the value of uncleaned data for
mental state detection and the challenges classifying code-mixed texts for
stress, indicating the potential for improved performance through cleaning
data, other preprocessing techniques, or more complex models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLP Case Study on Predicting the Before and After of the Ukraine-Russia
  and Hamas-Israel Conflicts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Miner, John E. Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to predict toxicity and other textual attributes through
the use of natural language processing (NLP) techniques for two recent events:
the Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis
for exploration in future conflicts with hopes to mitigate risk through the
analysis of social media before and after a conflict begins. Our work compiles
several datasets from Twitter and Reddit for both conflicts in a before and
after separation with an aim of predicting a future state of social media for
avoidance. More specifically, we show that: (1) there is a noticeable
difference in social media discussion leading up to and following a conflict
and (2) social media discourse on platforms like Twitter and Reddit is useful
in identifying future conflicts before they arise. Our results show that
through the use of advanced NLP techniques (both supervised and unsupervised)
toxicity and other attributes about language before and after a conflict is
predictable with a low error of nearly 1.2 percent for both conflicts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The clusters created using topic modeling can be viewed at
  https://naturallang.com/conflict/conflict.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERVQA: A <span class="highlight-title">Dataset</span> to Benchmark the Readiness of Large Vision Language
  Models in Hospital Environments <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourjyadip Ray, Kushal Gupta, Soumi Kundu, Payal Arvind Kasat, Somak Aditya, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global shortage of healthcare workers has demanded the development of
smart healthcare assistants, which can help monitor and alert healthcare
workers when necessary. We examine the healthcare knowledge of existing Large
Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in
hospital settings through expert annotated open-ended questions. We introduce
the Emergency Room Visual Question Answering (ERVQA) dataset, consisting of
<image, question, answer> triplets covering diverse emergency room scenarios, a
seminal benchmark for LVLMs. By developing a detailed error taxonomy and
analyzing answer trends, we reveal the nuanced nature of the task. We benchmark
state-of-the-art open-source and closed LVLMs using traditional and adapted VQA
metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across
models, we infer trends based on properties like decoder type, model size, and
in-context examples. Our findings suggest the ERVQA dataset presents a highly
complex task, highlighting the need for specialized, domain-specific solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirelle Bueno, Roberto Lotufo, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are now capable of solving tasks that require dealing with
long sequences consisting of hundreds of thousands of tokens. However, they
often fail on tasks that require repetitive use of simple rules, even on
sequences that are much shorter than those seen during training. For example,
state-of-the-art LLMs can find common items in two lists with up to 20 items
but fail when lists have 80 items. In this paper, we introduce MLissard, a
multilingual benchmark designed to evaluate models' abilities to process and
generate texts of varied lengths and offers a mechanism for controlling
sequence complexity.
  Our evaluation of open-source and proprietary models show a consistent
decline in performance across all models and languages as the complexity of the
sequence increases. Surprisingly, the use of in-context examples in languages
other than English helps increase extrapolation performance significantly. The
datasets and code are available at https://github.com/unicamp-dl/Lissard
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GenBench Workshop by EMNLP 2024: Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Causal Inference in Natural Language with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Jože M. Rožanec, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal structure discovery methods are commonly applied to structured data
where the causal variables are known and where statistical testing can be used
to assess the causal relationships. By contrast, recovering a causal structure
from unstructured natural language data such as news articles contains numerous
challenges due to the absence of known variables or counterfactual data to
estimate the causal links. Large Language Models (LLMs) have shown promising
results in this direction but also exhibit limitations. This work investigates
LLM's abilities to build causal graphs from text documents and perform
counterfactual causal inference. We propose an end-to-end causal structure
discovery and causal inference method from natural language: we first use an
LLM to extract the instantiated causal variables from text data and build a
causal graph. We merge causal graphs from multiple data sources to represent
the most exhaustive set of causes possible. We then conduct counterfactual
inference on the estimated graph. The causal graph conditioning allows
reduction of LLM biases and better represents the causal estimands. We use our
method to show that the limitations of LLMs in counterfactual causal reasoning
come from prediction errors and propose directions to mitigate them. We
demonstrate the applicability of our method on real-world news articles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 pages for the main paper, 12 pages for the references
  and appendix, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Validation of the Scientific Literature via Chemputation Augmented by
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Pagel, Michael Jirasek, Leroy Cronin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemputation is the process of programming chemical robots to do experiments
using a universal symbolic language, but the literature can be error prone and
hard to read due to ambiguities. Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains, including natural language
processing, robotic control, and more recently, chemistry. Despite significant
advancements in standardizing the reporting and collection of synthetic
chemistry data, the automatic reproduction of reported syntheses remains a
labour-intensive task. In this work, we introduce an LLM-based chemical
research agent workflow designed for the automatic validation of synthetic
literature procedures. Our workflow can autonomously extract synthetic
procedures and analytical data from extensive documents, translate these
procedures into universal XDL code, simulate the execution of the procedure in
a hardware-specific setup, and ultimately execute the procedure on an
XDL-controlled robotic system for synthetic chemistry. This demonstrates the
potential of LLM-based workflows for autonomous chemical synthesis with
Chemputers. Due to the abstraction of XDL this approach is safe, secure, and
scalable since hallucinations will not be chemputable and the XDL can be both
verified and encrypted. Unlike previous efforts, which either addressed only a
limited portion of the workflow, relied on inflexible hard-coded rules, or
lacked validation in physical systems, our approach provides four realistic
examples of syntheses directly executed from synthetic literature. We
anticipate that our workflow will significantly enhance automation in
robotically driven synthetic chemistry research, streamline data extraction,
improve the reproducibility, scalability, and safety of synthetic and
experimental chemistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures, 34 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumVI: A Multilingual <span class="highlight-title">Dataset</span> for Detecting Violent Incidents Impacting
  Humanitarian Aid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemank Lamba, Anton Abilov, Ke Zhang, Elizabeth M. Olson, Henry k. Dambanemuya, João c. Bárcia, David S. Batista, Christina Wille, Aoife Cahill, Joel Tetreault, Alex Jaimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanitarian organizations can enhance their effectiveness by analyzing data
to discover trends, gather aggregated insights, manage their security risks,
support decision-making, and inform advocacy and funding proposals. However,
data about violent incidents with direct impact and relevance for humanitarian
aid operations is not readily available. An automatic data collection and
NLP-backed classification framework aligned with humanitarian perspectives can
help bridge this gap. In this paper, we present HumVI - a dataset comprising
news articles in three languages (English, French, Arabic) containing instances
of different types of violent incidents categorized by the humanitarian sector
they impact, e.g., aid security, education, food security, health, and
protection. Reliable labels were obtained for the dataset by partnering with a
data-backed humanitarian organization, Insecurity Insight. We provide multiple
benchmarks for the dataset, employing various deep learning architectures and
techniques, including data augmentation and mask loss, to address different
task-related challenges, e.g., domain expansion. The dataset is publicly
available at https://github.com/dataminr-ai/humvi-dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markovian <span class="highlight-title">Transformer</span>s for Informative Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning holds great promise for explaining the
outputs of language models, but recent studies have highlighted significant
challenges in its practical application for interpretability. We propose to
address this issue via two key components: a technique to factor next-token
prediction through intermediate CoT text, ensuring the CoT is causally
load-bearing, and a reinforcement learning approach to train CoT to predict
future tokens independently of other context. This results in "Markovian"
language models, where CoT serves as a fixed-size state for future token
prediction. Our approach optimizes for "informativeness" -- the improvement in
next-token predictions using a trained CoT compared to a baseline. We
demonstrate our method's effectiveness using Proximal Policy Optimization (PPO)
on arithmetic problems and achieve an 11% performance boost on the GSM8K
benchmark using Mistral 7B Inst V2. The increased sensitivity of model
performance to CoT perturbations provides strong evidence of CoT reliance. This
work advances the development of more transparent and interpretable language
models, potentially enabling their extension to arbitrarily long contexts and
enhancing AI reasoning capabilities across various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Rewards Can Self-Train Dialogue Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barrett Martin Lattimer, Varun Gangal, Ryan McDonald, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)
agents, especially in multi-turn dialogue tasks, have been primarily driven by
supervised fine-tuning and high-quality human feedback. However, as base LLM
models continue to improve, acquiring meaningful human feedback has become
increasingly challenging and costly. In certain domains, base LLM agents may
eventually exceed human capabilities, making traditional feedback-driven
methods impractical. In this paper, we introduce a novel self-improvement
paradigm that empowers LLM agents to autonomously enhance their performance
without external human feedback. Our method, Juxtaposed Outcomes for Simulation
Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward
simulation environment to extract ideal behaviors and further train the LLM on
its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation
environment derived from MultiWOZ. We demonstrate that models trained with
JOSH, both small and frontier, significantly improve tool-based interactions
while preserving general model capabilities across diverse benchmarks. Our code
and data are publicly available on GitHub at
https://github.com/asappresearch/josh-llm-simulation-training
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor but nontrivial changes likely</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Poisoning Attacks on Reward Model Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang, Yevgeniy Vorobeychik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning reward models from pairwise comparisons is a fundamental component
in a number of domains, including autonomous control, conversational agents,
and recommendation systems, as part of a broad goal of aligning automated
decisions with user preferences. These approaches entail collecting preference
information from people, with feedback often provided anonymously. Since
preferences are subjective, there is no gold standard to compare against; yet,
reliance of high-impact systems on preference learning creates a strong
motivation for malicious actors to skew data collected in this fashion to their
ends. We investigate the nature and extent of this vulnerability by considering
an attacker who can flip a small subset of preference comparisons to either
promote or demote a target outcome. We propose two classes of algorithmic
approaches for these attacks: a gradient-based framework, and several variants
of rank-by-distance methods. Next, we evaluate the efficacy of best attacks in
both these classes in successfully achieving malicious goals on datasets from
three domains: autonomous control, recommendation system, and textual
prompt-response preference learning. We find that the best attacks are often
highly successful, achieving in the most extreme case 100\% success rate with
only 0.3\% of the data poisoned. However, \emph{which} attack is best can vary
significantly across domains. In addition, we observe that the simpler and more
scalable rank-by-distance approaches are often competitive with, and on
occasion significantly outperform, gradient-based methods. Finally, we show
that state-of-the-art defenses against other classes of poisoning attacks
exhibit limited efficacy in our setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-07T00:00:00Z">2024-10-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">142</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Partially-Defined Events in Multimodal Data <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 9 pages; 2024 EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A PTQ method to significantly boost the performance of static
  activation quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TurtleBench: Evaluating Top Language Models via Real-World Yes/No
  Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the application of Large Language Models (LLMs) expands, the demand for
reliable evaluations increases. Existing LLM evaluation benchmarks primarily
rely on static datasets, making it challenging to assess model performance in
dynamic interactions with users. Moreover, these benchmarks often depend on
specific background knowledge, complicating the measurement of a model's
logical reasoning capabilities. Other dynamic evaluation methods based on
strong models or manual efforts may introduce biases and incur high costs and
time demands, hindering large-scale application. To address these issues, we
propose TurtleBench. TurtleBench collects real user guesses from our online
Turtle Soup Puzzle platform that we developed. This approach allows for the
relatively dynamic generation of evaluation datasets, mitigating the risk of
model cheating while aligning assessments more closely with genuine user needs
for reasoning capabilities, thus enhancing the reliability of evaluations.
TurtleBench includes 1,532 user guesses along with the correctness of guesses
after annotation. Using this dataset, we thoroughly evaluated nine of the most
advanced LLMs available today. Notably, the OpenAI o1 series models did not
achieve leading results in these evaluations. We propose several hypotheses for
further research, such as "the latent reasoning of o1 utilizes trivial
Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides
reasoning benefits but also incurs noise costs."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and Benchmark for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuneVLSeg: <span class="highlight-title">Prompt</span> Tuning Benchmark for Vision-Language Segmentation
  Models <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACCV 2024 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CasiMedicos-Arg: A Medical Question Answering <span class="highlight-title">Dataset</span> Annotated with
  Explanatory Argumentative Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        katerina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cookbook: A framework for improving LLM ge<span class="highlight-title">ner</span>ative abilities via
  programmatic data ge<span class="highlight-title">ner</span>ating templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model Benchmarking with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density estimation with LLMs: a geometric investigation of in-context
  learning trajectories <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of <span class="highlight-title">Pre-train</span>ed VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying and Mitigating Biases in Sign Language Understanding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Danielle Bragg, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RevisEval: Improving LLM-as-a-Judge via Response-Adapted References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With significant efforts in recent studies, LLM-as-a-Judge has become a
cost-effective alternative to human evaluation for assessing the text
generation quality in a wide range of tasks. However, there still remains a
reliability gap between LLM-as-a-Judge and human evaluation. One important
reason is the lack of guided oracles in the evaluation process. Motivated by
the role of reference pervasively used in classic text evaluation, we introduce
RevisEval, a novel text generation evaluation paradigm via the response-adapted
references. RevisEval is driven by the key observation that an ideal reference
should maintain the necessary relevance to the response to be evaluated.
Specifically, RevisEval leverages the text revision capabilities of large
language models (LLMs) to adaptively revise the response, then treat the
revised text as the reference (response-adapted reference) for the subsequent
evaluation. Extensive experiments demonstrate that RevisEval outperforms
traditional reference-free and reference-based evaluation paradigms that use
LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.
More importantly, our response-adapted references can further boost the
classical text metrics, e.g., BLEU and BERTScore, compared to traditional
references and even rival the LLM-as-a-Judge. A detailed analysis is also
conducted to confirm RevisEval's effectiveness in bias reduction, the impact of
inference cost, and reference relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss
  Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Correlation: Interpretable Evaluation of Machine Translation
  Metrics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Perrella, Lorenzo Proietti, Pere-Lluís Huguet Cabot, Edoardo Barba, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) evaluation metrics assess translation quality
automatically. Recently, researchers have employed MT metrics for various new
use cases, such as data filtering and translation re-ranking. However, most MT
metrics return assessments as scalar scores that are difficult to interpret,
posing a challenge to making informed design choices. Moreover, MT metrics'
capabilities have historically been evaluated using correlation with human
judgment, which, despite its efficacy, falls short of providing intuitive
insights into metric performance, especially in terms of new metric use cases.
To address these issues, we introduce an interpretable evaluation framework for
MT metrics. Within this framework, we evaluate metrics in two scenarios that
serve as proxies for the data filtering and translation re-ranking use cases.
Furthermore, by measuring the performance of MT metrics using Precision,
Recall, and F-score, we offer clearer insights into their capabilities than
correlation with human judgments. Finally, we raise concerns regarding the
reliability of manually curated data following the Direct Assessments+Scalar
Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with
Multidimensional Quality Metrics (MQM) annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference. 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Equity in <span class="highlight-title">Large Language Model</span>s for Medical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Wenhe Ma, Sonish Sivarajkumar, Hang Zhang, Eugene Mathew Sadhu, Zhuochun Li, Xizhi Wu, Shyam Visweswaran, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have highlighted the potential of large language models
(LLMs) in medical applications, notably in automating Clinical Trial Matching
for translational research and providing medical question-answering for
clinical decision support. However, our study reveals significant inequities in
the use of LLMs, particularly for individuals from specific racial, gender, and
underrepresented groups influenced by social determinants of health. These
disparities could worsen existing health inequities if LLMs are broadly adopted
in healthcare. To address this, we propose and evaluate a novel framework,
EquityGuard, designed to detect and mitigate biases in LLM-based medical
applications. EquityGuard incorporates a Bias Detection Mechanism capable of
identifying and correcting unfair predictions, thus enhancing outcomes and
promoting equity across diverse population groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReasoningRank: Teaching Student Models to Rank through Reasoning-Based
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking documents based on their relevance to a given query is critical in
information retrieval. Traditional reranking methods often focus on improving
the initial rankings but lack transparency, failing to explain why one document
is ranked higher. In this paper, we introduce ReasoningRank, a novel reranking
approach that enhances clarity by generating two types of reasoning: explicit
reasoning, which explains how a document addresses the query, and comparison
reasoning, which justifies the relevance of one document over another. We
leverage large language models (LLMs) as teacher models to generate these
explanations and distill this knowledge into smaller, more resource-efficient
student models. While the student models may not outperform LLMs in speed, they
significantly reduce the computational burden by requiring fewer resources,
making them more suitable for large-scale or resource-constrained settings.
These student models are trained to both generate meaningful reasoning and
rerank documents, achieving competitive performance across multiple datasets,
including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank
improves reranking accuracy and provides valuable insights into the
decision-making process, offering a structured and interpretable solution for
reranking tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Inference for <span class="highlight-title">Large Language Model</span>-based Ge<span class="highlight-title">ner</span>ative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based generative recommendation has achieved
notable success, yet its practical deployment is costly particularly due to
excessive inference latency caused by autoregressive decoding. For lossless LLM
decoding acceleration, Speculative Decoding (SD) has emerged as a promising
solution. However, applying SD to generative recommendation presents unique
challenges due to the requirement of generating top-K items (i.e., K distinct
token sequences) as a recommendation list by beam search. This leads to more
stringent verification in SD, where all the top-K sequences from the target LLM
must be successfully drafted by the draft model at each decoding step. To
alleviate this, we consider 1) boosting top-K sequence alignment between the
draft model and the target LLM, and 2) relaxing the verification strategy to
reduce trivial LLM calls. To this end, we propose an alignment framework named
AtSpeed, which presents the AtSpeed-S optimization objective for top-K
alignment under the strict top-K verification. Moreover, we introduce a relaxed
sampling verification strategy that allows high-probability non-top-K drafted
sequences to be accepted, significantly reducing LLM calls. Correspondingly, we
propose AtSpeed-R for top-K alignment under this relaxed sampling verification.
Empirical results on two real-world datasets demonstrate that AtSpeed
significantly accelerates LLM-based generative recommendation, e.g., near 2x
speedup under strict top-K verification and up to 2.5 speedup under relaxed
sampling verification. The codes and datasets will be released in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Farahani, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training Vision-Language Models for Massive Multimodal
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTC-GMM: CTC guided modality matching for fast and accurate streaming
  speech translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Jinyu Li, Ruchao Fan, Matt Post
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for streaming speech translation (ST) can achieve high accuracy and
low latency if they're developed with vast amounts of paired audio in the
source language and written text in the target language. Yet, these text labels
for the target language are often pseudo labels due to the prohibitive cost of
manual ST data labeling. In this paper, we introduce a methodology named
Connectionist Temporal Classification guided modality matching (CTC-GMM) that
enhances the streaming ST model by leveraging extensive machine translation
(MT) text data. This technique employs CTC to compress the speech sequence into
a compact embedding sequence that matches the corresponding text sequence,
allowing us to utilize matched {source-target} language text pairs from the MT
corpora to refine the streaming ST model further. Our evaluations with FLEURS
and CoVoST2 show that the CTC-GMM approach can increase translation accuracy
relatively by 13.9% and 6.4% respectively, while also boosting decoding speed
by 59.7% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of LLMs via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating <span class="highlight-title">large language model</span>s for their competence in extracting
  grammatically sound sentences from transcribed noisy utterances <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selectively processing noisy utterances while effectively disregarding
speech-specific elements poses no considerable challenge for humans, as they
exhibit remarkable cognitive abilities to separate semantically significant
content from speech-specific noise (i.e. filled pauses, disfluencies, and
restarts). These abilities may be driven by mechanisms based on acquired
grammatical rules that compose abstract syntactic-semantic structures within
utterances. Segments without syntactic and semantic significance are
consistently disregarded in these structures. The structures, in tandem with
lexis, likely underpin language comprehension and thus facilitate effective
communication. In our study, grounded in linguistically motivated experiments,
we investigate whether large language models (LLMs) can effectively perform
analogical speech comprehension tasks. In particular, we examine the ability of
LLMs to extract well-structured utterances from transcriptions of noisy
dialogues. We conduct two evaluation experiments in the Polish language
scenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of
data contamination. Our results show that not all extracted utterances are
correctly structured, indicating that either LLMs do not fully acquire
syntactic-semantic rules or they acquire them but cannot apply them
effectively. We conclude that the ability of LLMs to comprehend noisy
utterances is still relatively superficial compared to human proficiency in
processing them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoNLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explanation sensitivity to the randomness of <span class="highlight-title">large language model</span>s: the
  case of journalistic text classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Bogaert, Marie-Catherine de Marneffe, Antonin Descampe, Louis Escouflaire, Cedrick Fairon, Francois-Xavier Standaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) perform very well in several natural language
processing tasks but raise explainability challenges. In this paper, we examine
the effect of random elements in the training of LLMs on the explainability of
their predictions. We do so on a task of opinionated journalistic text
classification in French. Using a fine-tuned CamemBERT model and an explanation
method based on relevance propagation, we find that training with different
random seeds produces models with similar accuracy but variable explanations.
We therefore claim that characterizing the explanations' statistical
distribution is needed for the explainability of LLMs. We then explore a
simpler model based on textual features which offers stable explanations but is
less accurate. Hence, this simpler model corresponds to a different tradeoff
between accuracy and explainability. We show that it can be improved by
inserting features derived from CamemBERT's explanations. We finally discuss
new research directions suggested by our results, in particular regarding the
origin of the sensitivity observed in the training randomness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a faithful translation of a paper which was
  peer-reviewed and published in the French journal Traitement Automatique des
  Langues, n. 64</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScienceAgentBench: Toward Rigorous Assessment of Language Agents for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense
  Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) have shown strong reasoning capabilities
in commonsense question answering benchmarks, but the process underlying their
success remains largely opaque. As a consequence, recent approaches have
equipped LLMs with mechanisms for knowledge retrieval, reasoning and
introspection, not only to improve their capabilities but also to enhance the
interpretability of their outputs. However, these methods require additional
training, hand-crafted templates or human-written explanations. To address
these issues, we introduce ZEBRA, a zero-shot question answering framework that
combines retrieval, case-based reasoning and introspection and dispenses with
the need for additional training of the LLM. Given an input question, ZEBRA
retrieves relevant question-knowledge pairs from a knowledge base and generates
new knowledge by reasoning over the relationships in these pairs. This
generated knowledge is then used to answer the input question, improving the
model's performance and interpretability. We evaluate our approach across 8
well-established commonsense reasoning benchmarks, demonstrating that ZEBRA
consistently outperforms strong LLMs and previous knowledge integration
approaches, achieving an average accuracy improvement of up to 4.5 points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization of <span class="highlight-title">Large Language Model</span>s via Reparameterization to
  Mitigate Loss Spikes <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Kyosuke Nishida, Kuniko Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a
fundamental issue in the pre-training of large language models. This paper
supposes that the non-uniformity of the norm of the parameters is one of the
causes of loss spikes. Here, in training of neural networks, the scale of the
gradients is required to be kept constant throughout the layers to avoid the
vanishing and exploding gradients problem. However, to meet these requirements
in the Transformer model, the norm of the model parameters must be non-uniform,
and thus, parameters whose norm is smaller are more sensitive to the parameter
update. To address this issue, we propose a novel technique, weight scaling as
reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter
matrix and adjusts it to the value satisfying the requirements. Because of the
gate parameter, WeSaR sets the norm of the original parameters uniformly, which
results in stable training. Experimental results with the Transformer decoders
consisting of 130 million, 1.3 billion, and 13 billion parameters showed that
WeSaR stabilizes and accelerates training and that it outperformed compared
methods including popular initialization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A test suite of <span class="highlight-title">prompt</span> injection attacks for LLM-based machine
  translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Valerio Miceli-Barone, Zhifan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based NLP systems typically work by embedding their input data into
prompt templates which contain instructions and/or in-context examples,
creating queries which are submitted to a LLM, and then parsing the LLM
response in order to generate the system outputs. Prompt Injection Attacks
(PIAs) are a type of subversion of these systems where a malicious user crafts
special inputs which interfere with the prompt templates, causing the LLM to
respond in ways unintended by the system designer.
  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based
machine translation. Specifically, the task is to translate questions from the
TruthfulQA test suite, where an adversarial prompt is prepended to the
questions, instructing the system to ignore the translation instruction and
answer the questions instead.
  In this test suite, we extend this approach to all the language pairs of the
WMT 2024 General Machine Translation task. Moreover, we include additional
attack formats in addition to the one originally studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Named Clinical Entity Recognition Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs plan paths with extra hints from solvers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wu, Sayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing, mathematical problem solving, and tasks related to program
synthesis. However, their effectiveness in long-term planning and higher-order
reasoning has been noted to be limited and fragile. This paper explores an
approach for enhancing LLM performance in solving a classical robotic planning
task by integrating solver-generated feedback. We explore four different
strategies for providing feedback, including visual feedback, we utilize
fine-tuning, and we evaluate the performance of three different LLMs across a
10 standard and 100 more randomly generated planning problems. Our results
suggest that the solver-generated feedback improves the LLM's ability to solve
the moderately difficult problems, but the harder problems still remain out of
reach. The study provides detailed analysis of the effects of the different
hinting strategies and the different planning tendencies of the evaluated LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEPT: Decoupled Embeddings for <span class="highlight-title">Pre-train</span>ing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the "curse of multilinguality". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Biased Assessment of Expert Finding Systems <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large organisations, identifying experts on a given topic is crucial in
leveraging the internal knowledge spread across teams and departments.
So-called enterprise expert retrieval systems automatically discover and
structure employees' expertise based on the vast amount of heterogeneous data
available about them and the work they perform. Evaluating these systems
requires comprehensive ground truth expert annotations, which are hard to
obtain. Therefore, the annotation process typically relies on automated
recommendations of knowledge areas to validate. This case study provides an
analysis of how these recommendations can impact the evaluation of expert
finding systems. We demonstrate on a popular benchmark that system-validated
annotations lead to overestimated performance of traditional term-based
retrieval models and even invalidate comparisons with more recent neural
methods. We also augment knowledge areas with synonyms to uncover a strong bias
towards literal mentions of their constituent words. Finally, we propose
constraints to the annotation process to prevent these biased evaluations, and
show that this still allows annotation suggestions of high utility. These
findings should inform benchmark creation or selection for expert finding, to
guarantee meaningful comparison of methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th Workshop on Recommender Systems for Human
  Resources (RecSys in HR 2024) as part of RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness <span class="chip">ECML-PKDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Thomas Demeester, Chris Develder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling the relationships between skills is a crucial part of
human resources processes such as recruitment and employee development. Yet, no
benchmarks exist to evaluate such methods directly. We construct and release
SkillMatch, a benchmark for the task of skill relatedness, based on expert
knowledge mining from millions of job ads. Additionally, we propose a scalable
self-supervised learning technique to adapt a Sentence-BERT model based on
skill co-occurrence in job ads. This new method greatly surpasses traditional
models for skill relatedness as measured on SkillMatch. By releasing SkillMatch
publicly, we aim to contribute a foundation for research towards increased
accuracy and transparency of skill-based recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International workshop on AI for Human Resources and
  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Rigour of Scientific Writing: Criteria, Analysis, and Insights <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Findings at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Scaling for Steering and Interpreting Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Stoehr, Kevin Du, Vésteinn Snæbjarnarson, Robert West, Ryan Cotterell, Aaron Schein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the prompt "Rome is in", can we steer a language model to flip its
prediction of an incorrect token "France" to a correct token "Italy" by only
multiplying a few relevant activation vectors with scalars? We argue that
successfully intervening on a model is a prerequisite for interpreting its
internal workings. Concretely, we establish a three-term objective: a
successful intervention should flip the correct with the wrong token and vice
versa (effectiveness), and leave other tokens unaffected (faithfulness), all
while being sparse (minimality). Using gradient-based optimization, this
objective lets us learn (and later evaluate) a specific kind of efficient and
interpretable intervention: activation scaling only modifies the signed
magnitude of activation vectors to strengthen, weaken, or reverse the steering
directions already encoded in the model. On synthetic tasks, this intervention
performs comparably with steering vectors in terms of effectiveness and
faithfulness, but is much more minimal allowing us to pinpoint interpretable
model components. We evaluate activation scaling from different angles, compare
performance on different datasets, and make activation scalars a learnable
function of the activation vectors themselves to generalize to varying-length
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent Classification for Bank Chatbots through LLM Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibiána Lajčinová, Patrik Valábek, Michal Spišiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the application of large language models (LLMs) for
intent classification within a chatbot with predetermined responses designed
for banking industry websites. Specifically, the research examines the
effectiveness of fine-tuning SlovakBERT compared to employing multilingual
generative models, such as Llama 8b instruct and Gemma 7b instruct, in both
their pre-trained and fine-tuned versions. The findings indicate that
SlovakBERT outperforms the other models in terms of in-scope accuracy and
out-of-scope false positive rate, establishing it as the benchmark for this
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, no figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Grammar Induction for Language Understanding and Ge<span class="highlight-title">ner</span>ation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammar induction has made significant progress in recent years. However, it
is not clear how the application of induced grammar could enhance practical
performance in downstream tasks. In this work, we introduce an unsupervised
grammar induction method for language understanding and generation. We
construct a grammar parser to induce constituency structures and dependency
relations, which is simultaneously trained on downstream tasks without
additional syntax annotations. The induced grammar features are subsequently
incorporated into Transformer as a syntactic mask to guide self-attention. We
evaluate and apply our method to multiple machine translation tasks and natural
language understanding tasks. Our method demonstrates superior performance
compared to the original Transformer and other models enhanced with external
parsers. Experimental results indicate that our method is effective in both
from-scratch and pre-trained scenarios. Additionally, our research highlights
the contribution of explicitly modeling the grammatical structure of texts to
neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Aware Answer Verification by Pairwise Self-Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akira Kawabata, Saku Sugawara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answer verification identifies correct solutions among candidates generated
by large language models (LLMs). Current approaches typically train verifier
models by labeling solutions as correct or incorrect based solely on whether
the final answer matches the gold answer. However, this approach neglects any
flawed rationale in the solution yielding the correct answer, undermining the
verifier's ability to distinguish between sound and flawed rationales. We
empirically show that in StrategyQA, only 19% of LLM-generated solutions with
correct answers have valid rationales, thus leading to an unreliable verifier.
Furthermore, we demonstrate that training a verifier on valid rationales
significantly improves its ability to distinguish valid and flawed rationale.
To make a better verifier without extra human supervision, we introduce REPS
(Rationale Enhancement through Pairwise Selection), a method for selecting
valid rationales from candidates by iteratively applying pairwise
self-evaluation using the same LLM that generates the solutions. Verifiers
trained on solutions selected by REPS outperform those trained using
conventional training methods on three reasoning benchmarks (ARC-Challenge,
DROP, and StrategyQA). Our results suggest that training reliable verifiers
requires ensuring the validity of rationales in addition to the correctness of
the final answers, which would be critical for models assisting humans in
solving complex reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative
  Feedback Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Wang Chen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has emerged as a more computationally
efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with
Proximal Policy Optimization (PPO), eliminating the need for reward models and
online sampling. Despite these benefits, DPO and its variants remain sensitive
to hyper-parameters and prone to instability, particularly on mathematical
datasets. We argue that these issues arise from the unidirectional
likelihood-derivative negative feedback inherent in the log-likelihood loss
function. To address this, we propose a novel LLM alignment loss that
establishes a stable Bidirectional Negative Feedback (BNF) during optimization.
Our proposed BNF loss eliminates the need for pairwise contrastive losses and
does not require any extra tunable hyper-parameters or pairwise preference
data, streamlining the alignment pipeline to be as simple as supervised
fine-tuning. We conduct extensive experiments across two challenging QA
benchmarks and four reasoning benchmarks. The experimental results show that
BNF achieves comparable performance to the best methods on QA benchmarks, while
its performance decrease on the four reasoning benchmarks is significantly
lower compared to the best methods, thus striking a better balance between
value alignment and reasoning ability. In addition, we further validate the
performance of BNF on non-pairwise datasets, and conduct in-depth analysis of
log-likelihood and logit shifts across different preference optimization
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MI<span class="highlight-title">NER</span>: Mining the Underlying Pattern of Modality-Specific Neurons in
  Multimodal <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichen Huang, Jiahao Huo, Yibo Yan, Kun Wang, Yutao Yue, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have significantly
advanced, integrating more modalities into diverse applications. However, the
lack of explainability remains a major barrier to their use in scenarios
requiring decision transparency. Current neuron-level explanation paradigms
mainly focus on knowledge localization or language- and domain-specific
analyses, leaving the exploration of multimodality largely unaddressed. To
tackle these challenges, we propose MINER, a transferable framework for mining
modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)
modality separation, (2) importance score calculation, (3) importance score
aggregation, (4) modality-specific neuron selection. Extensive experiments
across six benchmarks and two representative MLLMs show that (I) deactivating
ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for
Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly
converge in the lower layers, (III) MSNs influence how key information from
various modalities converges to the last token, (IV) two intriguing phenomena
worth further investigation, i.e., semantic probing and semantic telomeres. The
source code is available at this URL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LPZero: Language Model Zero-cost Proxy Search from Zero 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Dong, Lujun Li, Xiang Liu, Zhenheng Tang, Xuebo Liu, Qiang Wang, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the outstanding performance, Neural Architecture Search (NAS) is
criticized for massive computation. Recently, Zero-shot NAS has emerged as a
promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce
computational demands. Despite this, existing ZC proxies heavily rely on expert
knowledge and incur significant trial-and-error costs. Particularly in NLP
tasks, most existing ZC proxies fail to surpass the performance of the naive
baseline. To address these challenges, we introduce a novel framework,
\textbf{LPZero}, which is the first to automatically design ZC proxies for
various tasks, achieving higher ranking consistency than human-designed
proxies. Specifically, we model the ZC proxy as a symbolic equation and
incorporate a unified proxy search space that encompasses existing ZC proxies,
which are composed of a predefined set of mathematical symbols. To
heuristically search for the best ZC proxy, LPZero incorporates genetic
programming to find the optimal symbolic composition. We propose a
\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates
unpromising proxies, thereby mitigating the risk of proxy degradation.
Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's
superior ranking ability and performance on downstream tasks compared to
current approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 10 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAPE V2: Process Attention Score as Feature Map for Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. arXiv admin note: text overlap with arXiv:2405.14722</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing the Under-Represented: Cultural and Core Capability
  Benchmarks for Developing Thai <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Sukyung Lee, Yungi Kim, Attapol Rutherford, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has highlighted the
need for robust evaluation frameworks that assess their core capabilities, such
as reasoning, knowledge, and commonsense, leading to the inception of certain
widely-used benchmark suites such as the H6 benchmark. However, these benchmark
suites are primarily built for the English language, and there exists a lack
thereof for under-represented languages, in terms of LLM development, such as
Thai. On the other hand, developing LLMs for Thai should also include enhancing
the cultural understanding as well as core capabilities. To address these dual
challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai
Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough
evaluation of various LLMs with multi-lingual capabilities, we provide a
comprehensive analysis of the proposed benchmarks and how they contribute to
Thai LLM development. Furthermore, we will make both the datasets and
evaluation code publicly available to encourage further research and
development for Thai LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formality is Favored: Unraveling the Learning Preferences of Large
  Language Models on Data with Conflicting Knowledge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Li, Yiqing Cao, Shujian Huang, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Having been trained on massive pretraining data, large language models have
shown excellent performance on many knowledge-intensive tasks. However,
pretraining data tends to contain misleading and even conflicting information,
and it is intriguing to understand how LLMs handle these noisy data during
training. In this study, we systematically analyze LLMs' learning preferences
for data with conflicting knowledge. We find that pretrained LLMs establish
learning preferences similar to humans, i.e., preferences towards formal texts
and texts with fewer spelling errors, resulting in faster learning and more
favorable treatment of knowledge in data with such features when facing
conflicts. This finding is generalizable across models and languages and is
more evident in larger models. An in-depth analysis reveals that LLMs tend to
trust data with features that signify consistency with the majority of data,
and it is possible to instill new preferences and erase old ones by
manipulating the degree of consistency with the majority data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by EMNLP 2024, main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImProver: Agent-Based Automated Proof Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate formal proofs of
mathematical theorems in proofs assistants such as Lean. However, we often want
to optimize a formal proof with respect to various criteria, depending on its
downstream use. For example, we may want a proof to adhere to a certain style,
or to be readable, concise, or modularly structured. Having suitably optimized
proofs is also important for learning tasks, especially since human-written
proofs may not optimal for that purpose. To this end, we study a new problem of
automated proof optimization: rewriting a proof so that it is correct and
optimizes for an arbitrary criterion, such as length or readability. As a first
method for automated proof optimization, we present ImProver, a
large-language-model agent that rewrites proofs to optimize arbitrary
user-defined metrics in Lean. We find that naively applying LLMs to proof
optimization falls short, and we incorporate various improvements into
ImProver, such as the use of symbolic Lean context in a novel Chain-of-States
technique, as well as error-correction and retrieval. We test ImProver on
rewriting real-world undergraduate, competition, and research-level mathematics
theorems, finding that ImProver is capable of rewriting proofs so that they are
substantially shorter, more modular, and more readable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-level Causal Relation Extraction with Knowledge-guided Binary
  Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimu Wang, Lei Xia, Wei Wang, Xinya Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in information extraction (IE), Event-Event Causal
Relation Extraction (ECRE) aims to identify and classify the causal
relationships between event mentions in natural language texts. However,
existing research on ECRE has highlighted two critical challenges, including
the lack of document-level modeling and causal hallucinations. In this paper,
we propose a Knowledge-guided binary Question Answering (KnowQA) method with
event structures for ECRE, consisting of two stages: Event Structure
Construction and Binary Question Answering. We conduct extensive experiments
under both zero-shot and fine-tuning settings with large language models (LLMs)
on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the
usefulness of event structures on document-level ECRE and the effectiveness of
KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only
the effectiveness but also the high generalizability and low inconsistency of
our method, particularly when with complete event structures after fine-tuning
the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intriguing Properties of Large Language and Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Yechan Hwang, Ho-Jin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language and vision models (LLVMs) have received significant
attention and development efforts due to their remarkable generalization
performance across a wide range of tasks requiring perception and cognitive
abilities. A key factor behind their success is their simple architecture,
which consists of a vision encoder, a projector, and a large language model
(LLM). Despite their achievements in advanced reasoning tasks, their
performance on fundamental perception-related tasks (e.g., MMVP) remains
surprisingly low. This discrepancy raises the question of how LLVMs truly
perceive images and exploit the advantages of the vision encoder. To address
this, we systematically investigate this question regarding several aspects:
permutation invariance, robustness, math reasoning, alignment preserving and
importance, by evaluating the most common LLVM's families (i.e., LLaVA) across
10 evaluation benchmarks. Our extensive experiments reveal several intriguing
properties of current LLVMs: (1) they internally process the image in a global
manner, even when the order of visual patch sequences is randomly permuted; (2)
they are sometimes able to solve math problems without fully perceiving
detailed numerical information; (3) the cross-modal alignment is overfitted to
complex reasoning tasks, thereby, causing them to lose some of the original
perceptual capabilities of their vision encoder; (4) the representation space
in the lower layers (<25%) plays a crucial role in determining performance and
enhancing visual understanding. Lastly, based on the above observations, we
suggest potential future directions for building better LLVMs and constructing
more challenging evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available in https://github.com/passing2961/IP-LLVM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableRAG: Million-Token Table Understanding with Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-An Chen, Lesly Miculicich, Julian Martin Eisenschlos, Zifeng Wang, Zilong Wang, Yanfei Chen, Yasuhisa Fujii, Hsuan-Tien Lin, Chen-Yu Lee, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TLDR: Token-Level Detective Reward Model for Large Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although reward models have been successful in improving multimodal large
language models, the reward models themselves remain brutal and contain minimal
information. Notably, existing reward models only mimic human annotations by
assigning only one binary feedback to any text, no matter how long the text is.
In the realm of multimodal language models, where models are required to
process both images and texts, a naive reward model may learn implicit biases
toward texts and become less grounded in images. In this paper, we propose a
$\textbf{T}$oken-$\textbf{L}$evel $\textbf{D}$etective $\textbf{R}$eward Model
($\textbf{TLDR}$) to provide fine-grained annotations to each text token. We
first introduce a perturbation-based method to generate synthetic hard
negatives and their token-level labels to train TLDR models. Then we show the
rich usefulness of TLDR models both in assisting off-the-shelf models to
self-correct their generations, and in serving as a hallucination evaluation
tool. Finally, we show that TLDR models can significantly speed up human
annotation by 3 times to acquire a broader range of high-quality vision
language data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work done at Meta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">transformer</span> with reinforced position embedding for language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Che Hsiao, Abhishek Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an efficient transformer architecture that uses
reinforced positional embedding to obtain superior performance with half the
number of encoder decoder layers. We demonstrate that concatenating positional
encoding with trainable token embeddings, normalizing columns in the token
embedding matrix, and using the normalized token embedding matrix as the value
of the attention layer improve the training and validation loss and the
training time in an encoder-decoder Transformer model for a Portuguese-English
translation task with 10 epochs or 12 hours of training across 10 trials. Our
method, with roughly a threefold parameter reduction compared to the baseline
model, yields a mean training loss of 1.21, a mean validation loss of 1.51, and
an average training time of 1352.27 seconds per epoch, surpassing the baseline
model with the same embedding dimension that employs addition of positional
encoding and token embeddings, which achieves a mean training loss of 1.96, a
validation loss of 2.18, and an average training time of 4297.79 seconds per
epoch. Additionally, we evaluated our proposed architecture and the baseline
across 14 diverse translation datasets from TensorFlow. The results indicate
that our method consistently achieves lower or comparable training and
validation losses, suggesting enhanced learning efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forgetting Curve: A Reliable Method for Evaluating Memorization
  Capability for Long-context Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous recent works target to extend effective context length for language
models and various methods, tasks and benchmarks exist to measure model's
effective memorization length. However, through thorough investigations, we
find limitations for currently existing evaluations on model's memorization
capability. We provide an extensive survey for limitations in this work and
propose a new method called forgetting curve to measure the memorization
capability of long-context models. We show that forgetting curve has the
advantage of being robust to the tested corpus and the experimental settings,
of not relying on prompts and can be applied to any model size.
  We apply our forgetting curve to a large variety of models involving both
transformer and RNN/SSM based architectures. Our measurement provides empirical
evidence for the effectiveness of transformer extension techniques while raises
questions for the effective length of RNN/SSM based models. We also examine the
difference between our measurement and existing benchmarks as well as popular
metrics for various models. Our code and results can be found at
https://github.com/1azybug/ForgettingCurve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Ge<span class="highlight-title">ner</span>alization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule-based Data Selection for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of training data significantly impacts the performance of large
language models (LLMs). There are increasing studies using LLMs to rate and
select data based on several human-crafted metrics (rules). However, these
conventional rule-based approaches often depend too heavily on human
heuristics, lack effective metrics for assessing rules, and exhibit limited
adaptability to new tasks. In our study, we introduce an innovative rule-based
framework that utilizes the orthogonality of score vectors associated with
rules as a novel metric for rule evaluations. Our approach includes an
automated pipeline that first uses LLMs to generate a diverse set of rules,
encompassing various rating dimensions to evaluate data quality. Then it rates
a batch of data based on these rules and uses the determinantal point process
(DPP) from random matrix theory to select the most orthogonal score vectors,
thereby identifying a set of independent rules. These rules are subsequently
used to evaluate all data, selecting samples with the highest average scores
for downstream tasks such as LLM training. We verify the effectiveness of our
method through two experimental setups: 1) comparisons with ground truth
ratings and 2) benchmarking LLMs trained with the chosen data. Our
comprehensive experiments cover a range of scenarios, including general
pre-training and domain-specific fine-tuning in areas such as IMDB, Medical,
Math, and Code. The outcomes demonstrate that our DPP-based rule rating method
consistently outperforms other approaches, including rule-free rating, uniform
sampling, importance resampling, and QuRating, in terms of both rating
precision and model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How Hard to Think: Input-Adaptive Allocation of LM Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computationally intensive decoding procedures--including search, reranking,
and self-critique--can improve the quality of language model (LM) outputs in
problems spanning code generation, numerical reasoning, and dialog. Existing
work typically applies the same decoding procedure for every input to an LM.
But not all inputs require the same amount of computation to process. Can we
allocate decoding computation adaptively, using more resources to answer
questions whose answers will be harder to compute? We present an approach that
predicts the distribution of rewards given an input and computation budget,
then allocates additional computation to inputs for which it is predicted to be
most useful. We apply this approach in two decoding procedures: first, an
adaptive best-of-k procedure that dynamically selects the number of samples to
generate as input to a reranker; second, a routing procedure that dynamically
responds to a query using a decoding procedure that is expensive but accurate,
or one that is cheaper but less capable. Across a suite of programming,
mathematics, and dialog tasks, we show that accurate computation-allocation
procedures can be learned, and reduce computation by up to 50% at no cost to
response quality, or improve quality by up to 10% at a fixed computational
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling and Estimation of Vocal Tract and Glottal Source Parameters
  Using ARMAX-LF Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Lia, Masato Akagia, Yongwei Lib, Masashi Unokia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and estimation of the vocal tract and glottal source parameters of
vowels from raw speech can be typically done by using the Auto-Regressive with
eXogenous input (ARX) model and Liljencrants-Fant (LF) model with an
iteration-based estimation approach. However, the all-pole autoregressive model
in the modeling of vocal tract filters cannot provide the locations of
anti-formants (zeros), which increases the estimation errors in certain classes
of speech sounds, such as nasal, fricative, and stop consonants. In this paper,
we propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)
model to extend the ARX-LF model to a wider variety of speech sounds, including
vowels and nasalized consonants. The LF model represents the glottal source
derivative as a parametrized time-domain model, and the ARMAX model represents
the vocal tract as a pole-zero filter with an additional exogenous LF
excitation as input. To estimate multiple parameters with fewer errors, we
first utilize the powerful nonlinear fitting ability of deep neural networks
(DNNs) to build a mapping from extracted glottal source derivatives or speech
waveforms to corresponding LF parameters. Then, glottal source and vocal tract
parameters can be estimated with fewer estimation errors and without any
iterations as in the analysis-by-synthesis strategy. Experimental results with
synthesized speech using the linear source-filter model, synthesized speech
using the physical model, and real speech signals showed that the proposed
ARMAX-LF model with a DNN-based estimation method can estimate the parameters
of both vowels and nasalized sounds with fewer errors and estimation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Effect: Are Humans Truly Using LLMs, or Are They Being
  Influenced By Them Instead? <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown capabilities close to human
performance in various analytical tasks, leading researchers to use them for
time and labor-intensive analyses. However, their capability to handle highly
specialized and open-ended tasks in domains like policy studies remains in
question. This paper investigates the efficiency and accuracy of LLMs in
specialized tasks through a structured user study focusing on Human-LLM
partnership. The study, conducted in two stages-Topic Discovery and Topic
Assignment-integrates LLMs with expert annotators to observe the impact of LLM
suggestions on what is usually human-only analysis. Results indicate that
LLM-generated topic lists have significant overlap with human generated topic
lists, with minor hiccups in missing document-specific topics. However, LLM
suggestions may significantly improve task completion speed, but at the same
time introduce anchoring bias, potentially affecting the depth and nuance of
the analysis, raising a critical question about the trade-off between increased
efficiency and the risk of biased analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Main 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Shan Dong, Yuhui Xu, Hanze Dong, Yalu Wang, Amrita Saha, Ee-Peng Lim, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated versatile capabilities
in long-context scenarios. Although some recent benchmarks have been developed
to evaluate the long-context capabilities of LLMs, there is a lack of
benchmarks evaluating the mathematical reasoning abilities of LLMs over long
contexts, which is crucial for LLMs' application in real-world scenarios. In
this paper, we introduce MathHay, an automated benchmark designed to assess the
long-context mathematical reasoning capabilities of LLMs. Unlike previous
benchmarks like Needle in a Haystack, which focus primarily on information
retrieval within long texts, MathHay demands models with both
information-seeking and complex mathematical reasoning abilities. We conduct
extensive experiments on MathHay to assess the long-context mathematical
reasoning abilities of eight top-performing LLMs. Even the best-performing
model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over
long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights
the significant room for improvement on the MathHay benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work-in-Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deeper Insights Without Updates: The Power of In-Context Learning Over
  Fine-Tuning <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning and in-context learning (ICL) are two prevalent methods in
imbuing large language models with task-specific knowledge. It is commonly
believed that fine-tuning can surpass ICL given sufficient training samples as
it allows the model to adjust its internal parameters based on the data.
However, this paper presents a counterintuitive finding: For tasks with
implicit patterns, ICL captures these patterns significantly better than
fine-tuning. We developed several datasets featuring implicit patterns, such as
sequences determining answers through parity or identifying reducible terms in
calculations. We then evaluated the models' understanding of these patterns
under both fine-tuning and ICL across models ranging from 0.5B to 7B
parameters. The results indicate that models employing ICL can quickly grasp
deep patterns and significantly improve accuracy. In contrast, fine-tuning,
despite utilizing thousands of times more training samples than ICL, achieved
only limited improvements. We also proposed circuit shift theory from a
mechanistic interpretability's view to explain why ICL wins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP'24 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Multi-Agent Evaluation of <span class="highlight-title">Large Language Model</span>s through
  Iterative Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaithanya Bandi, Hari Bandi, Abir Harrasse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores optimal architectures for evaluating the outputs of large
language models (LLMs) using LLMs themselves. We propose a novel framework that
interprets LLMs as advocates within an ensemble of interacting agents, allowing
them to defend their answers and reach conclusions through a judge and jury
system. This approach offers a more dynamic and comprehensive evaluation
process compared to traditional human-based assessments or automated metrics.
We discuss the motivation behind this framework, its key components, and
comparative advantages. We also present a probabilistic model to evaluate the
error reduction achieved by iterative advocate systems. Finally, we outline
experiments to validate the effectiveness of multi-advocate architectures and
discuss future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning to Improve Retrieval for Real-world Fact Checking <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddh Sriram, Fangyuan Xu, Eunsol Choi, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on fact-checking addresses a realistic setting where models
incorporate evidence retrieved from the web to decide the veracity of claims. A
bottleneck in this pipeline is in retrieving relevant evidence: traditional
methods may surface documents directly related to a claim, but fact-checking
complex claims requires more inferences. For instance, a document about how a
vaccine was developed is relevant to addressing claims about what it might
contain, even if it does not address them directly. We present Contrastive
Fact-Checking Reranker (CFR), an improved retriever for this setting. By
leveraging the AVeriTeC dataset, which annotates subquestions for claims with
human written answers from evidence documents, we fine-tune Contriever with a
contrastive objective based on multiple training signals, including
distillation from GPT-4, evaluating subquestion answers, and gold labels in the
dataset. We evaluate our model on both retrieval and end-to-end veracity
judgments about claims. On the AVeriTeC dataset, we find a 6\% improvement in
veracity classification accuracy. We also show our gains can be transferred to
FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to
make inferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 FEVER Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How
  to Fix It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether LLMs memorize their training data and what this means, from privacy
leakage to detecting copyright violations -- has become a rapidly growing area
of research over the last two years. In recent months, more than 10 new methods
have been proposed to perform Membership Inference Attacks (MIAs) against LLMs.
Contrary to traditional MIAs which rely on fixed -- but randomized -- records
or models, these methods are mostly evaluated on datasets collected post-hoc.
Sets of members and non-members, used to evaluate the MIA, are constructed
using informed guesses after the release of a model. This lack of randomization
raises concerns of a distribution shift between members and non-members. In the
first part, we review the literature on MIAs against LLMs. While most work
focuses on sequence-level MIAs evaluated in post-hoc setups, we show that a
range of target models, motivations and units of interest have been considered
in the literature. We then quantify distribution shifts present in the 6
datasets used in the literature, ranging from books to papers, using a bag of
word classifier. Our analysis reveals that all of them suffer from severe
distribution shifts. This challenges the validity of using such setups to
measure LLM memorization and may undermine the benchmarking of recently
proposed methods. Yet, all hope might not be lost. In the second part, we
introduce important considerations to properly evaluate MIAs against LLMs and
discuss potential ways forward: randomized test splits, injections of
randomized (unique) sequences, randomized finetuning, and post-hoc control
methods. While each option comes with its advantages and limitations, we
believe they collectively provide solid grounds to guide the development of MIA
methods and study LLM memorization. We conclude by proposing comprehensive,
easy-to-use benchmarks for sequence- and document-level MIAs against LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding Benchmark (Published at NeurIPS 2024 Track <span class="highlight-title">Dataset</span>s and
  Benchmarks) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at NeurIPS 2024 Track
  Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: Benchmarking Code Ge<span class="highlight-title">ner</span>ation with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical requires the capability of utilizing diverse function calls as tools
to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question
  Answering (Published in Findings of EMNLP 2024) <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive
abilities in generating responses based on human instructions. However, their
use in the medical field can be challenging due to their lack of specific,
in-depth knowledge. In this study, we present a system called LLMs Augmented
with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in
specialized domains. LLM-AMT integrates authoritative medical textbooks into
the LLMs' framework using plug-and-play modules. These modules include a Query
Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,
they incorporate authoritative medical knowledge. Additionally, an LLM Reader
aids in contextual understanding. Our experimental results on three medical QA
tasks demonstrate that LLMAMT significantly improves response quality, with
accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the
base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on
a massive amount of medical corpus by 2-3%. We found that despite being 100x
smaller in size, medical textbooks as a retrieval corpus is proven to be a more
effective knowledge database than Wikipedia in the medical domain, boosting
performance by 7.8%-13.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation alignment: Comparing LLM and human annotations of
  conversational safety <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06369v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06369v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajiv Movva, Pang Wei Koh, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do LLMs align with human perceptions of safety? We study this question via
annotation alignment, the extent to which LLMs and humans agree when annotating
the safety of user-chatbot conversations. We leverage the recent DICES dataset
(Aroyo et al., 2023), in which 350 conversations are each rated for safety by
112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson
correlation of $r = 0.59$ with the average annotator rating, \textit{higher}
than the median annotator's correlation with the average ($r=0.51$). We show
that larger datasets are needed to resolve whether LLMs exhibit disparities in
how well they correlate with different demographic groups. Also, there is
substantial idiosyncratic variation in correlation within groups, suggesting
that race & gender do not fully capture differences in alignment. Finally, we
find that GPT-4 cannot predict when one demographic group finds a conversation
more unsafe than another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main). Main text contains 6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: LLM-as-a-Judge For Improving Response Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Ge<span class="highlight-title">ner</span>ation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Usage-centric Take on Intent Understanding in E-Commerce <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its essential role in product recommendation and business user
profiling analysis, intent understanding has not been consistently defined or
accurately benchmarked. In this paper, we focus on predicative user intents as
"how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:
category-rigidity and property-ambiguity. They limit its ability to strongly
align user intents with products having the most desirable property, and to
recommend useful products across diverse categories. Following these
observations, we introduce a Product Recovery Benchmark featuring a novel
evaluation framework and an example dataset. We further validate the above
FolkScope weaknesses on this benchmark. Our code and dataset are available at
https://github.com/stayones/Usgae-Centric-Intent-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acepted by EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Instruction-Following Through Minimum Bayes Risk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose LLM judges capable of human-level evaluation provide not only
a scalable and accurate way of evaluating instruction-following LLMs but also
new avenues for supervising and improving their performance. One promising way
of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)
decoding, which uses a reference-based evaluator to select a high-quality
output from amongst a set of candidate outputs. In the first part of this work,
we explore using MBR decoding as a method for improving the test-time
performance of instruction-following LLMs. We find that MBR decoding with
reference-based LLM judges substantially improves over greedy decoding,
best-of-N decoding with reference-free judges and MBR decoding with lexical and
embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent
across LLMs with up to 70B parameters, demonstrating that smaller LLM judges
can be used to supervise much larger LLMs. Then, seeking to retain the
improvements from MBR decoding while mitigating additional test-time costs, we
explore iterative self-training on MBR-decoded outputs. We find that
self-training using Direct Preference Optimisation leads to significant
performance gains, such that the self-trained models with greedy decoding
generally match and sometimes exceed the performance of their base models with
MBR decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation noising effectively prevents harmful fine-tuning on LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that is
effective even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the effectiveness of our defence lies in its "depth": the degree
to which information about harmful representations is removed across all layers
of the LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Bias Probing: Fairness Benchmarking for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09090v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09090v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the impact of social biases in language models has been recognized,
prior methods for bias evaluation have been limited to binary association tests
on small datasets, limiting our understanding of bias complexities. This paper
proposes a novel framework for probing language models for social biases by
assessing disparate treatment, which involves treating individuals differently
according to their affiliation with a sensitive demographic group. We curate
SoFa, a large-scale benchmark designed to address the limitations of existing
fairness collections. SoFa expands the analysis beyond the binary comparison of
stereotypical versus anti-stereotypical identities to include a diverse range
of identities and stereotypes. Comparing our methodology with existing
benchmarks, we reveal that biases within language models are more nuanced than
acknowledged, indicating a broader scope of encoded biases than previously
recognized. Benchmarking LMs on SoFa, we expose how identities expressing
different religions lead to the most pronounced disparate treatments across all
models. Finally, our findings indicate that real-life adversities faced by
various groups such as women and people with disabilities are mirrored in the
behavior of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and <span class="highlight-title">Prompt</span> Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAC$^2$E: Better Understanding <span class="highlight-title">Large Language Model</span> Capabilities by
  Dissociating Language and Cognition <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqiang Wang, Lingfei Wu, Tengfei Ma, Bang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are primarily evaluated by overall performance
on various text understanding and generation tasks. However, such a paradigm
fails to comprehensively differentiate the fine-grained language and cognitive
skills, rendering the lack of sufficient interpretation to LLMs' capabilities.
In this paper, we present FAC$^2$E, a framework for Fine-grAined and
Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate
LLMs' evaluation in a multi-dimensional and explainable manner by dissociating
the language-related capabilities and the cognition-related ones. Besides,
through extracting the intermediate reasoning from LLMs, we further break down
the process of applying a specific capability into three sub-steps: recalling
relevant knowledge, utilizing knowledge, and solving problems. Finally,
FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a
two-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common
shortfall in knowledge utilization among models and propose a straightforward,
knowledge-enhanced method to mitigate this issue. Our results not only showcase
promising performance enhancements but also highlight a direction for future
LLM advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Invasive Suicide Risk Prediction Through Speech Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Amiriparian, Maurice Gerczuk, Justina Lutz, Wolfgang Strube, Irina Papazova, Alkomiet Hasan, Alexander Kathan, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The delayed access to specialized psychiatric assessments and care for
patients at risk of suicidal tendencies in emergency departments creates a
notable gap in timely intervention, hindering the provision of adequate mental
health support during critical situations. To address this, we present a
non-invasive, speech-based approach for automatic suicide risk assessment. For
our study, we collected a novel speech recording dataset from $20$ patients. We
extract three sets of features, including wav2vec, interpretable speech and
acoustic features, and deep learning-based spectral representations. We proceed
by conducting a binary classification to assess suicide risk in a
leave-one-subject-out fashion. Our most effective speech model achieves a
balanced accuracy of $66.2\,\%$. Moreover, we show that integrating our speech
model with a series of patients' metadata, such as the history of suicide
attempts or access to firearms, improves the overall result. The metadata
integration yields a balanced accuracy of $94.4\,\%$, marking an absolute
improvement of $28.2\,\%$, demonstrating the efficacy of our proposed
approaches for automatic suicide risk assessment in emergency medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Native Design Bias: Studying the Impact of English Nativeness on
  Language Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at providing information acquired during
pretraining on large-scale corpora and following instructions through user
prompts. This study investigates whether the quality of LLM responses varies
depending on the demographic profile of users. Considering English as the
global lingua franca, along with the diversity of its dialects among speakers
of different native languages, we explore whether non-native English speakers
receive lower-quality or even factually incorrect responses from LLMs more
frequently. Our results show that performance discrepancies occur when LLMs are
prompted by native versus non-native English speakers and persist when
comparing native speakers from Western countries with others. Additionally, we
find a strong anchoring effect when the model recognizes or is made aware of
the user's nativeness, which further degrades the response quality when
interacting with non-native speakers. Our analysis is based on a newly
collected dataset with over 12,000 unique annotations from 124 annotators,
including information on their native language and English proficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UPCS: Unbiased Persona Construction for Dialogue Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyun Chen, Yanbin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative systems, such as dialogue and storytelling systems, often utilize
persona profiles to enhance personalized interactions. Existing persona
profiles frequently exhibit biases, posing risks to system integrity and
fairness. To address this, we introduce the UPCS framework, which categorizes
character descriptions into eight dimensions, including bias mitigation
strategies. Experimental results demonstrate UPCS's superiority in accuracy,
diversity, bias elimination, and user satisfaction, marking a significant
advancement in persona construction for reliable narrative systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity Over Size: On the Effect of Sample and Topic Sizes for
  Topic-Dependent Argument Mining <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Schiller, Johannes Daxenberger, Andreas Waldis, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Argument Mining, that is extracting and classifying argument
components for a specific topic from large document sources, is an inherently
difficult task for machine learning models and humans alike, as large Argument
Mining datasets are rare and recognition of argument components requires expert
knowledge. The task becomes even more difficult if it also involves stance
detection of retrieved arguments. In this work, we investigate the effect of
Argument Mining dataset composition in few- and zero-shot settings. Our
findings show that, while fine-tuning is mandatory to achieve acceptable model
performance, using carefully composed training samples and reducing the
training sample size by up to almost 90% can still yield 95% of the maximum
performance. This gain is consistent across three Argument Mining tasks on
three different datasets. We also publish a new dataset for future
benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KV-Compress: Paged KV-Cache Compression with Variable Compression Rates
  per Attention Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context lengths of Large Language Models (LLMs) have exploded in recent
years, with 128k-token context becoming a standard and million-token context
becoming a reality. Efficiently supporting long-context inference remains
challenging as the memory that must be allocated in key-value (KV) cache for a
generation scales with its context length, limiting the number of long-context
requests that can be served concurrently under a given memory budget. KV cache
compression can mitigate this issue by removing under-utilized KVs from each
attention head's cache and reducing its memory footprint. Higher theoretical
compression rates can be achieved when the number of removed KVs varies across
attention heads, but application of such a strategy within existing inference
frameworks adds fragmentation and cannot realize the theoretical compression
rates in physical memory. We introduce KV-Compress, a novel compression method
that evicts contiguous KV blocks within a PagedAttention framework, reducing
the memory footprint of the KV cache proportionally to this theoretical
compression rate. Our method achieves state-of-the-art performance on LongBench
for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the
total number of compressed KVs by 4x compared with prior methods. Evaluations
on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression
rates up to 8x with negligible impact on performance, and up to 64x while
retaining over 90% of full-cache performance for all but three of the suite's
subsets. We benchmark an integration of our method with vLLM that increases
total throughput by up to 5.18x by enabling larger decoding batches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Heuristic Then Rational: Dynamic Use of Heuristics in Language
  Model Reasoning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step reasoning instruction, such as chain-of-thought prompting, is
widely adopted to explore better language models (LMs) performance. We report
on the systematic strategy that LMs employ in such a multi-step reasoning
process. Our controlled experiments reveal that LMs rely more heavily on
heuristics, such as lexical overlap, in the earlier stages of reasoning, where
more reasoning steps remain to reach a goal. Conversely, their reliance on
heuristics decreases as LMs progress closer to the final answer through
multiple reasoning steps. This suggests that LMs can backtrack only a limited
number of future steps and dynamically combine heuristic strategies with
rationale ones in tasks involving multi-step reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Know More Than They Show: On the Intrinsic Representation of LLM
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructLM: Towards Building Ge<span class="highlight-title">ner</span>alist Models for Structured Knowledge
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16671v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16671v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured data sources, such as tables, graphs, and databases, are
ubiquitous knowledge sources. Despite the demonstrated capabilities of large
language models (LLMs) on plain text, their proficiency in interpreting and
utilizing structured data remains limited. Our investigation reveals a notable
deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags
behind state-of-the-art (SoTA) model by an average of 35%. To augment the
Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a
comprehensive instruction tuning dataset comprising 1.1 million examples.
Utilizing this dataset, we train a series of models, referred to as StructLM,
based on the Mistral and the CodeLlama model family, ranging from 7B to 34B
parameters. Our StructLM series surpasses task-specific models on 16 out of 18
evaluated datasets and establishes new SoTA performance on 8 SKG tasks.
Furthermore, StructLM demonstrates strong generalization across 6 novel
held-out SKG tasks, outperforming TableLlama by an average of 35\% and Flan-UL2
20B by an average of 10\%. Contrary to expectations, we observe that scaling
model size offers marginal benefits, with StructLM-34B showing only slight
improvements over StructLM-7B. This suggests that structured knowledge
grounding is still a challenging task and requires more innovative design to
push to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-MoE: Towards Compositional <span class="highlight-title">Large Language Model</span>s with
  Self-Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Self-MoE, an approach that transforms a monolithic LLM into a
compositional, modular system of self-specialized experts, named MiXSE (MiXture
of Self-specialized Experts). Our approach leverages self-specialization, which
constructs expert modules using self-generated synthetic data, each equipping a
shared base LLM with distinct domain-specific capabilities, activated via
self-optimized routing. This allows for dynamic and capability-specific
handling of various target tasks, enhancing overall capabilities, without
extensive human-labeled data and added parameters. Our empirical results reveal
that specializing LLMs may exhibit potential trade-offs in performances on
non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial
improvements (6.5%p on average) over the base LLM across diverse benchmarks
such as knowledge, reasoning, math, and coding. It also consistently
outperforms other methods, including instance merging and weight merging, while
offering better flexibility and interpretability by design with semantic
experts and routing. Our findings highlight the critical role of modularity,
the applicability of Self-MoE to multiple base LLMs, and the potential of
self-improvement in achieving efficient, scalable, and adaptable systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Russian Jeopardy! Data Set for Question-Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Mikhalkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) is one of the most common NLP tasks that relates to
named entity recognition, fact extraction, semantic search and some other
fields. In industry, it is much appreciated in chatbots and corporate
information systems. It is also a challenging task that attracted the attention
of a very general audience at the quiz show Jeopardy! In this article we
describe a Jeopardy!-like Russian QA data set collected from the official
Russian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like
questions with 29,375 from the Russian analogue of Jeopardy! - "Own Game". We
observe its linguistic features and the related QA-task. We conclude about
perspectives of a QA competition based on the data set collected from this
database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation with Zero Inference Overhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao Wu, Yongbo Wang, Feng Wang, Jingtong Wu, Xin Lu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  <span class="highlight-title">Large Language Model</span>s in Identifying Wellness Dimensions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on
two existing mental health and well-being datasets: (a) Multi-label
Classification-based MultiWD, and (b) WellXplain for evaluating attention
mechanism veracity against expert-labeled explanations. The labels are based on
Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We
reveal four surprising results about LMs/LLMs: (1) Despite their human-like
capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on
WellXplain fails to deliver any remarkable improvements in performance or
explanations. (2) Re-examining LMs' predictions based on a confidence-oriented
loss function reveals a significant performance drop. (3) Across all LMs/LLMs,
the alignment between attention and explanations remains low, with LLMs scoring
a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific
knowledge and undervalue explanations, causing these discrepancies. This study
highlights the need for further research into their consistency and
explanations in mental health and well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can <span class="highlight-title">Transformer</span>s Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Is More Than Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a foundational step in natural language processing (NLP)
tasks, bridging raw text and language models. Existing tokenization approaches
like Byte-Pair Encoding (BPE) originate from the field of data compression, and
it has been suggested that the effectiveness of BPE stems from its ability to
condense text into a relatively small number of tokens. We test the hypothesis
that fewer tokens lead to better downstream performance by introducing
PathPiece, a new tokenizer that segments a document's text into the minimum
number of tokens for a given vocabulary. Through extensive experimentation we
find this hypothesis not to be the case, casting doubt on the understanding of
the reasons for effective tokenization. To examine which other factors play a
role, we evaluate design decisions across all three phases of tokenization:
pre-tokenization, vocabulary construction, and segmentation, offering new
insights into the design of effective tokenizers. Specifically, we illustrate
the importance of pre-tokenization and the benefits of using BPE to initialize
vocabulary construction. We train 64 language models with varying tokenization,
ranging in size from 350M to 2.4B parameters, all of which are made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComplexTempQA: A Large-Scale <span class="highlight-title">Dataset</span> for Complex Temporal Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Gruber, Abdelrahman Abdallah, Michael Färber, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ComplexTempQA, a large-scale dataset consisting of over 100
million question-answer pairs designed to tackle the challenges in temporal
question answering. ComplexTempQA significantly surpasses existing benchmarks
like HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from
Wikipedia and Wikidata, the dataset covers questions spanning over two decades
and offers an unmatched breadth of topics. We introduce a unique taxonomy that
categorizes questions as attributes, comparisons, and counting questions, each
revolving around events, entities, and time periods. One standout feature of
ComplexTempQA is the high complexity of its questions, which demand effective
capabilities for answering such as across-time comparison, temporal
aggregation, and multi-hop reasoning involving temporal event ordering and
entity recognition. Additionally, each question is accompanied by detailed
metadata, including specific time scopes, allowing for comprehensive evaluation
and enhancement of the temporal reasoning abilities of large language models.
ComplexTempQA serves both as a testing ground for developing sophisticated AI
models and as a foundation for advancing research in question answering,
information retrieval, and language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstruct Your Previous Conversations! Comprehensively Investigating
  Privacy Leakage Risks in Conversations with <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have recently been made in large language models
represented by GPT models. Users frequently have multi-round private
conversations with cloud-hosted GPT models for task optimization. Yet, this
operational paradigm introduces additional attack surfaces, particularly in
custom GPTs and hijacked chat sessions. In this paper, we introduce a
straightforward yet potent Conversation Reconstruction Attack. This attack
targets the contents of previous conversations between GPT models and benign
users, i.e., the benign users' input contents during their interaction with GPT
models. The adversary could induce GPT models to leak such contents by querying
them with designed malicious prompts. Our comprehensive examination of privacy
risks during the interactions with GPT models under this attack reveals GPT-4's
considerable resilience. We present two advanced attacks targeting improved
reconstruction of past conversations, demonstrating significant privacy leakage
across all models under these advanced techniques. Evaluating various defense
mechanisms, we find them ineffective against these attacks. Our findings
highlight the ease with which privacy can be compromised in interactions with
GPT models, urging the community to safeguard against potential abuses of these
models' capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024. 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on Multimodal <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Vera Marjanović, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, Accepted to Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Stream Analysis with Multi-Layer SAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to `switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but this layer may differ for different
tokens or prompts. We quantify these phenomena by defining a distribution over
layers and considering its variance. We find that the variance of the
distributions of latent activations over layers is about two orders of
magnitude greater when aggregating over tokens compared with a single token.
For larger underlying models, the degree to which latents are active at
multiple layers increases, which is consistent with the fact that the residual
stream activation vectors at adjacent layers become more similar. Finally, we
relax the assumption that the residual stream basis is the same at every layer
by applying pre-trained tuned-lens transformations, but our findings remain
qualitatively similar. Our results represent a new approach to understanding
how representations change as they flow through transformers. We release our
code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph 2.0: Deep and Faithful <span class="highlight-title">Large Language Model</span> Reasoning
  with Knowledge-guided Retrieval Augmented Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10805v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10805v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has enhanced large language models
(LLMs) by using knowledge retrieval to address knowledge gaps. However,
existing RAG approaches often fail to ensure the depth and completeness of the
information retrieved, which is essential for complex reasoning tasks. In this
work, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that
iteratively retrieves information from both unstructured and structured
knowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages
knowledge graphs (KGs) to connect documents via entities, facilitating deep and
knowledge-guided context retrieval. Simultaneously, it uses documents as entity
contexts to enable precise and efficient graph retrieval.
  ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate accurate
answers. We conduct a series of experiments to demonstrate the following
advantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph
retrieval, enhancing context retrieval through the KG while enabling reliable
graph retrieval based on contexts; (2) it achieves deep and faithful reasoning
in LLMs through an iterative knowledge retrieval process that integrates
contexts and the KG; and (3) ToG-2 is training-free and compatible with various
LLMs as a plug-and-play solution. Extensive experiments show that ToG-2
achieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive
datasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,
LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language in Vivo vs. in Silico: Size Matters but Larger Language Models
  Still Do Not Comprehend Language on a Par with Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittoria Dentella, Fritz Guenther, Evelina Leivada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the limits of language is a prerequisite for Large Language
Models (LLMs) to act as theories of natural language. LLM performance in some
language tasks presents both quantitative and qualitative differences from that
of humans, however it remains to be determined whether such differences are
amenable to model size. This work investigates the critical role of model
scaling, determining whether increases in size make up for such differences
between humans and models. We test three LLMs from different families (Bard,
137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a
grammaticality judgment task featuring anaphora, center embedding,
comparatives, and negative polarity. N=1,200 judgments are collected and scored
for accuracy, stability, and improvements in accuracy upon repeated
presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are
compared to results of n=80 humans on the same stimuli. We find that humans are
overall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but
that this is due to ChatGPT-4 outperforming humans only in one task condition,
namely on grammatical sentences. Additionally, ChatGPT-4 wavers more than
humans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,
respectively). Thus, while increased model size may lead to better performance,
LLMs are still not sensitive to (un)grammaticality the same way as humans are.
It seems possible but unlikely that scaling alone can fix this issue. We
interpret these results by comparing language learning in vivo and in silico,
identifying three critical differences concerning (i) the type of evidence,
(ii) the poverty of the stimulus, and (iii) the occurrence of semantic
hallucinations due to impenetrable linguistic reference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple
hallucination tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBF-LLM: Safe Control for LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Miyaoka, Masaki Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the safety
filter, designed based on the CBF, to the output generation of the baseline
LLM, i.e., the sequence of the token, with the aim of intervening in the
generated text. The overall text-generation system is implemented with Llama 3
and a RoBERTa model, and the source code is available at
https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control
ability and effectiveness in reducing the number of interventions needed for
user-specified alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparison of Language Modeling and Translation as Multilingual
  <span class="highlight-title">Pretrain</span>ing Objectives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, Jörg Tiedemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) display impressive performances and have
captured the attention of the NLP community. Establishing best practices in
pretraining has, therefore, become a major focus of NLP research, especially
since insights gained from monolingual English models may not necessarily apply
to more complex multilingual models. One significant caveat of the current
state of the art is that different works are rarely comparable: they often
discuss different parameter counts, training data, and evaluation methodology.
  This paper proposes a comparison of multilingual pretraining objectives in a
controlled methodological environment. We ensure that training data and model
architectures are comparable, and discuss the downstream performances across 6
languages that we observe in probing and fine-tuning scenarios. We make two key
observations: (1) the architecture dictates which pretraining objective is
optimal; (2) multilingual translation is a very effective pretraining objective
under the right conditions. We make our code, data, and model weights available
at \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine
  Translation with a Human-centered Study <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Sara Papi, Matteo Negri, Ana Guerberof, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender bias in machine translation (MT) is recognized as an issue that can
harm people and society. And yet, advancements in the field rarely involve
people, the final MT users, or inform how they might be impacted by biased
technologies. Current evaluations are often restricted to automatic methods,
which offer an opaque estimate of what the downstream impact of gender
disparities might be. We conduct an extensive human-centered study to examine
if and to what extent bias in MT brings harms with tangible costs, such as
quality of service gaps across women and men. To this aim, we collect
behavioral data from 90 participants, who post-edited MT outputs to ensure
correct gender translation. Across multiple datasets, languages, and types of
users, our study shows that feminine post-editing demands significantly more
technical and temporal effort, also corresponding to higher financial costs.
Existing bias measurements, however, fail to reflect the found disparities. Our
findings advocate for human-centered approaches that can inform the societal
impact of bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ad EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OffsetBias: Leveraging Debiased Data for Tuning Evaluators <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, Sanghyuk Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to assess the quality of generated
responses, such as prompting instruct-tuned models or fine-tuning judge models,
has become a widely adopted evaluation method. It is also known that such
evaluators are vulnerable to biases, such as favoring longer responses. While
it is important to overcome this problem, the specifics of these biases remain
under-explored. In this work, we qualitatively identify six types of biases
inherent in various judge models. We propose EvalBiasBench as a meta-evaluation
collection of hand-crafted test cases for each bias type. Additionally, we
present de-biasing dataset construction methods and the associated preference
dataset OffsetBias. Experimental results demonstrate that fine-tuning on our
dataset significantly enhances the robustness of judge models against biases
and improves performance across most evaluation scenarios. We release our
datasets and the fine-tuned judge model to public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Large Language Model</span>s Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse
  Representation Adjustment in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Universal Method for Meaningful Signal Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Mahon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is known that human speech and certain animal vocalizations can convey
meaningful content because we can decipher the content that a given utterance
does convey. This paper explores an alternative approach to determining whether
a signal is meaningful, one that analyzes only the signal itself and is
independent of what the conveyed meaning might be. We devise a method that
takes a waveform as input and outputs a score indicating its degree of
`meaningfulness`. We cluster contiguous portions of the input to minimize the
total description length, and then take the length of the code of the assigned
cluster labels as meaningfulness score. We evaluate our method empirically,
against several baselines, and show that it is the only one to give a high
score to human speech in various languages and with various speakers, a
moderate score to animal vocalizations from birds and orcas, and a low score to
ambient noise from various sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Persuasion: Towards Conversational Recommender System with
  Credible Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aid of large language models, current conversational recommender
system (CRS) has gaining strong abilities to persuade users to accept
recommended items. While these CRSs are highly persuasive, they can mislead
users by incorporating incredible information in their explanations, ultimately
damaging the long-term trust between users and the CRS. To address this, we
propose a simple yet effective method, called PC-CRS, to enhance the
credibility of CRS's explanations during persuasion. It guides the explanation
generation through our proposed credibility-aware persuasive strategies and
then gradually refines explanations via post-hoc self-reflection. Experimental
results demonstrate the efficacy of PC-CRS in promoting persuasive and credible
explanations. Further analysis reveals the reason behind current methods
producing incredible explanations and the potential of credible explanations to
improve recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024. Our code is available at
  https://github.com/mumen798/PC-CRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at EMNLP2024 - system demonstration track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can easily isolate a single speaker from a complex acoustic
environment, a capability referred to as the "Cocktail Party Effect." However,
replicating this ability has been a significant challenge in the field of
target speaker extraction (TSE). Traditional TSE approaches predominantly rely
on voiceprints, which raise privacy concerns and face issues related to the
quality and availability of enrollment samples, as well as intra-speaker
variability. To address these issues, this work introduces a novel text-guided
TSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language
model, LLaMA 2, processes typed text input from users to extract semantic cues.
We demonstrate that textual descriptions alone can effectively serve as cues
for extraction, thus addressing privacy concerns and reducing dependency on
voiceprints. Furthermore, our approach offers flexibility by allowing the user
to specify the extraction or suppression of a speaker and enhances robustness
against intra-speaker variability by incorporating context-dependent textual
information. Experimental results show competitive performance with text-based
cues alone and demonstrate the effectiveness of using text as a task selector.
Additionally, they achieve a new state-of-the-art when combining text-based
cues with pre-registered cues. This work represents the first integration of
LLMs with TSE, potentially establishing a new benchmark in solving the cocktail
party problem and expanding the scope of TSE applications by providing a
versatile, privacy-conscious solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, https://github.com/haoxiangsnr/llm-tse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Contrastive Decoding in Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation for
  Handling Noisy Contexts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge the gap between
external knowledge and the LLMs' parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLMs
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech
  Corpus for Scaling Indian TTS <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Sankar, Srija Anand, Praveen Srinivasa Varadhan, Sherry Thomas, Mehak Singal, Shridhar Kumar, Deovrat Mehendale, Aditi Krishana, Giri Raju, Mitesh Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-speech (TTS) synthesis show that large-scale
models trained with extensive web data produce highly natural-sounding output.
However, such data is scarce for Indian languages due to the lack of
high-quality, manually subtitled data on platforms like LibriVox or YouTube. To
address this gap, we enhance existing large-scale ASR datasets containing
natural conversations collected in low-quality environments to generate
high-quality TTS training data. Our pipeline leverages the cross-lingual
generalization of denoising and speech enhancement models trained on English
and applied to Indian languages. This results in IndicVoices-R (IV-R), the
largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704
hours of high-quality speech from 10,496 speakers across 22 Indian languages.
IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,
and IndicTTS. We also introduce the IV-R Benchmark, the first to assess
zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS
models on Indian voices, ensuring diversity in age, gender, and style. We
demonstrate that fine-tuning an English pre-trained model on a combined dataset
of high-quality IndicTTS and our IV-R dataset results in better zero-shot
speaker generalization compared to fine-tuning on the IndicTTS dataset alone.
Further, our evaluation reveals limited zero-shot generalization for Indian
voices in TTS models trained on prior datasets, which we improve by fine-tuning
the model on our data containing diverse set of speakers across language
families. We open-source all data and code, releasing the first TTS model for
all 22 official Indian languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets and Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMR Scaling Law: Predicting Critical Mixture Ratios for Continual
  <span class="highlight-title">Pre-train</span>ing of Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Gu, Zacc Yang, Chuanghao Ding, Rui Zhao, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in diverse tasks but often underperform in
specialized fields due to limited domain-specific or proprietary corpus.
Continual pre-training (CPT) enhances LLM capabilities by imbuing new
domain-specific or proprietary knowledge while replaying general corpus to
prevent catastrophic forgetting. The data mixture ratio of general corpus and
domain-specific corpus, however, has been chosen heuristically, leading to
sub-optimal training efficiency in practice. In this context, we attempt to
re-visit the scaling behavior of LLMs under the hood of CPT, and discover a
power-law relationship between loss, mixture ratio, and training tokens scale.
We formalize the trade-off between general and domain-specific capabilities,
leading to a well-defined Critical Mixture Ratio (CMR) of general and domain
data. By striking the balance, CMR maintains the model's general ability and
achieves the desired domain transfer, ensuring the highest utilization of
available resources. Considering the balance between efficiency and
effectiveness, CMR can be regarded as the optimal mixture ratio. Through
extensive experiments, we ascertain the predictability of CMR, propose CMR
scaling law and have substantiated its generalization. These findings offer
practical guidelines for optimizing LLM training in specialized domains,
ensuring both general and domain-specific performance while efficiently
managing training resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive-Hint <span class="highlight-title">Prompt</span>ing Improves Reasoning in <span class="highlight-title">Large Language Model</span>s <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09797v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09797v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted extensive and comprehensive experiments on seven benchmarks. The
results show that PHP significantly improves accuracy while remaining highly
efficient. For instance, with text-davinci-003, we observed a 4.2% improvement
on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction
in sample paths with self-consistency. With GPT-4 and PHP, we achieve
state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),
AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML AI4MATH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sDPO: Don't Use Your Data All at Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As development of large language models (LLM) progresses, aligning them with
human preferences has become increasingly important. We propose stepwise DPO
(sDPO), an extension of the recently popularized direct preference optimization
(DPO) for alignment tuning. This approach involves dividing the available
preference datasets and utilizing them in a stepwise manner, rather than
employing it all at once. We demonstrate that this method facilitates the use
of more precisely aligned reference models within the DPO training framework.
Furthermore, sDPO trains the final model to be more performant, even
outperforming other popular LLMs with more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I Like Sunnie More Than I Expected!": Exploring User Expectation and
  Perception of an Anthropomorphic LLM-based Conversational Agent for
  Well-Being Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Wu, Julie Y. A. Cachia, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human-computer interaction (HCI) research community has a longstanding
interest in exploring the mismatch between users' actual experiences and
expectation toward new technologies, for instance, large language models
(LLMs). In this study, we compared users' (N = 38) initial expectations against
their post-interaction perceptions of two LLM-powered mental well-being
intervention activity recommendation systems. Both systems have a built-in LLM
to recommend a personalized well-being intervention activity, but one system
(Sunnie) has an anthropomorphic conversational interaction design via elements
such as appearance, persona, and natural conversation. Results showed that user
engagement was high with both systems, and both systems exceeded users'
expectations along the utility dimension, highlighting AI's potential to offer
useful intervention activity recommendations. In addition, Sunnie further
outperformed the non-anthropomorphic baseline system in relational warmth.
These findings suggest that anthropomorphic conversational interaction design
may be particularly effective in fostering warmth in mental health support
contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAG-SQL: Multi-Agent Ge<span class="highlight-title">ner</span>ative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Xie, Gaochen Wu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher M. Ackerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, I extend the idea of
active steering with vectors that represent a behavioral direction of interest
to tuning those vectors directly into the model, obviating the need for online
control. First, I identify activation vectors related to honesty in an
open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can
be made more or less honest by adding positive or negative multiples of these
vectors to residual stream activations during generation. Then, I show that a
similar effect can be achieved by fine-tuning the vectors directly into the
model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss ("representation tuning"). Finally, I compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning; tuned
models are available at https://huggingface.co/collections/cackerman/
representation-tuning-66da1e5ab41cd1b824687d9f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability
  of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to exhibit remarkable performance in
natural language understanding tasks, there is a crucial need to measure their
ability for human-like multi-step logical reasoning. Existing logical reasoning
evaluation benchmarks often focus primarily on simplistic single-step or
multi-step reasoning with a limited set of inference rules. Furthermore, the
lack of datasets for evaluating non-monotonic reasoning represents a crucial
gap since it aligns more closely with human-like reasoning. To address these
limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset
encompassing multi-step logical reasoning with various inference rules and
depths. Multi-LogiEval covers three logic types--propositional, first-order,
and non-monotonic--consisting of more than 30 inference rules and more than 60
of their combinations with various depths. Leveraging this dataset, we conduct
evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,
and Mistral, employing a zero-shot chain-of-thought. Experimental results show
that there is a significant drop in the performance of LLMs as the reasoning
steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).
We further conduct a thorough investigation of reasoning chains generated by
LLMs which reveals several important findings. We believe that Multi-LogiEval
facilitates future research for evaluating and enhancing the logical reasoning
ability of LLMs. Data is available at
https://github.com/Mihir3009/Multi-LogiEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal
  Intervention Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Who's Harry Potter (WHP), a pioneering yet
insufficiently understood method for LLM unlearning. We explore it in two
steps. First, we introduce a new task of LLM targeted unlearning, where given
an unlearning target (e.g., a person) and some unlearning documents, we aim to
unlearn only the information about the target, rather than everything in the
unlearning documents. We further argue that a successful unlearning should
satisfy criteria such as not outputting gibberish, not fabricating facts about
the unlearning target, and not releasing factual information under jailbreak
attacks. Second, we construct a causal intervention framework for targeted
unlearning, where the knowledge of the unlearning target is modeled as a
confounder between LLM input and output, and the unlearning process as a
deconfounding process. This framework justifies and extends WHP, deriving a
simple unlearning algorithm that includes WHP as a special case. Experiments on
existing and new datasets show that our approach, without explicitly optimizing
for the aforementioned criteria, achieves competitive performance in all of
them. Our code is available at
https://github.com/UCSB-NLP-Chang/causal_unlearn.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PILLOW: Enhancing Efficient Instruction Fine-tuning via <span class="highlight-title">Prompt</span> Matching <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu, Yinghui Xu, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction fine-tuning has conventionally been employed to adapt Large
Language Models (LLMs) to a variety of tasks. Nonetheless, this technique often
necessitates substantial computational resources, making it impractical for
deployment by individuals or small-scale entities. Recently, Low-Rank
Adaptation (LoRA) has become a promising alternative, offering high
capabilities on par with full tuning with reduced resource overhead. However,
attaining satisfactory performance through the fine-tuning of LoRA is a
non-trivial challenge. In this paper, we propose PILLOW, which aims to improve
LoRA's performance by a discrimination-based prompting method, leveraging LLMs'
In-Context Learning ability. PILLOW incorporates a matching network that
selects prompts from a user-defined prompt pool, concatenates the selected
prompts with the user instruction as input, and performs inference using the
LoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits
commensurate performance on various evaluation metrics compared with typical
instruction fine-tuning methods, utilizing only consumer-grade GPU resources
and exhibiting a large reduction in computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2023 (Industry Track), Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaAlig<span class="highlight-title">ner</span>: Towards Ge<span class="highlight-title">ner</span>alizable Multi-Objective Alignment of Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Tianlin Zhang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) focus on aligning to
heterogeneous human expectations and values via multi-objective preference
alignment. However, existing methods are dependent on the policy model
parameters, which require high-cost repetition of their alignment algorithms
for each new policy model, and they cannot expand to unseen objectives due to
their static alignment objectives. In this work, we propose Meta-Objective
Aligner (MetaAligner), the first policy-agnostic and generalizable method for
multi-objective preference alignment. MetaAligner models multi-objective
alignment into three stages: (1) dynamic objectives reformulation algorithm
reorganizes traditional alignment datasets to supervise the model on performing
flexible alignment across different objectives; (2) conditional weak-to-strong
correction paradigm aligns the weak outputs of fixed policy models to approach
strong outputs with higher preferences in the corresponding alignment
objectives, enabling plug-and-play inferences on any policy models, which
significantly reduces training costs and facilitates alignment on close-source
policy models; (3) generalizable inference method flexibly adjusts target
objectives by updating their text descriptions in the prompts, facilitating
generalizable alignment to unseen objectives. Experimental results show that
MetaAligner achieves significant and balanced improvements in multi-objective
alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU
training hours compared to previous alignment methods. The model also
effectively aligns unseen objectives, marking the first step towards
generalizable multi-objective preference alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 main track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in GPU computational power has outpaced memory capacity
and bandwidth growth, creating bottlenecks in Large Language Model (LLM)
inference. Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but it suffers from significant
performance degradation below 4-bit precision. This paper addresses these
challenges by investigating the pretraining of low-bitwidth models specifically
Ternary Language Models (TriLMs) as an alternative to traditional
floating-point models (FloatLMs) and their post-training quantized versions
(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning
multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M
to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation
demonstrates that TriLMs offer superior scaling behavior in terms of model size
(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs
consistently outperform their QuantLM and FloatLM counterparts for a given bit
size across various benchmarks. Notably, the 3.9B parameter TriLM matches the
performance of the FloatLM 3.9B across all benchmarks, despite having fewer
bits than FloatLM 830M. Overall, this research provides valuable insights into
the feasibility and scalability of low-bitwidth language models, paving the way
for the development of more efficient LLMs.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 21 figures, and 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating and Safeguarding the Adversarial Robustness of
  Retrieval-Based In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15984v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15984v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Yu, Jie He, Pasquale Minervini, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,
In-Context Learning (ICL) gained significant attention due to its effectiveness
and efficiency. However, ICL is very sensitive to the choice, order, and
verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented
ICL methods try to address this problem by leveraging retrievers to extract
semantically related examples as demonstrations. While this approach yields
more accurate results, its robustness against various types of adversarial
attacks, including perturbations on test samples, demonstrations, and retrieved
data, remains under-explored. Our study reveals that retrieval-augmented models
can enhance robustness against test sample attacks, outperforming vanilla ICL
with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit
overconfidence in the demonstrations, leading to a 2% increase in ASR for
demonstration attacks. Adversarial training can help improve the robustness of
ICL methods to adversarial attacks; however, such a training scheme can be too
costly in the context of LLMs. As an alternative, we introduce an effective
training-free adversarial defence method, DARD, which enriches the example pool
with those attacked samples. We show that DARD yields improvements in
performance and robustness, achieving a 15% reduction in ASR over the
baselines. Code and data are released to encourage further research:
https://github.com/simonucl/adv-retreival-icl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024, 30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FacLens: Transferable Probe for Foreseeing Non-Factuality in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanling Wang, Haoyang Li, Hao Zou, Jing Zhang, Xinlei He, Qi Li, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in large language models (LLMs), non-factual responses
remain prevalent. Unlike extensive studies on post-hoc detection of such
responses, this work studies non-factuality prediction (NFP), aiming to predict
whether an LLM will generate a non-factual response to a question before the
generation process. Previous efforts on NFP have demonstrated LLMs' awareness
of their internal knowledge, but they still face challenges in efficiency and
transferability. In this work, we propose a lightweight NFP model named
Factuality Lens (FacLens), which effectively probes hidden representations of
questions for the NFP task. Besides, we discover that hidden question
representations sourced from different LLMs exhibit similar NFP patterns, which
enables the transferability of FacLens across LLMs to reduce development costs.
Extensive experiments highlight FacLens's superiority in both effectiveness and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frame-Voyager: Learning to Query Frames for Video <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Large Language Models (Video-LLMs) have made remarkable progress in
video understanding tasks. However, they are constrained by the maximum length
of input tokens, making it impractical to input entire videos. Existing frame
selection approaches, such as uniform frame sampling and text-frame retrieval,
fail to account for the information density variations in the videos or the
complex instructions in the tasks, leading to sub-optimal performance. In this
paper, we propose Frame-Voyager that learns to query informative frame
combinations, based on the given textual queries in the task. To train
Frame-Voyager, we introduce a new data collection and labeling pipeline, by
ranking frame combinations using a pre-trained Video-LLM. Given a video of M
frames, we traverse its T-frame combinations, feed them into a Video-LLM, and
rank them based on Video-LLM's prediction losses. Using this ranking as
supervision, we train Frame-Voyager to query the frame combinations with lower
losses. In experiments, we evaluate Frame-Voyager on four Video Question
Answering benchmarks by plugging it into two different Video-LLMs. The
experimental results demonstrate that Frame-Voyager achieves impressive results
in all settings, highlighting its potential as a plug-and-play solution for
Video-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and
  Committee Discussions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20267v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20267v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs continuously evolve, there is an urgent need for a reliable
evaluation method that delivers trustworthy results promptly. Currently, static
benchmarks suffer from inflexibility and unreliability, leading users to prefer
human voting platforms like Chatbot Arena. However, human evaluations require
significant manual effort. To address this, we propose the Auto-Arena, an
innovative framework that automates the entire evaluation process using
LLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM
candidates engage in a multi-round peer battle based on individual questions,
aiming at revealing their true performance differences. Finally, a committee of
LLM judges collaboratively discusses and decides the winner, reducing bias and
enhancing fairness. During the peer battles, we observe intriguing scenarios
where the LLM candidates display competitive behaviors and even learn from the
opponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena
shows a 92.14% correlation with human preferences, surpassing all previous
expert-annotated benchmarks without any manual efforts. As a result, Auto-Arena
offers a promising alternative to current human evaluation platforms for
evaluating LLMs automatically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evalverse: Unified and Accessible Library for <span class="highlight-title">Large Language Model</span>
  Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Evalverse, a novel library that streamlines the
evaluation of Large Language Models (LLMs) by unifying disparate evaluation
tools into a single, user-friendly framework. Evalverse enables individuals
with limited knowledge of artificial intelligence to easily request LLM
evaluations and receive detailed reports, facilitated by an integration with
communication platforms like Slack. Thus, Evalverse serves as a powerful tool
for the comprehensive assessment of LLMs, offering both researchers and
practitioners a centralized and easily accessible evaluation framework.
Finally, we also provide a demo video for Evalverse, showcasing its
capabilities and implementation in a two-minute format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corrective Retrieval Augmented Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update results, add more analysis, and fix typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magpie: Alignment Data Synthesis from Scratch by <span class="highlight-title">Prompt</span>ing Aligned LLMs
  with Nothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality instruction data is critical for aligning large language models
(LLMs). Although some models, such as Llama-3-Instruct, have open weights,
their alignment data remain private, which hinders the democratization of AI.
High human labor costs and a limited, predefined scope for prompting prevent
existing open-source data creation methods from scaling effectively,
potentially limiting the diversity and quality of public alignment datasets. Is
it possible to synthesize high-quality instruction data at scale by extracting
it directly from an aligned LLM? We present a self-synthesis method for
generating large-scale alignment data named Magpie. Our key observation is that
aligned LLMs like Llama-3-Instruct can generate a user query when we input only
the left-side templates up to the position reserved for user messages, thanks
to their auto-regressive nature. We use this method to prompt Llama-3-Instruct
and generate 4 million instructions along with their corresponding responses.
We perform a comprehensive analysis of the extracted data and select 300K
high-quality instances. To compare Magpie data with other public instruction
datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the
performance of the fine-tuned models. Our results indicate that in some tasks,
models fine-tuned with Magpie perform comparably to the official
Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data
points through supervised fine-tuning (SFT) and subsequent feedback learning.
We also show that using Magpie solely for SFT can surpass the performance of
previous public datasets utilized for both SFT and preference optimization,
such as direct preference optimization with UltraFeedback. This advantage is
evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://magpie-align.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinQuant: LLM quantization with learned rotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoodPuzzle: Developing <span class="highlight-title">Large Language Model</span> Agents as Flavor Scientists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tenghao Huang, Donghee Lee, John Sweeney, Jiatong Shi, Emily Steliotes, Matthew Lange, Jonathan May, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flavor development in the food industry is increasingly challenged by the
need for rapid innovation and precise flavor profile creation. Traditional
flavor research methods typically rely on iterative, subjective testing, which
lacks the efficiency and scalability required for modern demands. This paper
presents three contributions to address the challenges. Firstly, we define a
new problem domain for scientific agents in flavor science, conceptualized as
the generation of hypotheses for flavor profile sourcing and understanding. To
facilitate research in this area, we introduce the FoodPuzzle, a challenging
benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We
propose a novel Scientific Agent approach, integrating in-context learning and
retrieval augmented techniques to generate grounded hypotheses in the domain of
food science. Experimental results indicate that our model significantly
surpasses traditional methods in flavor profile prediction tasks, demonstrating
its potential to transform flavor development practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Ge<span class="highlight-title">ner</span>ation Gap: Exploring Age Bias in the Value Systems of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Liu, Trish Maturi, Bowen Yi, Siqi Shen, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the alignment of values in Large Language Models (LLMs) with
specific age groups, leveraging data from the World Value Survey across
thirteen categories. Through a diverse set of prompts tailored to ensure
response robustness, we find a general inclination of LLM values towards
younger demographics, especially when compared to the US population. Although a
general inclination can be observed, we also found that this inclination toward
younger groups can be different across different value categories.
Additionally, we explore the impact of incorporating age identity information
in prompts and observe challenges in mitigating value discrepancies with
different age cohorts. Our findings highlight the age bias in LLMs and provide
insights for future work. Materials for our analysis are available at \url{
https://github.com/MichiganNLP/Age-Bias-In-LLMs}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-06T00:00:00Z">2024-10-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for
  Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David-Gabriel Ion, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-performing speech models are trained on large amounts of data in the
language they are meant to work for. However, most languages have sparse data,
making training models challenging. This shortage of data is even more
prevalent in speech emotion recognition. Our work explores the model's
performance in limited data, specifically for speech emotion recognition.
Meta-learning specializes in improving the few-shot learning. As a result, we
employ meta-learning techniques on speech emotion recognition tasks, accent
recognition, and person identification. To this end, we propose a series of
improvements over the multistage meta-learning method. Unlike other works
focusing on smaller models due to the high computational cost of meta-learning
algorithms, we take a more practical approach. We incorporate a large
pre-trained backbone and a prototypical network, making our methods more
feasible and applicable. Our most notable contribution is an improved
fine-tuning technique during meta-testing that significantly boosts the
performance on out-of-distribution datasets. This result, together with
incremental improvements from several other works, helped us achieve accuracy
scores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition
datasets not included in the training or validation splits in the context of
4-way 5-shot learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure, Accepted by WISE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> of Query-based Text Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yu, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-based text summarization is an important real world problem that
requires to condense the prolix text data into a summary under the guidance of
the query information provided by users. The topic has been studied for a long
time and there are many existing interesting research related to query-based
text summarization. Yet much of the work is not systematically surveyed. This
survey aims at summarizing some interesting work in query-based text
summarization methods as well as related generic text summarization methods.
Not all taxonomies in this paper exist the related work to the best of our
knowledge and some analysis will be presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>s have evil twins <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rimon Melamed, Lucas H. McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, Enric Boix-Adsera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discover that many natural-language prompts can be replaced by
corresponding prompts that are unintelligible to humans but that provably
elicit similar behavior in language models. We call these prompts "evil twins"
because they are obfuscated and uninterpretable (evil), but at the same time
mimic the functionality of the original natural-language prompts (twins).
Remarkably, evil twins transfer between models. We find these prompts by
solving a maximum-likelihood problem which has applications of independent
interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main, camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adversarial Perspective on Machine Unlearning for AI Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are finetuned to refuse questions about hazardous
knowledge, but these protections can often be bypassed. Unlearning methods aim
at completely removing hazardous capabilities from models and make them
inaccessible to adversaries. This work challenges the fundamental differences
between unlearning and traditional safety post-training from an adversarial
perspective. We demonstrate that existing jailbreak methods, previously
reported as ineffective against unlearning, can be successful when applied
carefully. Furthermore, we develop a variety of adaptive methods that recover
most supposedly unlearned capabilities. For instance, we show that finetuning
on 10 unrelated examples or removing specific directions in the activation
space can recover most hazardous capabilities for models edited with RMU, a
state-of-the-art unlearning method. Our findings challenge the robustness of
current unlearning approaches and question their advantages over safety
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Efficient Language and Vision Assistants for Visually-Situated
  Natural Language Understanding: What Matters in Reading and Reasoning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geewook Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language and vision assistants have showcased
impressive capabilities but suffer from a lack of transparency, limiting
broader research and reproducibility. While open-source models handle general
image tasks effectively, they face challenges with the high computational
demands of complex visually-situated text understanding. Such tasks often
require increased token inputs and large vision modules to harness
high-resolution information. Striking a balance between model size and data
importance remains an open question. This study aims to redefine the design of
vision-language models by identifying key components and creating efficient
models with constrained inference costs. By strategically formulating datasets,
optimizing vision modules, and enhancing supervision techniques, we achieve
significant improvements in inference throughput while maintaining high
performance. Extensive experiments across models ranging from 160M to 13B
parameters offer insights into model optimization. We will fully open-source
our codebase, models, and datasets at https://github.com/naver-ai/elva.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks
  of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy K. Zhang, Neil Perry, Riya Dulepet, Joey Ji, Justin W. Lin, Eliot Jones, Celeste Menders, Gashon Hussein, Samantha Liu, Donovan Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Mike Yang, Teddy Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Polycarpos Yiorkadjis, Kenny Osele, Gautham Raghupathi, Dan Boneh, Daniel E. Ho, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model (LM) agents for cybersecurity that are capable of autonomously
identifying vulnerabilities and executing exploits have the potential to cause
real-world impact. Policymakers, model providers, and other researchers in the
AI and cybersecurity communities are interested in quantifying the capabilities
of such agents to help mitigate cyberrisk and investigate opportunities for
penetration testing. Toward that end, we introduce Cybench, a framework for
specifying cybersecurity tasks and evaluating agents on those tasks. We include
40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF
competitions, chosen to be recent, meaningful, and spanning a wide range of
difficulties. Each task includes its own description, starter files, and is
initialized in an environment where an agent can execute bash commands and
observe outputs. Since many tasks are beyond the capabilities of existing LM
agents, we introduce subtasks for each task, which break down a task into
intermediary steps for a more detailed evaluation. To evaluate agent
capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o,
OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,
Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask
guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and
Claude 3 Opus successfully solved complete tasks that took human teams up to 11
minutes to solve. In comparison, the most difficult task took human teams 24
hours and 54 minutes to solve. All code and data are publicly available at
https://cybench.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>78 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Ge<span class="highlight-title">ner</span>ation for
  Aligning <span class="highlight-title">Large Language Model</span>s to Online Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, Kristina Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social scientists use surveys to probe the opinions and beliefs of
populations, but these methods are slow, costly, and prone to biases. Recent
advances in large language models (LLMs) enable the creating of computational
representations or "digital twins" of populations that generate human-like
responses mimicking the population's language, styles, and attitudes. We
introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs
to online communities to elicit their beliefs. Given a corpus of a community's
online discussions, Community-Cross-Instruct automatically generates
instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM
to faithfully represent that community, and (2) evaluate the alignment of the
finetuned model to the community. We demonstrate the method's utility in
accurately representing political and diet communities on Reddit. Unlike prior
methods requiring human-authored instructions, Community-Cross-Instruct
generates instructions in a fully unsupervised manner, enhancing scalability
and generalization across domains. This work enables cost-effective and
automated surveying of diverse online communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superposed Decoding: Multiple Ge<span class="highlight-title">ner</span>ations from a Single Autoregressive
  Inference Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the most recent token
embeddings from the $k$ drafts as input to the next decoding step of the
language model. At every inference step we combine the $k$ drafts with the
top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,
using an n-gram interpolation with minimal compute overhead to filter out
incoherent generations. Our experiments show that $k$ drafts from Superposed
Decoding are at least as coherent and factual as Nucleus Sampling and Greedy
Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In
a compute-normalized setting, user evaluations demonstrably favor text
generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can
also be combined with other decoding strategies, resulting in universal
coverage gains when scaling inference time compute. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-03T00:00:00Z">2024-10-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Zhang, Mu Cai, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vinoground.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erasing Conceptual Knowledge from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure in language models has traditionally lacked a comprehensive
evaluation framework, leading to incomplete assessments of effectiveness of
erasure methods. We propose an evaluation paradigm centered on three critical
criteria: innocence (complete knowledge removal), seamlessness (maintaining
conditional fluent generation), and specificity (preserving unrelated task
performance). Our evaluation metrics naturally motivate the development of
Erasure of Language Memory (ELM), a new method designed to address all three
dimensions. ELM employs targeted low-rank updates to alter output distributions
for erased concepts while preserving overall model capabilities including
fluency when prompted for an erased concept. We demonstrate ELM's efficacy on
biosecurity, cybersecurity, and literary domain erasure tasks. Comparative
analysis shows that ELM achieves superior performance across our proposed
metrics, including near-random scores on erased topic assessments, generation
fluency, maintained accuracy on unrelated benchmarks, and robustness under
adversarial attacks. Our code, data, and trained models are available at
https://elm.baulab.info
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://elm.baulab.info</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on
Multilingual Coreference Resolution. In this third iteration of the shared
task, a novel objective is to also predict empty nodes needed for zero
coreference mentions (while the empty nodes were given on input in previous
years). This way, coreference resolution can be performed on raw text. We
evaluate two model variants: a~two-stage approach (where the empty nodes are
predicted first using a pretrained encoder model and then processed together
with sentence words by another pretrained model) and a single-stage approach
(where a single pretrained encoder model generates empty nodes, coreference
mentions, and coreference links jointly). In both settings, CorPipe surpasses
other participants by a large margin of 3.9 and 2.8 percent points,
respectively. The source code and the trained model are available at
https://github.com/ufal/crac2024-corpipe .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIEVE: Ge<span class="highlight-title">ner</span>al Purpose Data Filtering System Matching <span class="highlight-title">GPT</span>-4o Accuracy at
  1% the Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Zhang, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating specialized large language models requires vast amounts of clean,
special purpose data for training and fine-tuning. With only a handful of
existing large-scale, domain-specific datasets, creation of new datasets is
required in most applications. This requires the development of new
application-specific filtering of web-scale data. Filtering with a
high-performance, general-purpose LLM such as GPT-4o can be highly effective,
but this is extremely expensive at web-scale. This paper proposes SIEVE, a
lightweight alternative that matches GPT-4o accuracy at a fraction of the cost.
SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o
filtering call. The key to SIEVE is a seamless integration of GPT-4o and
lightweight T5 models, using active learning to fine-tune T5 in the background
with a small number of calls to GPT-4o. Once trained, it performs as well as
GPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the
OpenWebText dataset, using five highly customized filter tasks targeting high
quality and domain-specific content. Our results demonstrate the effectiveness
and efficiency of our method in curating large, high-quality datasets for
language model training at a substantially lower cost (1%) than existing
techniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o
achieve similar accuracy, with human evaluators preferring SIEVE's filtering
results to those of GPT-4o.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Language Models on Synthetic Edit Sequences Improves Code
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers mainly write code by editing existing programs. In
contrast, large language models (LLMs) autoregressively synthesize programs in
a single pass. One explanation for this is the scarcity of open-sourced edit
data. While high-quality instruction data for code synthesis is already scarce,
high-quality edit data is even scarcer. To fill this gap, we develop a
synthetic data generation algorithm called LintSeq. This algorithm refactors
existing code into a sequence of code edits by using a linter to procedurally
sample across the error-free insertions that can be used to sequentially write
programs. It outputs edit sequences as text strings consisting of consecutive
program diffs. To test LintSeq, we use it to refactor a dataset of instruction
+ program pairs into instruction + program-diff-sequence tuples. Then, we
instruction finetune a series of smaller LLMs ranging from 2.6B to 14B
parameters on both the re-factored and original versions of this dataset,
comparing zero-shot performance on code synthesis benchmarks. We show that
during repeated sampling, edit sequence finetuned models produce more diverse
programs than baselines. This results in better inference-time scaling for
benchmark coverage as a function of samples, i.e. the fraction of problems
"pass@k" solved by any attempt given "k" tries. For example, on HumanEval
pass@50, small LLMs finetuned on synthetic edit sequences are competitive with
GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)
in absolute score. Finally, we also pretrain our own tiny LMs for code
understanding. We show that finetuning tiny models on synthetic code edits
results in state-of-the-art code synthesis for the on-device model class. Our
150M parameter edit sequence LM matches or outperforms code models with twice
as many parameters, both with and without repeated sampling, including Codex
and AlphaCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic <span class="highlight-title">Prompt</span>
  Optimization for Text Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned
to extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neutral residues: revisiting adapters for model extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franck Signe Talla, Herve Jegou, Edouard Grave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of extending a pretrained large language model to a
new domain that was not seen at training time, like adding a language for which
the original model has seen no or little training data. Popular solutions like
fine-tuning or low-rank adaptation are successful at domain adaptation, but
formally they do not add any extra capacity and degrade the performance in the
original domain.
  Our paper analyzes this extension problem under three angles: data,
architecture and training procedure, which are advantageously considered
jointly. In particular, we improve adapters and make it possible to learn an
entire new language while ensuring that the output of the neural network is
almost unchanged in the original domain. For this purpose, we modify the new
residual blocks in a way that leads each new residual block to output
near-zeros in the original domain.
  This solution of neutral residues, which borrows architectural components
from mixture of experts, is effective: with only 20% extra learnable weights
compared to an original model trained on English, we get results that are
significantly better than concurrent approaches (fine-tuning, low-rank or
vanilla adapters) in terms of the trade-off between learning a new language and
not forgetting English.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has demonstrated
effectiveness in aligning large language models (LLMs) with human preferences.
However, token-level RLHF suffers from the credit assignment problem over long
sequences, where delayed rewards make it challenging for the model to discern
which actions contributed to successful outcomes. This hinders learning
efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple
yet effective RLHF framework that incorporates macro actions -- sequences of
tokens or higher-level language constructs -- into the learning process. By
operating at this higher level of abstraction, our approach reduces the
temporal distance between actions and rewards, facilitating faster and more
accurate credit assignment. This results in more stable policy gradient
estimates and enhances learning efficiency within each episode, all without
increasing computational complexity during training or inference. We validate
our approach through extensive experiments across various model sizes and
tasks, including text summarization, dialogue generation, question answering,
and program synthesis. Our method achieves substantial performance improvements
over standard RLHF, with performance gains of up to 30% in text summarization
and code generation, 18% in dialogue, and 8% in question answering tasks.
Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in
terms of training time and continues to outperform it with further training. We
will make our code and data publicly available at
https://github.com/ernie-research/MA-RLHF .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding <span class="highlight-title">Large Language Model</span>s In Embodied Environment With Imperfect
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Liu, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Salient Information <span class="highlight-title">Prompt</span>ing to Steer Content in <span class="highlight-title">Prompt</span>-based
  Abstractive Summarization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned
to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge has been widely utilized as an evaluation method in various
benchmarks and served as supervised rewards in model training. However, despite
their excellence in many domains, potential issues are under-explored,
undermining their reliability and the scope of their utility. Therefore, we
identify 12 key potential biases and propose a new automated bias
quantification framework-CALM-which systematically quantifies and analyzes each
type of bias in LLM-as-a-Judge by using automated and principle-guided
modification. Our experiments cover multiple popular language models, and the
results indicate that while advanced models have achieved commendable overall
performance, significant biases persist in certain specific tasks. Empirical
results suggest that there remains room for improvement in the reliability of
LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence
of these biases and give some suggestions for the reliable application of
LLM-as-a-Judge. Our work highlights the need for stakeholders to address these
issues and remind users to exercise caution in LLM-as-a-Judge applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes
  and Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation in unknown environments is crucial for deploying embodied
agents in real-world applications. While we have witnessed huge progress due to
large-scale scene datasets, faster simulators, and stronger models, previous
studies mainly focus on limited scene types and target objects. In this paper,
we study a new task of navigating to diverse target objects in a large number
of scene types. To benchmark the problem, we present a large-scale scene
dataset, DivScene, which contains 4,614 scenes across 81 different types. With
the dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a
Large Vision Language Model (LVLM) through imitation learning. The LVLM is
trained to take previous observations from the environment and generate the
next actions. We also introduce CoT explanation traces of the action prediction
for better performance when tuning LVLMs. Our extensive experiments find that
we can build a performant LVLM-based agent through imitation learning on the
shortest paths constructed by a BFS planner without any human supervision. Our
agent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we
carry out various analyses showing the generalization ability of our agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Multi-Modal Interleaved Document Representation for Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) methods aim to identify relevant documents in
response to a given query, which have gained remarkable attention due to their
successful application in various natural language tasks. However, existing
approaches typically consider only the textual information within the
documents, which overlooks the fact that documents can contain multiple
modalities, including texts, images, and tables. Further, they often segment
each long document into multiple discrete passages for embedding, preventing
them from capturing the overall document context and interactions between
paragraphs. We argue that these two limitations lead to suboptimal document
representations for retrieval. In this work, to address them, we aim to produce
more comprehensive and nuanced document representations by holistically
embedding documents interleaved with different modalities. Specifically, we
achieve this by leveraging the capability of recent vision-language models that
enable the processing and integration of text, images, and tables into a
unified format and representation. Moreover, to mitigate the information loss
from segmenting documents into passages, instead of representing and retrieving
passages individually, we further merge the representations of segmented
passages into one single document representation, while we additionally
introduce a reranking strategy to decouple and identify the relevant passage
within the document if necessary. Then, through extensive experiments on
diverse information retrieval scenarios considering both the textual and
multimodal queries, we show that our approach substantially outperforms
relevant baselines, thanks to the consideration of the multimodal information
interleaved within the documents in a unified way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,
  Even Mid-Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohin Manvi, Anikait Singh, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span>s as Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, Ievgen Redko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have proven to be remarkably efficient, both
across a wide range of natural language processing tasks and well beyond them.
However, a comprehensive theoretical analysis of the origins of their
impressive performance remains elusive. In this paper, we approach this
challenging task by drawing an equivalence between generic autoregressive
language models with vocabulary of size $T$ and context window of size $K$ and
Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We
derive several surprising findings related to the existence of a stationary
distribution of Markov chains that capture the inference power of LLMs, their
speed of convergence to it, and the influence of the temperature on the latter.
We then prove pre-training and in-context generalization bounds and show how
the drawn equivalence allows us to enrich their interpretation. Finally, we
illustrate our theoretical guarantees with experiments on several recent LLMs
to highlight how they capture the behavior observed in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Specific Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling
  for Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Instruction Tuning With Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of video large multimodal models (LMMs) has been hindered by
the difficulty of curating large amounts of high-quality raw data from the web.
To address this, we propose an alternative approach by creating a high-quality
synthetic dataset specifically for video instruction-following, namely
LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,
open-ended question-answering (QA), and multiple-choice QA. By training on this
dataset, in combination with existing visual instruction tuning data, we
introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that
LLaVA-Video achieves strong performance across various video benchmarks,
highlighting the effectiveness of our dataset. We plan to release the dataset,
its generation pipeline, and the model checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-Critic: Learning to Evaluate Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)
designed as a generalist evaluator to assess performance across a wide range of
multimodal tasks. LLaVA-Critic is trained using a high-quality critic
instruction-following dataset that incorporates diverse evaluation criteria and
scenarios. Our experiments demonstrate the model's effectiveness in two key
areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation
scores, performing on par with or surpassing GPT models on multiple evaluation
benchmarks; and (2) Preference Learning, where it generates reward signals for
preference learning, enhancing model alignment capabilities. This work
underscores the potential of open-source LMMs in self-critique and evaluation,
setting the stage for future research into scalable, superhuman alignment
feedback mechanisms for LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Know More Than They Show: On the Intrinsic Representation of LLM
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Attention Improves <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaniv Leviathan, Matan Kalman, Yossi Matias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention improves language modeling performance in a variety of model sizes
and context lengths. For example, a range of transformers trained with the
language modeling objective on C4 with selective attention perform equivalently
to standard transformers with ~2X more heads and parameters in their attention
modules. Selective attention also allows decreasing the size of the attention's
context buffer, leading to meaningful reductions in the memory and compute
requirements during inference. For example, transformers with 100M parameters
trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and
47X less memory for their attention module, respectively, when equipped with
selective attention, as those without selective attention, with the same
validation perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HELMET: How to Evaluate Long-Context Language Models Effectively and
  Thoroughly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izasak, Moshe Wasserblat, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been many benchmarks for evaluating long-context language models
(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack
(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate
to the diverse downstream applications of LCLMs, and the inconsistency further
complicates model comparison. We investigate the underlying reasons behind
current practices and find that existing benchmarks often provide noisy signals
due to low coverage of applications, insufficient lengths, unreliable metrics,
and incompatibility with base models. In this work, we present HELMET (How to
Evaluate Long-context Models Effectively and Thoroughly), a comprehensive
benchmark encompassing seven diverse, application-centric categories. We also
address many issues in previous benchmarks by adding controllable lengths up to
128k tokens, model-based evaluation for reliable metrics, and few-shot
prompting for robustly evaluating base models. Consequently, we demonstrate
that HELMET offers more reliable and consistent rankings of frontier LCLMs.
Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks
like NIAH are not good predictors of downstream performance; (2) the diverse
categories in HELMET exhibit distinct trends and low correlation with each
other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models
significantly lag behind closed ones when the task requires full-context
reasoning or following complex instructions -- the gap widens with increased
lengths. Finally, we recommend using our RAG tasks for fast model development,
as they are easy to run and more predictive of other downstream performance;
ultimately, we advocate for a holistic evaluation across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data are available here:
  https://github.com/princeton-nlp/HELMET</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Proper Treatment of Tokenization in Psycholinguistics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian DuSell, Tim Vieira, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are widely used in computational psycholinguistics to test
theories that relate the negative log probability (the surprisal) of a region
of interest (a substring of characters) under a language model to its cognitive
cost experienced by readers, as operationalized, for example, by gaze duration
on the region. However, the application of modern language models to
psycholinguistic studies is complicated by the practice of using tokenization
as an intermediate step in training a model. Doing so results in a language
model over token strings rather than one over character strings. Vexingly,
regions of interest are generally misaligned with these token strings. The
paper argues that token-level language models should be (approximately)
marginalized into character-level language models before they are used in
psycholinguistic studies to compute the surprisal of a region of interest;
then, the marginalized character-level language model can be used to compute
the surprisal of an arbitrary character substring, which we term a focal area,
that the experimenter may wish to use as a predictor. Our proposal of
marginalizing a token-level model into a character-level one solves this
misalignment issue independently of the tokenization scheme. Empirically, we
discover various focal areas whose surprisal is a better psychometric predictor
than the surprisal of the region of interest itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main conference long paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiddenGuard: Fine-Grained Safe Ge<span class="highlight-title">ner</span>ation with Specialized
  Representation Router 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) grow increasingly powerful, ensuring their
safety and alignment with human values remains a critical challenge. Ideally,
LLMs should provide informative responses while avoiding the disclosure of
harmful or sensitive information. However, current alignment approaches, which
rely heavily on refusal strategies, such as training models to completely
reject harmful prompts or applying coarse filters are limited by their binary
nature. These methods either fully deny access to information or grant it
without sufficient nuance, leading to overly cautious responses or failures to
detect subtle harmful content. For example, LLMs may refuse to provide basic,
public information about medication due to misuse concerns. Moreover, these
refusal-based methods struggle to handle mixed-content scenarios and lack the
ability to adapt to context-dependent sensitivities, which can result in
over-censorship of benign content. To overcome these challenges, we introduce
HiddenGuard, a novel framework for fine-grained, safe generation in LLMs.
HiddenGuard incorporates Prism (rePresentation Router for In-Stream
Moderation), which operates alongside the LLM to enable real-time, token-level
detection and redaction of harmful content by leveraging intermediate hidden
states. This fine-grained approach allows for more nuanced, context-aware
moderation, enabling the model to generate informative responses while
selectively redacting or replacing sensitive information, rather than outright
refusal. We also contribute a comprehensive dataset with token-level
fine-grained annotations of potentially harmful information across diverse
contexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1
score for detecting and redacting harmful content while preserving the overall
utility and informativeness of the model's responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of
  Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling an End-to-End Voice Assistant Without Instruction Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants, such as Siri and Google Assistant, typically model audio
and text separately, resulting in lost speech information and increased
complexity. Recent efforts to address this with end-to-end Speech Large
Language Models (LLMs) trained with supervised finetuning (SFT)
  have led to models ``forgetting" capabilities from text-only LLMs. Our work
proposes an alternative paradigm for training Speech LLMs without instruction
data, using the response of a text-only LLM to transcripts as self-supervision.
Importantly, this process can be performed without annotated responses. We show
that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question
Answering, Classification, and Translation. Furthermore, we show that DiVA
better meets user preferences, achieving a 72\% win rate compared with
state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training
compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring
  the (Lack of) Cultural Knowledge of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To make large language models (LLMs) more helpful across diverse cultures, it
is essential to have effective cultural knowledge benchmarks to measure and
track our progress. Effective benchmarks need to be robust, diverse, and
challenging. We introduce CulturalBench: a set of 1,227 human-written and
human-verified questions for effectively assessing LLMs' cultural knowledge,
covering 45 global regions including the underrepresented ones like Bangladesh,
Zimbabwe, and Peru. Questions - each verified by five independent annotators -
span 17 diverse topics ranging from food preferences to greeting etiquettes. We
evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which
share the same questions but asked differently. We find that LLMs are sensitive
to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to
human performance (92.6% accuracy), CulturalBench-Hard is more challenging for
frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the
worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with
tricky questions that have multiple correct answers (e.g., What utensils do the
Chinese usually use?), revealing a tendency to converge to a single answer. Our
results also indicate that OpenAI GPT-4o substantially outperform other
proprietary and open source models in questions related to all but one region
(Oceania). Nonetheless, all models consistently underperform on questions
related to South America and the Middle East.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining Language Modeling Assumptions Using an Annotated Literary
  Dialect Corpus <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Messner, Tom Lippincott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dataset of 19th century American literary orthovariant tokens
with a novel layer of human-annotated dialect group tags designed to serve as
the basis for computational experiments exploring literarily meaningful
orthographic variation. We perform an initial broad set of experiments over
this dataset using both token (BERT) and character (CANINE)-level contextual
language models. We find indications that the "dialect effect" produced by
intentional orthographic variation employs multiple linguistic channels, and
that these channels are able to be surfaced to varied degrees given particular
language modelling assumptions. Specifically, we find evidence showing that
choice of tokenization scheme meaningfully impact the type of orthographic
information a model is able to surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NLP4DH@EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Train Long-Context Language Models (Effectively) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study continued training and supervised fine-tuning (SFT) of a language
model (LM) to make effective use of long-context information. We first
establish a reliable evaluation protocol to guide model development -- Instead
of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set
of long-context tasks, and we evaluate models after SFT with instruction data
as this better reveals long-context abilities. Supported by our robust
evaluations, we run thorough experiments to decide the data mix for continued
pre-training, the instruction tuning dataset, and many other design choices. We
find that (1) code repositories and books are excellent sources of long data,
but it is crucial to combine them with high-quality short data; (2) training
with a sequence length beyond the evaluation length boosts long-context
performance; (3) for SFT, using only short instruction datasets yields strong
performance on long-context tasks. Our final model, ProLong-8B, which is
initialized from Llama-3 and trained on 40B tokens, demonstrates
state-of-the-art long-context performance among similarly sized models at a
length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of
long-context tasks despite having seen only 5% as many tokens during
long-context training. Additionally, ProLong can effectively process up to 512K
tokens, one of the longest context windows of publicly available LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code, data, and models are available at
  https://github.com/princeton-nlp/ProLong</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate Personified: Investigating the role of LLMs in content moderation <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Masud, Sahajpreet Singh, Viktor Hangya, Alexander Fraser, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For subjective tasks such as hate detection, where people perceive hate
differently, the Large Language Model's (LLM) ability to represent diverse
groups is unclear. By including additional context in prompts, we
comprehensively analyze LLM's sensitivity to geographical priming, persona
attributes, and numerical information to assess how well the needs of various
groups are reflected. Our findings on two LLMs, five languages, and six
datasets reveal that mimicking persona-based attributes leads to annotation
variability. Meanwhile, incorporating geographical signals leads to better
regional alignment. We also find that the LLMs are sensitive to numerical
anchors, indicating the ability to leverage community-based flagging efforts
and exposure to adversaries. Our work provides preliminary guidelines and
highlights the nuances of applying LLMs in culturally sensitive cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Improving Persuasiveness of Ge<span class="highlight-title">ner</span>ative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somesh Singh, Yaman K Singla, Harini SI, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly being used in workflows involving generating content to
be consumed by humans (e.g., marketing) and also in directly interacting with
humans (e.g., through chatbots). The development of such systems that are
capable of generating verifiably persuasive messages presents both
opportunities and challenges for society. On the one hand, such systems could
positively impact domains like advertising and social good, such as addressing
drug addiction, and on the other, they could be misused for spreading
misinformation and shaping political opinions. To channel LLMs' impact on
society, we need to develop systems to measure and benchmark their
persuasiveness. With this motivation, we introduce PersuasionBench and
PersuasionArena, the first large-scale benchmark and arena containing a battery
of tasks to measure the persuasion ability of generative models automatically.
We investigate to what extent LLMs know and leverage linguistic patterns that
can help them generate more persuasive language. Our findings indicate that the
persuasiveness of LLMs correlates positively with model size, but smaller
models can also be made to have a higher persuasiveness than much larger
models. Notably, targeted training using synthetic and natural datasets
significantly enhances smaller models' persuasive capabilities, challenging
scale-dependent assumptions. Our findings carry key implications for both model
developers and policymakers. For instance, while the EU AI Act and California's
SB-1047 aim to regulate AI models based on the number of floating point
operations, we demonstrate that simple metrics like this alone fail to capture
the full scope of AI's societal impact. We invite the community to explore and
contribute to PersuasionArena and PersuasionBench, available at
https://bit.ly/measure-persuasion, to advance our understanding of AI-driven
persuasion and its societal implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Undesirable Memorization in <span class="highlight-title">Large Language Model</span>s: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Satvaty, Suzan Verberne, Fatih Turkmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Immunogenicity Prediction with Dual Attention Enables Vaccine Target
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immunogenicity prediction is a central topic in reverse vaccinology for
finding candidate vaccines that can trigger protective immune responses.
Existing approaches typically rely on highly compressed features and simple
model architectures, leading to limited prediction accuracy and poor
generalizability. To address these challenges, we introduce ProVaccine, a novel
deep learning solution with a dual attention mechanism that integrates
pre-trained latent vector representations of protein sequences and structures.
We also compile the most comprehensive immunogenicity dataset to date,
encompassing over 9,500 antigen sequences, structures, and immunogenicity
labels from bacteria, viruses, and tumors. Extensive experiments demonstrate
that ProVaccine outperforms existing methods across a wide range of evaluation
metrics. Furthermore, we establish a post-hoc validation protocol to assess the
practical significance of deep learning models in tackling vaccine design
challenges. Our work provides an effective tool for vaccine design and sets
valuable benchmarks for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention in <span class="highlight-title">Large Language Model</span>s Yields Efficient Zero-Shot Re-Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Chen, Bernal Jiménez Gutiérrez, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Model</span> for Multi-Domain Translation: Benchmarking and
  Domain CoT Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Hu, Pei Zhang, Baosong Yang, Jun Xie, Derek F. Wong, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving consistent high-quality machine translation (MT) across diverse
domains remains a significant challenge, primarily due to the limited and
imbalanced parallel training data available in various domains. While large
language models (LLMs) have demonstrated impressive general understanding and
generation abilities, their potential in multi-domain MT is under-explored. We
establish a comprehensive benchmark for multi-domain translation, featuring 25
German$\Leftrightarrow$English and 22 Chinese$\Leftrightarrow$English test sets
respectively covering 15 domains. Our evaluation of prominent LLMs reveals a
discernible performance gap against traditional MT systems, highlighting domain
overfitting and catastrophic forgetting issues after fine-tuning on
domain-limited corpora. To mitigate this, we propose a domain Chain of Thought
(CoT) fine-tuning technique that utilizes the intrinsic multi-domain
intelligence of LLMs to improve translation performance. This method inspires
the LLM to perceive domain information from the source text, which then serves
as a helpful hint to guide the translation process. Despite being trained on a
small dataset of four domains, our CoT fine-tune approach achieves notable
enhancements in translation accuracy and domain robustness than traditional
fine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20
German$\rightarrow$English distinct out-of-domain tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-Eye: Abductive NLI for Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Will a Visual Language Model (VLM)-based bot warn us about slipping if it
detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet
their ability to infer outcomes and causes remains underexplored. To address
this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual
abductive reasoning skills. NL-Eye adapts the abductive Natural Language
Inference (NLI) task to the visual domain, requiring models to evaluate the
plausibility of hypothesis images based on a premise image and explain their
decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050
images) spanning diverse reasoning categories: physical, functional, logical,
emotional, cultural, and social. The data curation process involved two steps -
writing textual descriptions and generating images using text-to-image models,
both requiring substantial human involvement to ensure high-quality and
challenging scenes. Our experiments show that VLMs struggle significantly on
NL-Eye, often performing at random baseline levels, while humans excel in both
plausibility prediction and explanation quality. This demonstrates a deficiency
in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a
crucial step toward developing VLMs capable of robust multimodal reasoning for
real-world applications, including accident-prevention bots and generated video
verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndicSentEval: How Effectively do Multilingual <span class="highlight-title">Transformer</span> Models encode
  Linguistic Properties for Indic Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhilesh Aravapalli, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi, Manish Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in
  Under-Resourced Languages Using Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mesay Gemeda Yigezu, Melkamu Abay Mersha, Girma Yohannis Bade, Jugal Kalita, Olga Kolesnikova, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of fake news has emerged as a significant threat to the
integrity of information dissemination, particularly on social media platforms.
Misinformation can spread quickly due to the ease of creating and disseminating
content, affecting public opinion and sociopolitical events. Identifying false
information is therefore essential to reducing its negative consequences and
maintaining the reliability of online news sources. Traditional approaches to
fake news detection often rely solely on content-based features, overlooking
the crucial role of social context in shaping the perception and propagation of
news articles. In this paper, we propose a comprehensive approach that
integrates social context-based features with news content features to enhance
the accuracy of fake news detection in under-resourced languages. We perform
several experiments utilizing a variety of methodologies, including traditional
machine learning, neural networks, ensemble learning, and transfer learning.
Assessment of the outcomes of the experiments shows that the ensemble learning
approach has the highest accuracy, achieving a 0.99 F1 score. Additionally,
when compared with monolingual models, the fine-tuned model with the target
language outperformed others, achieving a 0.94 F1 score. We analyze the
functioning of the models, considering the important features that contribute
to model performance, using explainable AI techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agents' Room: Narrative Ge<span class="highlight-title">ner</span>ation through Multi-step Collaboration <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing compelling fiction is a multifaceted process combining elements such
as crafting a plot, developing interesting characters, and using evocative
language. While large language models (LLMs) show promise for story writing,
they currently rely heavily on intricate prompting, which limits their use. We
propose Agents' Room, a generation framework inspired by narrative theory, that
decomposes narrative writing into subtasks tackled by specialized agents. To
illustrate our method, we introduce Tell Me A Story, a high-quality dataset of
complex writing prompts and human-written stories, and a novel evaluation
framework designed specifically for assessing long narratives. We show that
Agents' Room generates stories that are preferred by expert evaluators over
those produced by baseline systems by leveraging collaboration and
specialization to decompose the complex story writing task into tractable
components. We provide extensive analysis with automated and human-based
metrics of the generated output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM
  Interactions <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angana Borah, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to evolve, they are increasingly
being employed in numerous studies to simulate societies and execute diverse
social tasks. However, LLMs are susceptible to societal biases due to their
exposure to human-generated data. Given that LLMs are being used to gain
insights into various societal aspects, it is essential to mitigate these
biases. To that end, our study investigates the presence of implicit gender
biases in multi-agent LLM interactions and proposes two strategies to mitigate
these biases. We begin by creating a dataset of scenarios where implicit gender
biases might arise, and subsequently develop a metric to assess the presence of
biases. Our empirical analysis reveals that LLMs generate outputs characterized
by strong implicit bias associations (>= 50\% of the time). Furthermore, these
biases tend to escalate following multi-agent interactions. To mitigate them,
we propose two strategies: self-reflection with in-context examples (ICE); and
supervised fine-tuning. Our research demonstrates that both methods effectively
mitigate implicit biases, with the ensemble of fine-tuning and self-reflection
proving to be the most successful.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Variational Autoencoders for Spectrogram Compression in
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Yakovenko, Ivan Bondarenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many Automatic Speech Recognition (ASR) tasks audio features as
spectrograms show better results than Mel-frequency Cepstral Coefficients
(MFCC), but in practice they are hard to use due to a complex dimensionality of
a feature space. The following paper presents an alternative approach towards
generating compressed spectrogram representation, based on Convolutional
Variational Autoencoders (VAE). A Convolutional VAE model was trained on a
subsample of the LibriSpeech dataset to reconstruct short fragments of audio
spectrograms (25 ms) from a 13-dimensional embedding. The trained model for a
40-dimensional (300 ms) embedding was used to generate features for corpus of
spoken commands on the GoogleSpeechCommands dataset. Using the generated
features an ASR system was built and compared to the model with MFCC features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Theory and Practice of Natural Computing 9th International
  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Unsupervised Constituency Parsing via Maximizing Semantic
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised constituency parsers organize phrases within a sentence into a
tree-shaped syntactic constituent structure that reflects the organization of
sentence semantics. However, the traditional objective of maximizing sentence
log-likelihood (LL) does not explicitly account for the close relationship
between the constituent structure and the semantics, resulting in a weak
correlation between LL values and parsing accuracy. In this paper, we introduce
a novel objective for training unsupervised parsers: maximizing the information
between constituent structures and sentence semantics (SemInfo). We introduce a
bag-of-substrings model to represent the semantics and apply the
probability-weighted information metric to estimate the SemInfo. Additionally,
we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the
SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)
induction, the state-of-the-art method for unsupervised constituency parsing.
Experiments demonstrate that SemInfo correlates more strongly with parsing
accuracy than LL. Our algorithm significantly enhances parsing accuracy by an
average of 7.85 points across five PCFG variants and in four languages,
achieving new state-of-the-art results in three of the four languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Tianlong Wang, Wen Tang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Junyi Gao, Liantao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVisionLlama: Leveraging <span class="highlight-title">Pre-Train</span>ed <span class="highlight-title">Large Language Model</span> Layers to
  Enhance Medical Image Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), known for their versatility in textual data,
are increasingly being explored for their potential to enhance medical image
segmentation, a crucial task for accurate diagnostic imaging. This study
explores enhancing Vision Transformers (ViTs) for medical image segmentation by
integrating pre-trained LLM transformer blocks. Our approach, which
incorporates a frozen LLM transformer block into the encoder of a ViT-based
model, leads to substantial improvements in segmentation performance across
various medical imaging modalities. We propose a Hybrid Attention Mechanism
that combines global and local feature learning with a Multi-Scale Fusion Block
for aggregating features across different scales. The enhanced model shows
significant performance gains, including an average Dice score increase from
0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.
These results demonstrate the effectiveness of LLM-based transformers in
refining medical image segmentation, highlighting their potential to
significantly boost model accuracy and robustness. The source code and our
implementation are available at: https://bit.ly/3zf2CVs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithms For Automatic Accentuation And Transcription Of Russian Texts
  In Speech Recognition Systems <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Iakovenko, Ivan Bondarenko, Mariya Borovikova, Daniil Vodolazsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an overview of rule-based system for automatic
accentuation and phonemic transcription of Russian texts for speech connected
tasks, such as Automatic Speech Recognition (ASR). Two parts of the developed
system, accentuation and transcription, use different approaches to achieve
correct phonemic representations of input phrases. Accentuation is based on
"Grammatical dictionary of the Russian language" of A.A. Zaliznyak and
wiktionary corpus. To distinguish homographs, the accentuation system also
utilises morphological information of the sentences based on Recurrent Neural
Networks (RNN). Transcription algorithms apply the rules presented in the
monograph of B.M. Lobanov and L.I. Tsirulnik "Computer Synthesis and Voice
Cloning". The rules described in the present paper are implemented in an
open-source module, which can be of use to any scientific study connected to
ASR or Speech To Text (STT) tasks. Automatically marked up text annotations of
the Russian Voxforge database were used as training data for an acoustic model
in CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,
mean Word Accuracy being 71.2%. The developed toolkit is written in the Python
language and is accessible on GitHub for any researcher interested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Speech and Computer 20th International Conference, SPECOM 2018,
  Leipzig, Germany, Proceedings 20</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods for Automatic Matrix Language Determination of Code-Switched
  Speech <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Iakovenko, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS) is the process of speakers interchanging between two or
more languages which in the modern world becomes increasingly common. In order
to better describe CS speech the Matrix Language Frame (MLF) theory introduces
the concept of a Matrix Language, which is the language that provides the
grammatical structure for a CS utterance. In this work the MLF theory was used
to develop systems for Matrix Language Identity (MLID) determination. The MLID
of English/Mandarin and English/Spanish CS text and speech was compared to
acoustic language identity (LID), which is a typical way to identify a language
in monolingual utterances. MLID predictors from audio show higher correlation
with the textual principles than LID in all cases while also outperforming LID
in an MLID recognition task based on F1 macro (60\%) and correlation score
(0.38). This novel approach has identified that non-English languages (Mandarin
and Spanish) are preferred over the English language as the ML contrary to the
monolingual choice of LID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Large Language Model</span>s Grasp Legal Theories? Enhance Legal Reasoning
  with Insights from Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, tianqianjin lin, Pengwei Yan, Changlong Sun, Xiaozhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) could struggle to fully understand legal
theories and perform complex legal reasoning tasks. In this study, we introduce
a challenging task (confusing charge prediction) to better evaluate LLMs'
understanding of legal theories and reasoning capabilities. We also propose a
novel framework: Multi-Agent framework for improving complex Legal Reasoning
capability (MALR). MALR employs non-parametric learning, encouraging LLMs to
automatically decompose complex legal tasks and mimic human learning process to
extract insights from legal rules, helping LLMs better understand legal
theories and enhance their legal reasoning abilities. Extensive experiments on
multiple real-world datasets demonstrate that the proposed framework
effectively addresses complex reasoning issues in practical scenarios, paving
the way for more reliable applications in the legal domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Session Conversation with Egocentric Memory <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyoung Jang, Taeyoung Kim, Hyounghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently introduced dialogue systems have demonstrated high usability.
However, they still fall short of reflecting real-world conversation scenarios.
Current dialogue systems exhibit an inability to replicate the dynamic,
continuous, long-term interactions involving multiple partners. This shortfall
arises because there have been limited efforts to account for both aspects of
real-world dialogues: deeply layered interactions over the long-term dialogue
and widely expanded conversation networks involving multiple participants. As
the effort to incorporate these aspects combined, we introduce Mixed-Session
Conversation, a dialogue system designed to construct conversations with
various partners in a multi-session dialogue setup. We propose a new dataset
called MiSC to implement this system. The dialogue episodes of MiSC consist of
6 consecutive sessions, with four speakers (one main speaker and three
partners) appearing in each episode. Also, we propose a new dialogue model with
a novel memory management mechanism, called Egocentric Memory Enhanced
Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories
from the main speaker's perspective during conversations with partners,
enabling seamless continuity in subsequent interactions. Extensive human
evaluations validate that the dialogues in MiSC demonstrate a seamless
conversational flow, even when conversation partners change in each session.
EMMA trained with MiSC is also evaluated to maintain high memorability without
contradiction throughout the entire conversation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024 (30 pages); Project website:
  https://mixed-session.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining Knowledge: Bridging Epistemology and <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Ruchira Dhar, Filippos Stamatiou, Nicolas Garneau, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge claims are abundant in the literature on large language models
(LLMs); but can we say that GPT-4 truly "knows" the Earth is round? To address
this question, we review standard definitions of knowledge in epistemology and
we formalize interpretations applicable to LLMs. In doing so, we identify
inconsistencies and gaps in how current NLP research conceptualizes knowledge
with respect to epistemological frameworks. Additionally, we conduct a survey
of 100 professional philosophers and computer scientists to compare their
preferences in knowledge definitions and their views on whether LLMs can really
be said to know. Finally, we suggest evaluation protocols for testing knowledge
in accordance to the most relevant definitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Gradient Alignment for Online Data Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Fan, David Grangier, Pierre Ablin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The composition of training data mixtures is critical for effectively
training large language models (LLMs), as it directly impacts their performance
on downstream tasks. Our goal is to identify an optimal data mixture to
specialize an LLM for a specific task with access to only a few examples.
Traditional approaches to this problem include ad-hoc reweighting methods,
importance sampling, and gradient alignment techniques. This paper focuses on
gradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable
online gradient alignment algorithm. DGA dynamically estimates the pre-training
data mixture on which the models' gradients align as well as possible with
those of the model on the specific task. DGA is the first gradient alignment
approach that incurs minimal overhead compared to standard pre-training and
outputs a competitive model, eliminating the need for retraining the model.
Experimentally, we demonstrate significant improvements over importance
sampling in two key scenarios: (i) when the pre-training set is small and
importance sampling overfits due to limited data; and (ii) when there is
insufficient specialized data, trapping importance sampling on narrow pockets
of data. Our findings underscore the effectiveness of gradient alignment
methods in optimizing training data mixtures, particularly in data-constrained
environments, and offer a practical solution for enhancing LLM performance on
specific tasks with limited data availability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking
  Based on LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Response Tuning: Aligning <span class="highlight-title">Large Language Model</span>s without Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyun An, Hyounghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning-supervised fine-tuning using instruction-response pairs-is
a foundational step in transitioning pre-trained Large Language Models (LLMs)
into helpful and safe chat assistants. Our hypothesis is that establishing an
adequate output space can enable such a transition given the capabilities
inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),
which eliminates the instruction-conditioning step in instruction tuning and
solely focuses on response space supervision. Our experiments demonstrate that
RT models, trained only using responses, can effectively respond to a wide
range of instructions and exhibit helpfulness comparable to that of their
instruction-tuned counterparts. Furthermore, we observe that controlling the
training response distribution can significantly improve their user preference
or elicit target behaviors such as refusing assistance for unsafe queries. Our
findings illuminate the role of establishing an adequate output space in
alignment, highlighting the potential of the extensive inherent capabilities of
pre-trained LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded Topic Models Enhanced by Wikification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Shibuya, Takehito Utsuro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling analyzes a collection of documents to learn meaningful
patterns of words. However, previous topic models consider only the spelling of
words and do not take into consideration the homography of words. In this
study, we incorporate the Wikipedia knowledge into a neural topic model to make
it aware of named entities. We evaluate our method on two datasets, 1) news
articles of \textit{New York Times} and 2) the AIDA-CoNLL dataset. Our
experiments show that our method improves the performance of neural topic
models in generalizability. Moreover, we analyze frequent terms in each topic
and the temporal dependencies between topics to demonstrate that our
entity-aware topic models can capture the time-series development of topics
well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Workshop NLP for Wikipedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Call SAUL: Fluent and Consistent Language Model Editing with
  Ge<span class="highlight-title">ner</span>ation Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Wang, Lukas Lange, Heike Adel, Jannik Strötgen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure large language models contain up-to-date knowledge, they need to be
updated regularly. However, model editing is challenging as it might also
affect knowledge that is unrelated to the new data. State-of-the-art methods
identify parameters associated with specific knowledge and then modify them via
direct weight updates. However, these locate-and-edit methods suffer from heavy
computational overhead and lack theoretical validation. In contrast, directly
fine-tuning the model on requested edits affects the model's behavior on
unrelated knowledge, and significantly damages the model's generation fluency
and consistency. To address these challenges, we propose SAUL, a streamlined
model editing method that uses sentence concatenation with augmented random
facts for generation regularization. Evaluations on three model editing
benchmarks show that SAUL is a practical and reliable solution for model
editing outperforming state-of-the-art methods while maintaining generation
quality and reducing computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuo An, Yunjiao Zhou, Han Zou, Jianfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
textual and visual domains but often generate outputs that violate physical
laws, revealing a gap in their understanding of the physical world. Inspired by
human cognition, where perception is fundamental to reasoning, we explore
augmenting LLMs with enhanced perception abilities using Internet of Things
(IoT) sensor data and pertinent knowledge for IoT task reasoning in the
physical world. In this work, we systematically study LLMs capability to
address real-world IoT tasks by augmenting their perception and knowledge base,
and then propose a unified framework, IoT-LLM, to enhance such capability. In
IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats
amenable to LLMs, activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions, and expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning. To evaluate the performance, We design a new benchmark
with five real-world IoT tasks with different data types and reasoning
difficulties and provide the benchmarking results on six open-source and
close-source LLMs. Experimental results demonstrate the limitations of existing
LLMs with naive textual inputs that cannot perform these tasks effectively. We
show that IoT-LLM significantly enhances the performance of IoT tasks reasoning
of LLM, such as GPT-4, achieving an average improvement of 65% across various
tasks against previous methods. The results also showcase LLMs ability to
comprehend IoT data and the physical law behind data by providing a reasoning
process. Limitations of our work are claimed to inspire future research in this
new era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, submitted to ICLR 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collective Critics for Creative Story Ge<span class="highlight-title">ner</span>ation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwook Bae, Hyounghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a long story of several thousand words with narrative coherence
using Large Language Models (LLMs) has been a challenging task. Previous
research has addressed this challenge by proposing different frameworks that
create a story plan and generate a long story based on that plan. However,
these frameworks have been mainly focusing on maintaining narrative coherence
in stories, often overlooking creativity in story planning and the
expressiveness of the stories generated from those plans, which are desirable
properties to captivate readers' interest. In this paper, we propose Collective
Critics for Creative Story Generation framework (CritiCS), which is composed of
plan refining stage (CrPlan) and story generation stage (CrText), to integrate
a collective revision mechanism that promotes those properties into long-form
story generation process. Specifically, in each stage, a group of LLM critics
and one leader collaborate to incrementally refine drafts of plan and story
throughout multiple rounds. Extensive human evaluation shows that the CritiCS
can significantly enhance story creativity and reader engagement, while also
maintaining narrative coherence. Furthermore, the design of the framework
allows active participation from human writers in any role within the critique
process, enabling interactive human-machine collaboration in story writing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (36 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Latent Rules of a Game from Data: A Chess Story 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Fauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate that small pretrained foundational generative language models
with millions of parameters can learn the latent rules of a process from data
associated with the process. Inspired by Stefan Zweig's novella
"Schachnovelle," also known as "The Royal Game" in English, we show that 28M
and 125M parameter pretrained foundational small language models (SLMs) can be
instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of
chess, propose legal moves, and accurately solve chess problems. We also
explore the impact of successive language model fine-tuning epochs on improved
outcomes and demonstrate reductions in model hallucinations by increasing the
number of instruction fine-tuning examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Pilot: Characterize and Optimize Performance of your LLM Inference
  Services <span class="chip">SC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Małgorzata Łazuka, Andreea Anghel, Thomas Parnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are rapidly growing in popularity, LLM
inference services must be able to serve requests from thousands of users while
satisfying performance requirements. The performance of an LLM inference
service is largely determined by the hardware onto which it is deployed, but
understanding of which hardware will deliver on performance requirements
remains challenging. In this work we present LLM-Pilot - a first-of-its-kind
system for characterizing and predicting performance of LLM inference services.
LLM-Pilot performs benchmarking of LLM inference services, under a realistic
workload, across a variety of GPUs, and optimizes the service configuration for
each considered GPU to maximize performance. Finally, using this
characterization data, LLM-Pilot learns a predictive model, which can be used
to recommend the most cost-effective hardware for a previously unseen LLM.
Compared to existing methods, LLM-Pilot can deliver on performance requirements
33% more frequently, whilst reducing costs by 60% on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Menak<span class="highlight-title">BERT</span> -- Hebrew Diacriticizer <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ido Cohen, Jacob Gidron, Idan Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diacritical marks in the Hebrew language give words their vocalized form. The
task of adding diacritical marks to plain Hebrew text is still dominated by a
system that relies heavily on human-curated resources. Recent models trained on
diacritized Hebrew texts still present a gap in performance. We use a recently
developed char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a
character level transformer pretrained on Hebrew text and fine-tuned to produce
diacritical marks for Hebrew sentences. We continue to show how finetuning a
model for diacritizing transfers to a task such as part of speech tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISCOL2022 as a poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter Competition Balancing for Model Merging <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While fine-tuning pretrained models has become common practice, these models
often underperform outside their specific domains. Recently developed model
merging techniques enable the direct integration of multiple models, each
fine-tuned for distinct tasks, into a single model. This strategy promotes
multitasking capabilities without requiring retraining on the original
datasets. However, existing methods fall short in addressing potential
conflicts and complex correlations between tasks, especially in parameter-level
adjustments, posing a challenge in effectively balancing parameter competition
across various tasks. This paper introduces an innovative technique named
PCB-Merging (Parameter Competition Balancing), a lightweight and training-free
technique that adjusts the coefficients of each parameter for effective model
merging. PCB-Merging employs intra-balancing to gauge parameter significance
within individual tasks and inter-balancing to assess parameter similarities
across different tasks. Parameters with low importance scores are dropped, and
the remaining ones are rescaled to form the final merged model. We assessed our
approach in diverse merging scenarios, including cross-task, cross-domain, and
cross-training configurations, as well as out-of-domain generalization. The
experimental results reveal that our approach achieves substantial performance
enhancements across multiple modalities, domains, model sizes, number of tasks,
fine-tuning forms, and large language models, outperforming existing model
merging methods. The code is publicly available at:
\url{https://github.com/duguodong7/pcb-merging}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaMetrics: Calibrating Metrics For Ge<span class="highlight-title">ner</span>ation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Comprehensive Detection of Chinese Harmful Memes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes
have proliferated on the Chinese Internet, while research on detecting Chinese
harmful memes significantly lags behind due to the absence of reliable datasets
and effective detectors. To this end, we focus on the comprehensive detection
of Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful
meme dataset, which consists of 12,000 samples with fine-grained annotations
for various meme types. Additionally, we propose a baseline detector,
Multimodal Knowledge Enhancement (MKE), incorporating contextual information of
meme content generated by the LLM to enhance the understanding of Chinese
memes. During the evaluation phase, we conduct extensive quantitative
experiments and qualitative analyses on multiple baselines, including LLMs and
our MKE. The experimental results indicate that detecting Chinese harmful memes
is challenging for existing models while demonstrating the effectiveness of
MKE. The resources for this paper are available at
https://github.com/DUT-lujunyu/ToxiCN_MM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Concrete to Abstract: A Multimodal Ge<span class="highlight-title">ner</span>ative Approach to Abstract
  Concept Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Xie, Rahul Singh Maharjan, Federico Tavella, Angelo Cangelosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating concrete and abstract concepts is fundamental
to human intelligence. Yet, they remain challenging for artificial agents. This
paper introduces a multimodal generative approach to high order abstract
concept learning, which integrates visual and categorical linguistic
information from concrete ones. Our model initially grounds subordinate level
concrete concepts, combines them to form basic level concepts, and finally
abstracts to superordinate level concepts via the grounding of basic-level
concepts. We evaluate the model language learning ability through
language-to-visual and visual-to-language tests with high order abstract
concepts. Experimental results demonstrate the proficiency of the model in both
language understanding and language naming tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, Tat-seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit hallucinations due to incorrect or
outdated knowledge. Hence, model editing methods have emerged to enable
targeted knowledge updates. To achieve this, a prevailing paradigm is the
locating-then-editing approach, which first locates influential parameters and
then edits them by introducing a perturbation. While effective, current studies
have demonstrated that this perturbation inevitably disrupt the originally
preserved knowledge within LLMs, especially in sequential editing scenarios. To
address this, we introduce AlphaEdit, a novel solution that projects
perturbation onto the null space of the preserved knowledge before applying it
to the parameters. We theoretically prove that this projection ensures the
output of post-edited LLMs remains unchanged when queried about the preserved
knowledge, thereby mitigating the issue of disruption. Extensive experiments on
various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts
the performance of most locating-then-editing methods by an average of 36.4%
with a single line of additional code for projection solely. Our code is
available at: https://github.com/jianghoucheng/AlphaEdit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listening to the Wise Few: Select-and-Copy Attention Heads for
  Multiple-Choice QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Tulchinskii, Laida Kushnareva, Kristian Kuznetsov, Anastasia Voznyuk, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A standard way to evaluate the abilities of LLM involves presenting a
multiple-choice question and selecting the option with the highest logit as the
model's predicted answer. However, such a format for evaluating LLMs has
limitations, since even if the model knows the correct answer, it may struggle
to select the corresponding letter simply due to difficulties in following this
rigid format. To address this, we introduce new scores that better capture and
reveal model's underlying knowledge: the Query-Key Score (QK-score), derived
from the interaction between query and key representations in attention heads,
and the Attention Score, based on attention weights. These scores are extracted
from specific \textit{select-and-copy} heads, which show consistent performance
across popular Multi-Choice Question Answering (MCQA) datasets. Based on these
scores, our method improves knowledge extraction, yielding up to 16\% gain for
LLaMA2-7B and up to 10\% for larger models on popular MCQA benchmarks. At the
same time, the accuracy on a simple synthetic dataset, where the model
explicitly knows the right answer, increases by almost 60\%, achieving nearly
perfect accuracy, therefore demonstrating the method's efficiency in mitigating
MCQA format limitations. To support our claims, we conduct experiments on
models ranging from 7 billion to 70 billion parameters in both zero- and
few-shot setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Much Can RAG Help the Reasoning of LLM? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Liu, Jiaen Lin, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has gained significant popularity in
modern Large Language Models (LLMs) due to its effectiveness in introducing new
knowledge and reducing hallucinations. However, the deep understanding of RAG
remains limited, how does RAG help the reasoning process and can RAG help
improve the reasoning capability remains question. While external documents are
typically considered as a method to incorporate domain-specific information,
they also contain intermediate reasoning results related to the query, this
suggests that documents could enhance the reasoning capability of LLMs, which
has not been previously explored. In this paper, we investigate this issue in
depth and find that while RAG can assist with reasoning, the help is limited.
If we conceptualize the reasoning process as a tree with fixed depth, then RAG
struggles to assist LLMs in performing deeper reasoning. Additionally, the
information in the documents requires preprocessing to filter out noise. We
demonstrate that this preprocessing is difficult to achieve simply fine-tuning
of the LLM, it often necessitates numerous additional transformer layers to
solve the problem. To simplify the problem, we propose DPrompt tuning, which
effectively resolves the issue within just limited transformer layers, leading
to improved performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Chen, Zhentao Tan, Tao Gong, Yue Wu, Qi Chu, Bin Liu, Jieping Ye, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a manner to augment pre-trained large language models (LLM), knowledge
injection is critical to develop vertical domain large models and has been
widely studied. Although most current approaches, including parameter-efficient
fine-tuning (PEFT) and block expansion methods, uniformly apply knowledge
across all LLM layers, it raises the question: are all layers equally crucial
for knowledge injection? We begin by evaluating the importance of each layer in
finding the optimal layer range for knowledge injection. Intuitively, the more
important layers should play a more critical role in knowledge injection and
deserve a denser injection. We observe performance dips in question-answering
benchmarks after the removal or expansion of the shallow layers, and the
degradation shrinks as the layer gets deeper, indicating that the shallow
layers hold the key to knowledge injection. This insight leads us to propose
the S strategy, a post-pretraining strategy of selectively enhancing shallow
layers while pruning the less effective deep ones. Based on this strategy, we
introduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the
corpus of code $\&$ math and demonstrated the effectiveness of our strategy.
Further experiments across different LLM, Mistral-7B, and a legal corpus
confirmed the general applicability of the approach, underscoring its
wide-ranging efficacy. Our code is available at:
\https://github.com/txchen-USTC/Llama-Slayer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-edits Are Preferences Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Berger, Stefan Riezler, Miriam Exel, Matthias Huck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) techniques are currently one of the state of the
art techniques for fine-tuning large language models (LLMs) on pairwise
preference feedback from human annotators. However, in machine translation,
this sort of feedback can be difficult to solicit. Additionally, Kreutzer et
al. (2018) have shown that, for machine translation, pairwise preferences are
less reliable than other forms of human feedback, such as 5-point ratings.
  We examine post-edits to see if they can be a source of reliable human
preferences by construction. In PO, a human annotator is shown sequences $s_1$
and $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for
post-editing, editors \emph{create} $s_1$ and know that it should be better
than $s_2$. We attempt to use these implicit preferences for PO and show that
it helps the model move towards post-edit-like hypotheses and away from machine
translation-like hypotheses. Furthermore, we show that best results are
obtained by pre-training the model with supervised fine-tuning (SFT) on
post-edits in order to promote post-edit-like hypotheses to the top output
ranks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Ninth Conference on Machine Translation (WMT24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Lifu Tu, Daqing He, Yingbo Zhou, Semih Yavuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phrases are fundamental linguistic units through which humans convey
semantics. This study critically examines the capacity of API-based large
language models (LLMs) to comprehend phrase semantics, utilizing three
human-annotated datasets. We assess the performance of LLMs in executing phrase
semantic reasoning tasks guided by natural language instructions and explore
the impact of common prompting techniques, including few-shot demonstrations
and Chain-of-Thought reasoning. Our findings reveal that LLMs greatly
outperform traditional embedding methods across the datasets; however, they do
not show a significant advantage over fine-tuned methods. The effectiveness of
advanced prompting strategies shows variability. We conduct detailed error
analyses to interpret the limitations faced by LLMs in comprehending phrase
semantics. Code and data can be found at
https://github.com/memray/llm_phrase_semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse
  Representation Adjustment in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Compound Sentences Simple to Analyze: Learning to Split Sentences
  for Aspect-based Sentiment Analysis <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsik Seo, Sungwon Song, Ryang Heo, Jieyong Kim, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods
have shown promising results and achieved substantial advancements. However,
despite these advancements, the tasks of extracting sentiment quadruplets,
which capture the nuanced sentiment expressions within a sentence, remain
significant challenges. In particular, compound sentences can potentially
contain multiple quadruplets, making the extraction task increasingly difficult
as sentence complexity grows. To address this issue, we are focusing on
simplifying sentence structures to facilitate the easier recognition of these
elements and crafting a model that integrates seamlessly with various ABSA
tasks. In this paper, we propose Aspect Term Oriented Sentence Splitter
(ATOSS), which simplifies compound sentence into simpler and clearer forms,
thereby clarifying their structure and intent. As a plug-and-play module, this
approach retains the parameters of the ABSA model while making it easier to
identify essential intent within input sentences. Extensive experimental
results show that utilizing ATOSS outperforms existing methods in both ASQP and
ACOS tasks, which are the primary tasks for extracting sentiment quadruplets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Findings, long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Graph Lear<span class="highlight-title">ner</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Kaveh Hassani, Si Zhang, Hanqing Zeng, Michihiro Yasunaga, Limei Wang, Dongqi Fu, Ning Yao, Bo Long, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are increasingly challenging the dominance of
domain-specific models, including Graph Neural Networks (GNNs) and Graph
Transformers (GTs), in graph learning tasks. Following this trend, we propose a
novel approach that empowers off-the-shelf LMs to achieve performance
comparable to state-of-the-art GNNs on node classification tasks, without
requiring any architectural modification. By preserving the LM's original
architecture, our approach retains a key benefit of LM instruction tuning: the
ability to jointly train on diverse datasets, fostering greater flexibility and
efficiency. To achieve this, we introduce two key augmentation strategies: (1)
Enriching LMs' input using topological and semantic retrieval methods, which
provide richer contextual information, and (2) guiding the LMs' classification
process through a lightweight GNN classifier that effectively prunes class
candidates. Our experiments on real-world datasets show that backbone Flan-T5
models equipped with these augmentation strategies outperform state-of-the-art
text-output node classifiers and are comparable to top-performing vector-output
node classifiers. By bridging the gap between specialized task-specific node
classifiers and general LMs, this work paves the way for more versatile and
widely applicable graph learning models. We will open-source the code upon
publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Second-Order Neural Network Optimization via Adaptive Trust
  Region Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order optimization methods offer notable advantages in training deep
neural networks by utilizing curvature information to achieve faster
convergence. However, traditional second-order techniques are computationally
prohibitive, primarily due to the large matrix inversions and high memory
demands they require. While adaptive trust-region methods have been developed
to mitigate these issues, their performance is often hindered by conservative
estimates of key parameters, such as the Lipschitz constant of the Hessian,
resulting in suboptimal outcomes. In this paper, we introduce
SecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to
overcome these limitations. SOAA approximates the Fisher information matrix
using a diagonal representation, reducing computational complexity from
\(O(n^{2})\) to \(O(n)\), thereby making it suitable for large-scale deep
learning models, including large language models (LLMs). Additionally, the
algorithm integrates an adaptive trust-region mechanism that dynamically
adjusts the trust region size based on observed loss reduction, ensuring both
robust convergence and computational efficiency. We empirically demonstrate
that SOAA achieves faster and more stable convergence compared to first-order
optimizers, such as Adam, under similar computational constraints. However, the
diagonal approximation of the Fisher information matrix may be less effective
in capturing higher-order interactions between gradients, suggesting potential
areas for further refinement and future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation and Navigation in the Vocabulary Key Representation Space of
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letian Peng, Chenyang An, Jingbo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) decoding is based on the next-token prediction (NTP)
probability distribution. For neural LMs (e.g., Transformer-based), NTP
distribution is essentially a softmax-regularized dot product between an
encoded input context (query) and fixed vocabulary representations (keys). In
this paper, we study the effect of the key distribution on the NTP
distribution, with a focus on whether the similarity between keys will trigger
spurious correlations in NTP. Through knowledge-probing tasks, we show that in
the NTP distribution, the few top-ranked tokens are typically accurate.
However, the middle-ranked prediction is highly biased towards the tokens that
are distributionally (not necessarily semantically) similar to these top ones.
For instance, if "P" is predicted as the top-1 token, "A"-"Z" will all be
ranked high in NTP, no matter whether they can lead to correct decoding
results. This hurts the sampling diversity and makes the sampling of correct,
long-tail results hopeless and noisy. We attempt to alleviate this issue via a
novel in-context method that iteratively pushes the query representation away
from explored regions. Specifically, we include the explored decoding results
in the context and prompt the LM to generate something else, which encourages
the LM to produce a query representation that has small dot products with
explored keys. Experiments on knowledge-probing tasks show that our method
leads to efficient navigation away from explored keys to correct new keys. We
further extend our method to open-ended and chain-of-thought (for reasoning)
generation. Experiment results show that ICN contributes to better generation
diversity and improved self-consistency voting performance. Finally, we discuss
potential training issues caused by the fixed key space together with the
challenges and possible ways to address them in future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Morphological evaluation of subwords vocabulary used by BETO language
  model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Óscar García-Sierra, Ana Fernández-Pampillón Cesteros, Miguel Ortega-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subword tokenization algorithms used by Large Language Models are
significantly more efficient and can independently build the necessary
vocabulary of words and subwords without human intervention. However, those
subwords do not always align with real morphemes, potentially impacting the
models' performance, though it remains uncertain when this might occur. In
previous research, we proposed a method to assess the morphological quality of
vocabularies, focusing on the overlap between these vocabularies and the
morphemes of a given language. Our evaluation method was built on three quality
measures, relevance, cohesion, and morphological accuracy, and a procedure for
their assessment. By applying this method to vocabularies created by three
subword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that
these vocabularies generally exhibit very low morphological quality. In this
article, we apply this evaluation to the tokenizer of BETO, a BERT language
model trained on large Spanish corpora. This evaluation, along with our
previous results, helped us conclude that its vocabulary has a low
morphological quality, and we also found that training the tokenizer in a
larger corpus does not improve the morphological quality of the generated
vocabulary. Additionally, this evaluation helps clarify the algorithm used by
the tokenizer, that is, Wordpiece, given the inconsistencies between the
authors' claims and the model's configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Spanish language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Amalvy, Vincent Labatut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Novelties corpus is a collection of novels (and parts of novels)
annotated for Named Entity Recognition (NER) among other tasks. This document
describes the guidelines applied during its annotation. It contains the
instructions used by the annotators, as well as a number of examples retrieved
from the annotated novels, and illustrating expressions that should be marked
as entities as well as expressions that should not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which questions should I answer? Salience Prediction of Inquisitive
  Questions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inquisitive questions -- open-ended, curiosity-driven questions people ask as
they read -- are an integral part of discourse processing (Kehler and Rohde,
2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has
taken advantage of question generation capabilities of LLMs to enhance a wide
range of applications. But the space of inquisitive questions is vast: many
questions can be evoked from a given context. So which of those should be
prioritized to find answers? Linguistic theories, unfortunately, have not yet
provided an answer to this question. This paper presents QSALIENCE, a salience
predictor of inquisitive questions. QSALIENCE is instruction-tuned over our
dataset of linguist-annotated salience scores of 1,766 (context, question)
pairs. A question scores high on salience if answering it would greatly enhance
the understanding of the text (Van Rooy, 2003). We show that highly salient
questions are empirically more likely to be answered in the same article,
bridging potential questions (Onea, 2016) with Questions Under Discussion
(Roberts, 2012). We further validate our findings by showing that answering
salient questions is an indicator of summarization quality in news.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready for EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: Language Model Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach to using Large Language Models (LLMs) for
classification tasks in an explainable way. Unlike ML models that rely heavily
on data cleaning and feature engineering, this method streamlines the process
using LLMs. This paper proposes a new concept called "Language Model Learning
(LML)" powered by a new method called "Data-Augmented Prediction (DAP)". The
classification is performed by LLMs using a method similar to humans manually
exploring and understanding the data and deciding classifications using data as
a reference. In the LML process, a dataset is summarized and evaluated to
determine the features that lead to the classification of each label the most.
In the process of DAP, the system uses the data summary and a row of the
testing dataset to automatically generate a query, which is used to retrieve
relevant rows from the dataset. A classification is generated by the LLM using
data summary and relevant rows, ensuring satisfactory accuracy even with
complex data using context-aware decision-making. LML and DAP unlock the
possibilities of new applications. The proposed method uses the words "Act as
an Explainable Machine Learning Model" in the prompt to enhance the
interpretability of the predictions by allowing users to review the logic
behind each prediction. In some test cases, the system scored an accuracy above
90%, proving the effectiveness of the system and its potential to outperform
conventional ML models in various scenarios. The code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated title, abstract, and images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Falling Short: The Curse of Tokenization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models typically tokenize raw text into sequences of subword
identifiers from a predefined vocabulary, a process inherently sensitive to
typographical errors, length variations, and largely oblivious to the internal
structure of tokens--issues we term the curse of tokenization. In this study,
we delve into these drawbacks and demonstrate that large language models (LLMs)
remain susceptible to these problems. This study systematically investigates
these challenges and their impact on LLMs through three critical research
questions: (1) complex problem solving, (2) token structure probing, and (3)
resilience to typographical variation. Our findings reveal that scaling model
parameters can mitigate the issue of tokenization; however, LLMs still suffer
from biases induced by typos and other text format variations. Our experiments
show that subword regularization such as BPE-dropout can mitigate this issue.
We release our evaluation code and data at https://github.com/FloatAI/TKEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Training Data Influence of <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Shuohuan Wang, Yu Sun, Qiwei Peng, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid advancements in generative language models, the
investigation of how training data shapes the performance of GPT models is
still emerging. This paper presents GPTfluence, a novel approach that leverages
a featurized simulation to assess the impact of training examples on the
training dynamics of GPT models. Our approach not only traces the influence of
individual training instances on performance trajectories, such as loss and
other key metrics, on targeted test points but also enables a comprehensive
comparison with existing methods across various training scenarios in GPT
models, ranging from 14 million to 2.8 billion parameters, across a range of
downstream tasks. Contrary to earlier methods that struggle with generalization
to new data, GPTfluence introduces a parameterized simulation of training
dynamics, demonstrating robust generalization capabilities to unseen training
data. This adaptability is evident across both fine-tuning and
instruction-tuning scenarios, spanning tasks in natural language understanding
and generation. We make our code and data publicly available at
https://github.com/ernie-research/gptfluence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pharmacy<span class="highlight-title">GPT</span>: The AI Pharmacist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Quanzheng Li, Xiang Li, Brian Murray, Tianming Liu, Andrea Sikora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce PharmacyGPT, a novel framework to assess the
capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in
emulating the role of clinical pharmacists. Our methodology encompasses the
utilization of LLMs to generate comprehensible patient clusters, formulate
medication plans, and forecast patient outcomes. We conduct our investigation
using real data acquired from the intensive care unit (ICU) at the University
of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable
insights into the potential applications and limitations of LLMs in the field
of clinical pharmacy, with implications for both patient care and the
development of future AI-driven healthcare solutions. By evaluating the
performance of PharmacyGPT, we aim to contribute to the ongoing discourse
surrounding the integration of artificial intelligence in healthcare settings,
ultimately promoting the responsible and efficacious use of such technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive <span class="highlight-title">Pre-Train</span>ing on Pixels and Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Qingyi Liu, Jingwu Xiao, Shuohuan Wang, Yu Sun, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of visual and textual information represents a promising
direction in the advancement of language models. In this paper, we explore the
dual modality of language--both visual and textual--within an autoregressive
framework, pre-trained on both document images and texts. Our method employs a
multimodal training strategy, utilizing visual data through next patch
prediction with a regression head and/or textual data through next token
prediction with a classification head. We focus on understanding the
interaction between these two modalities and their combined impact on model
performance. Our extensive evaluation across a wide range of benchmarks shows
that incorporating both visual and textual data significantly improves the
performance of pixel-based language models. Remarkably, we find that a
unidirectional pixel-based model trained solely on visual data can achieve
comparable results to state-of-the-art bidirectional models on several language
understanding tasks. This work uncovers the untapped potential of integrating
visual and textual modalities for more effective language modeling. We release
our code, data, and model checkpoints at
\url{https://github.com/ernie-research/pixelgpt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is
  Needed? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tannon Kew, Florian Schottmann, Rico Sennrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of today's large language models (LLMs) are
English-centric, having been pretrained predominantly on English text. Yet, in
order to meet user expectations, models need to be able to respond
appropriately in multiple languages once deployed in downstream applications.
This requires strong cross-lingual transfer abilities. In this work, we
investigate the minimal amount of multilinguality required during finetuning to
elicit cross-lingual generalisation in English-centric LLMs. In experiments
across four LLMs, we find that multilingual instruction tuning with as few as
two to three languages is both necessary and sufficient to elicit effective
cross-lingual generalisation, with the limiting factor being the degree to
which a target language is seen during pretraining. Evaluations on five
different tasks further reveal that multilingual instruction tuning is most
beneficial for generative tasks that assume input/output language agreement,
such as in chat settings, while being of less importance for highly structured
classification-style tasks. Our code and data is available at
https://github.com/ZurichNLP/multilingual-instruction-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
  <span class="highlight-title">Large Language Model</span>s Using Only Attention Maps <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When asked to summarize articles or answer questions given a passage, large
language models (LLMs) can hallucinate details and respond with unsubstantiated
answers that are inaccurate with respect to the input context. This paper
describes a simple approach for detecting such contextual hallucinations. We
hypothesize that contextual hallucinations are related to the extent to which
an LLM attends to information in the provided context versus its own
generations. Based on this intuition, we propose a simple hallucination
detection model whose input features are given by the ratio of attention
weights on the context versus newly generated tokens (for each attention head).
We find that a linear classifier based on these lookback ratio features is as
effective as a richer detector that utilizes the entire hidden states of an LLM
or a text-based entailment model. The lookback ratio-based detector -- Lookback
Lens -- is found to transfer across tasks and even models, allowing a detector
that is trained on a 7B model to be applied (without retraining) to a larger
13B model. We further apply this detector to mitigate contextual
hallucinations, and find that a simple classifier-guided decoding approach is
able to reduce the amount of hallucination, for example by 9.6% in the XSum
summarization task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference long paper. The source code is available
  at https://github.com/voidism/Lookback-Lens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mystery of In-Context Learning: A Comprehensive <span class="highlight-title">Survey</span> on
  Interpretation and Analysis <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding in-context learning (ICL) capability that enables large
language models (LLMs) to excel in proficiency through demonstration examples
is of utmost importance. This importance stems not only from the better
utilization of this capability across various tasks, but also from the
proactive identification and mitigation of potential risks, including concerns
regarding truthfulness, bias, and toxicity, that may arise alongside the
capability. In this paper, we present a thorough survey on the interpretation
and analysis of in-context learning. First, we provide a concise introduction
to the background and definition of in-context learning. Then, we give an
overview of advancements from two perspectives: 1) a theoretical perspective,
emphasizing studies on mechanistic interpretability and delving into the
mathematical foundations behind ICL; and 2) an empirical perspective,
concerning studies that empirically analyze factors associated with ICL. We
conclude by highlighting the challenges encountered and suggesting potential
avenues for future research. We believe that our work establishes the basis for
further exploration into the interpretation of in-context learning.
Additionally, we have created a repository containing the resources referenced
in our survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of EMNLP 2024. Resources are
  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Automated Code Vul<span class="highlight-title">ner</span>ability Repair using <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the complex challenge of automated repair of code
vulnerabilities, vital for enhancing digital security in an increasingly
technology-driven world. The study introduces a novel and efficient format for
the representation of code modification, using advanced Large Language Models
(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets
featuring C code vulnerabilities, significantly improve the accuracy and
adaptability of automated code repair techniques. A key finding is the enhanced
repair accuracy of these models when compared to previous methods such as
VulRepair, which underscores their practical utility and efficiency. The
research also offers a critical assessment of current evaluation metrics, such
as perfect predictions, and their limitations in reflecting the true
capabilities of automated repair models in real-world scenarios. Following
this, it underscores the importance of using test datasets devoid of train
samples, emphasizing the need for dataset integrity to enhance the
effectiveness of LLMs in code repair tasks. The significance of this work is
its contribution to digital security, setting new standards for automated code
vulnerability repair and paving the way for future advancements in the fields
of cybersecurity and artificial intelligence. The study does not only highlight
the potential of LLMs in enhancing code security but also fosters further
exploration and research in these crucial areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Limited Ge<span class="highlight-title">ner</span>alization Capability of the Implicit Reward Model
  Induced by Direct Preference Optimization <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is an effective approach
for aligning language models to human preferences. Central to RLHF is learning
a reward function for scoring human preferences. Two main approaches for
learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in
RLHF, and 2) using an implicit reward learned from preference data through
methods such as Direct Preference Optimization (DPO). Prior work has shown that
the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in
the limit. DPORM's effectiveness directly implies the optimality of the learned
policy, and also has practical implication for LLM alignment methods including
iterative DPO. However, it is unclear how well DPORM empirically matches the
performance of EXRM. This work studies the accuracy at distinguishing preferred
and rejected answers for both DPORM and EXRM. Our findings indicate that even
though DPORM fits the training dataset comparably, it generalizes less
effectively than EXRM, especially when the validation datasets contain
distribution shifts. Across five out-of-distribution settings, DPORM has a mean
drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that
DPORM has limited generalization ability and substantiates the integration of
an explicit reward model in iterative DPO approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking LLMs with Arabic Transliteration and Arabizi <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study identifies the potential vulnerabilities of Large Language Models
(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and
its various forms. While most research has concentrated on English-based prompt
manipulation, our investigation broadens the scope to investigate the Arabic
language. We initially tested the AdvBench benchmark in Standardized Arabic,
finding that even with prompt manipulation techniques like prefix injection, it
was insufficient to provoke LLMs into generating unsafe content. However, when
using Arabic transliteration and chatspeak (or arabizi), we found that unsafe
content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3
Sonnet. Our findings suggest that using Arabic and its various forms could
expose information that might remain hidden, potentially increasing the risk of
jailbreak attacks. We hypothesize that this exposure could be due to the
model's learned connection to specific words, highlighting the need for more
comprehensive safety training across all language forms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for
  Children's Story-Based Learning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive story reading is a common parent-child activity, where parents
expect to teach both language skills and real-world knowledge beyond the story.
While increasing storytelling and reading systems have been developed for this
activity, they often fail to infuse real-world knowledge into the conversation.
This limitation can be attributed to the existing question-answering (QA)
datasets used for children's education, upon which the systems are built,
failing to capture the nuances of how education experts think when conducting
interactive story reading activities. To bridge this gap, we design an
annotation framework, empowered by existing knowledge graph to capture experts'
annotations and thinking process, and leverage this framework to construct
StorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with
real-world knowledge. We conduct automated and human expert evaluations across
various QA pair generation settings to demonstrate that our StorySparkQA can
effectively support models in generating QA pairs that target real-world
knowledge beyond story content. StorySparkQA is available at
https://huggingface.co/datasets/NEU-HAI/StorySparkQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM
  Reliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to communicate uncertainty, risk, and limitation is crucial for
the safety of large language models. However, current evaluations of these
abilities rely on simple calibration, asking whether the language generated by
the model matches appropriate probabilities. Instead, evaluation of this aspect
of LLM communication should focus on the behaviors of their human
interlocutors: how much do they rely on what the LLM says? Here we introduce an
interaction-centered evaluation framework called Rel-A.I. (pronounced "rely"})
that measures whether humans rely on LLM generations. We use this framework to
study how reliance is affected by contextual features of the interaction (e.g,
the knowledge domain that is being discussed), or the use of greetings
communicating warmth or competence (e.g., "I'm happy to help!"). We find that
contextual characteristics significantly affect human reliance behavior. For
example, people rely 10% more on LMs when responding to questions involving
calculations and rely 30% more on LMs that are perceived as more competent. Our
results show that calibration and language quality alone are insufficient in
evaluating the risks of human-LM interactions, and illustrate the need to
consider features of the interactional context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Leakage of Code Ge<span class="highlight-title">ner</span>ation Evaluation <span class="highlight-title">Dataset</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gallé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider contamination by code generation test sets, in
particular in their use in modern large language models. We discuss three
possible sources of such contamination and show findings supporting each of
them: (i) direct data leakage, (ii) indirect data leakage through the use of
synthetic data and (iii) overfitting to evaluation sets during model selection.
To address this, we release Less Basic Python Problems (LBPP): an
uncontaminated new benchmark of 161 prompts with their associated Python
solutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. 5 main pages, 9 in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Refusal Training in LLMs Ge<span class="highlight-title">ner</span>alize to the Past Tense? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,
GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,
o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For
example, the success rate of this simple attack on GPT-4o increases from 1%
using direct requests to 88% using 20 past tense reformulation attempts on
harmful requests from JailbreakBench with GPT-4 as a jailbreak judge.
Interestingly, we also find that reformulations in the future tense are less
effective, suggesting that refusal guardrails tend to consider past historical
questions more benign than hypothetical future questions. Moreover, our
experiments on fine-tuning GPT-3.5 Turbo show that defending against past
reformulations is feasible when past tense examples are explicitly included in
the fine-tuning data. Overall, our findings highlight that the widely used
alignment techniques -- such as SFT, RLHF, and adversarial training -- employed
to align the studied models can be brittle and do not always generalize as
intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update in v3: o1-mini and o1-preview results (on top of GPT-4o and
  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at
  https://github.com/tml-epfl/llm-past-tense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Immunization against harmful fine-tuning attacks <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often trained with safety guards intended to
prevent harmful text generation. However, such safety training can be removed
by fine-tuning the LLM on harmful datasets. While this emerging threat (harmful
fine-tuning attacks) has been characterized by previous work, there is little
understanding of how we should proceed in constructing and validating defenses
against these attacks especially in the case where defenders would not have
control of the fine-tuning process. We introduce a formal framework based on
the training budget of an attacker which we call "Immunization" conditions.
Using a formal characterisation of the harmful fine-tuning problem, we provide
a thorough description of what a successful defense must comprise of and
establish a set of guidelines on how rigorous defense research that gives us
confidence should proceed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundations of <span class="highlight-title">Large Language Model</span> Compression -- Part 1: Weight
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, compression of large language models (LLMs) has emerged as
an important problem to enable language model deployment on
resource-constrained devices, reduce computational costs, and mitigate the
environmental footprint of large-scale AI infrastructure. In this paper, we lay
down the foundation for LLM quantization from a convex optimization perspective
and propose a quantization technique that builds on this foundation for optimum
quantization outcomes. Our quantization framework, CVXQ, scales to models
containing hundreds of billions of weight parameters and provides users with
the flexibility to compress models to any specified model size, post-training.
A reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 17 pages, 4 figures, 5 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EIA: Environmental Injection Attack on Ge<span class="highlight-title">ner</span>alist Web Agents for Privacy
  Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">PRompt</span> Optimization in Multi-Step Tasks (PROMST): Integrating Human
  Feedback and Heuristic-based Sampling <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08702v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08702v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization aims to find the best prompt to a large language model
(LLM) for a given task. LLMs have been successfully used to help find and
improve prompt candidates for single-step tasks. However, realistic tasks for
agents are multi-step and introduce new challenges: (1) Prompt content is
likely to be more extensive and complex, making it more difficult for LLMs to
analyze errors, (2) the impact of an individual step is difficult to evaluate,
and (3) different people may have varied preferences about task execution.
While humans struggle to optimize prompts, they are good at providing feedback
about LLM outputs; we therefore introduce a new LLM-driven discrete prompt
optimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that
incorporates human-designed feedback rules to automatically offer direct
suggestions for improvement. We also use an extra learned heuristic model that
predicts prompt performance to efficiently sample from prompt candidates. This
approach significantly outperforms both human-engineered prompts and several
other prompt optimization methods across 11 representative multi-step tasks (an
average 10.6\%-29.3\% improvement to current best methods on five LLMs
respectively). We believe our work can serve as a benchmark for automatic
prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are
available at https://github.com/yongchao98/PROMST. Project Page is available at
https://yongchao98.github.io/MIT-REALM-PROMST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 14 figures, Published in EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARAMANU-AYN: <span class="highlight-title">Pretrain</span> from scratch or Continual <span class="highlight-title">Pretrain</span>ing of LLMs for
  Legal Domain Adaptation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Paramanu-Ayn, a collection of legal language models
trained exclusively on Indian legal case documents. This 97-million-parameter
Auto-Regressive (AR) decoder-only model was pretrained from scratch with a
context size of 8192 on a single GPU for just 185 hours, achieving an efficient
MFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We
evaluated our model using perplexity and zero-shot tasks: case judgment
prediction with explanation and abstractive case summarization. Paramanu-Ayn
outperformed Llama-2 7B and Gemini-Pro in case judgment prediction with
explanation task on test accuracy by nearly 2 percentage points, despite being
72 times smaller. In zero-shot abstractive summarization, it surpassed
decoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10
percentage points in BLEU and METEOR metrics, and by nearly 4 percentage points
in BERTScore. Further evaluations on zero-shot commonsense and mathematical
benchmarks showed that Paramanu-Ayn excelled despite being trained exclusively
on legal documents, outperforming Llama-1, Llama-2, and Falcon on
AGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our
model on 10,763 diverse legal tasks, including legal clause generation, legal
drafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above
8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by
GPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge
and generalize to draft legal contracts and legal clauses with limited
instruction-tuning. Hence, we conclude that for a strong domain-specialized
generative language model (such as legal), domain specialized pretraining from
scratch is more cost effective, environmentally friendly, and remains
competitive with larger models or even better than adapting LLMs for legal
domain tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose Leopard, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large
  Language Models Attentive Readers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art Large Language Models (LLMs) are accredited with an
increasing number of different capabilities, ranging from reading
comprehension, over advanced mathematical and reasoning skills to possessing
scientific knowledge. In this paper we focus on their multi-hop reasoning
capability: the ability to identify and integrate information from multiple
textual sources.
  Given the concerns with the presence of simplifying cues in existing
multi-hop reasoning benchmarks, which allow models to circumvent the reasoning
requirement, we set out to investigate, whether LLMs are prone to exploiting
such simplifying cues. We find evidence that they indeed circumvent the
requirement to perform multi-hop reasoning, but they do so in more subtle ways
than what was reported about their fine-tuned pre-trained language model (PLM)
predecessors. Motivated by this finding, we propose a challenging multi-hop
reasoning benchmark, by generating seemingly plausible multi-hop reasoning
chains, which ultimately lead to incorrect answers. We evaluate multiple open
and proprietary state-of-the-art LLMs, and find that their performance to
perform multi-hop reasoning is affected, as indicated by up to 45% relative
decrease in F1 score when presented with such seemingly plausible alternatives.
We conduct a deeper analysis and find evidence that while LLMs tend to ignore
misleading lexical cues, misleading reasoning paths indeed present a
significant challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at
  Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
https://github.com/QwenLM/Qwen2-VL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin
  note: text overlap with arXiv:2408.15262 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Llamipa: An Incremental Discourse Parser <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18256v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18256v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides the first discourse parsing experiments with a large
language model(LLM) finetuned on corpora annotated in the style of SDRT
(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,
2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),
that leverages discourse context, leading to substantial performance gains over
approaches that use encoder-only models to provide local, context-sensitive
representations of discourse units. Furthermore, it can process discourse data
incrementally, which is essential for the eventual use of discourse information
in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nebula: A discourse aware Minecraft Builder <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Chaturvedi, Kate Thompson, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We finetune an LLM to
predict actions based on prior context; our model, Nebula, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongForm: Effective Instruction Tuning with Reverse Instructions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enables language models to more effectively generalize and
better follow user intent. However, obtaining instruction data is costly and
challenging. Prior work employs methods such as expensive human annotation,
crowd-sourced datasets with alignment issues, and generating noisy examples via
LLMs. We introduce the LongForm-C dataset, which is created by reverse
instructions. We generate instructions via LLMs for human-written corpus
examples using reverse instructions. First we select a diverse set of
human-written documents from corpora such as C4 and Wikipedia; then we generate
instructions for these documents via LLMs. This approach provides a cheaper and
cleaner instruction-tuning dataset with natural output and one suitable for
long text generation. Our models outperform 10x larger language models without
instruction tuning on tasks such as story/recipe generation and long-form
question answering. Moreover, LongForm models outperform prior
instruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and
improve language understanding capabilities further. We publicly release our
data and models: https://github.com/akoksal/LongForm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. This version extends the training with recent
  LLMs, evaluation with new metrics, and NLU tasks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TurkishMMLU: Measuring Massive Multitask Language Understanding in
  Turkish <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arda Yüksel, Abdullatif Köksal, Lütfi Kerem Şenel, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple choice question answering tasks evaluate the reasoning,
comprehension, and mathematical abilities of Large Language Models (LLMs).
While existing benchmarks employ automatic translation for multilingual
evaluation, this approach is error-prone and potentially introduces culturally
biased questions, especially in social sciences. We introduce the first
multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'
understanding of the Turkish language. TurkishMMLU includes over 10,000
questions, covering 9 different subjects from Turkish high-school education
curricula. These questions are written by curriculum experts, suitable for the
high-school curricula in Turkey, covering subjects ranging from natural
sciences and math questions to more culturally representative topics such as
Turkish Literature and the history of the Turkish Republic. We evaluate over 20
LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),
closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)
models. We provide an extensive evaluation, including zero-shot and few-shot
evaluation of LLMs, chain-of-thought reasoning, and question difficulty
analysis along with model performance. We provide an in-depth analysis of the
Turkish capabilities and limitations of current LLMs to provide insights for
future LLMs for the Turkish language. We publicly release our code for the
dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ge<span class="highlight-title">ner</span>ate-on-Graph: Treat LLM as both Agent and KG in Incomplete
  Knowledge Graph Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Guang Liu, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the issues of insufficient knowledge and hallucination in Large
Language Models (LLMs), numerous studies have explored integrating LLMs with
Knowledge Graphs (KGs). However, these methods are typically evaluated on
conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where
all factual triples required for each question are entirely covered by the
given KG. In such cases, LLMs primarily act as an agent to find answer entities
within the KG, rather than effectively integrating the internal knowledge of
LLMs and external knowledge sources such as KGs. In fact, KGs are often
incomplete to cover all the knowledge required to answer questions. To simulate
these real-world scenarios and evaluate the ability of LLMs to integrate
internal and external knowledge, we propose leveraging LLMs for QA under
Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the
factual triples for each question, and construct corresponding datasets. To
handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),
which can generate new factual triples while exploring KGs. Specifically, GoG
performs reasoning through a Thinking-Searching-Generating framework, which
treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets
demonstrate that our GoG outperforms all previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Ge<span class="highlight-title">ner</span>alization Complexity for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown exceptional capabilities in
understanding complex queries and performing sophisticated tasks, their
generalization abilities are often deeply entangled with memorization,
necessitating more precise evaluation. To address this challenge, we introduce
Scylla, a dynamic evaluation framework that quantitatively measures the
generalization abilities of LLMs. Scylla disentangles generalization from
memorization via assessing model performance on both in-distribution (ID) and
out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.
Through extensive experiments, we uncover a non-monotonic relationship between
task complexity and the performance gap between ID and OOD data, which we term
the generalization valley. Specifically, this phenomenon reveals a critical
threshold - referred to as critical complexity - where reliance on
non-generalizable behavior peaks, indicating the upper bound of LLMs'
generalization capabilities. As model size increases, the critical complexity
shifts toward higher levels of task complexity, suggesting that larger models
can handle more complex reasoning tasks before over-relying on memorization.
Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs
including both open-sourced models such as LLaMA and Qwen families, and
close-sourced models like Claude and GPT, providing a more robust evaluation
and establishing a clearer understanding of LLMs' generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-Instruct: Adapting Instruction Ge<span class="highlight-title">ner</span>ators for Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyun Cui, Qianle Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instructions augmentation is a crucial step for unleashing the full potential
of large language models (LLMs) in downstream tasks. Existing Self-Instruct
methods primarily simulate new instructions from a few initial instructions
with in-context learning. However, our study identifies a critical flaw in this
approach: even with GPT4o, Self-Instruct cannot generate complex instructions
of length $\ge 100$, which is necessary in complex tasks such as code
completion.
  To address this issue, our key insight is that fine-tuning open source LLMs
with only ten examples can produce complex instructions that maintain
distributional consistency for complex reasoning tasks. We introduce
Ada-Instruct, an adaptive instruction generator developed through fine-tuning.
We empirically validated Ada-Instruct's efficacy across different applications.
The results highlight Ada-Instruct's capacity to generate long, intricate, and
distributionally consistent instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Editing: Learning Knowledge from Self-Induced Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Qi, Bangcheng Yang, Kailin Jiang, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In scenarios where language models must incorporate new information
efficiently without extensive retraining, traditional fine-tuning methods are
prone to overfitting, degraded generalization, and unnatural language
generation. To address these limitations, we introduce Consistent In-Context
Editing (ICE), a novel approach leveraging the model's in-context learning
capability to optimize toward a contextual distribution rather than a one-hot
target. ICE introduces a simple yet effective optimization framework for the
model to internalize new knowledge by aligning its output distributions with
and without additional context. This method enhances the robustness and
effectiveness of gradient-based tuning methods, preventing overfitting and
preserving the model's integrity. We analyze ICE across four critical aspects
of knowledge editing: accuracy, locality, generalization, and linguistic
quality, demonstrating its advantages. Experimental results confirm the
effectiveness of ICE and demonstrate its potential for continual editing,
ensuring that the integrity of the model is preserved while updating
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fighting Randomness with Randomness: Mitigating Optimisation Instability
  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branislav Pecher, Jan Cegin, Robert Belanec, Jakub Simko, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While fine-tuning of pre-trained language models generally helps to overcome
the lack of labelled training samples, it also displays model performance
instability. This instability mainly originates from randomness in
initialisation or data shuffling. To address this, researchers either modify
the training process or augment the available samples, which typically results
in increased computational costs. We propose a new mitigation strategy, called
Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths
of ensembling, noise regularisation and model interpolation, while retaining
computational efficiency. We compare DENI with 9 representative mitigation
strategies across 3 models, 4 tuning strategies and 7 text classification
datasets. We show that: 1) DENI outperforms the best performing mitigation
strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation
strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,
outperforming full fine-tuning in specific cases; and 3) combining DENI with
data augmentation often leads to even more effective instability mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of the EMNLP'24 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Sensitivity of Learning with Limited Labelled Data to the Effects of
  Randomness: Impact of Interactions and Systematic Choices <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branislav Pecher, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the EMNLP'24 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding
  for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Jian Guan, Wei Wu, Zhengtao Yu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables are ubiquitous across various domains for concisely representing
structured information. Empowering large language models (LLMs) to reason over
tabular data represents an actively explored direction. However, since typical
LLMs only support one-dimensional~(1D) inputs, existing methods often flatten
the two-dimensional~(2D) table structure into a sequence of tokens, which can
severely disrupt the spatial relationships and result in an inevitable loss of
vital contextual information. In this paper, we first empirically demonstrate
the detrimental impact of such flattening operations on the performance of LLMs
in capturing the spatial information of tables through two elaborate proxy
tasks. Subsequently, we introduce a simple yet effective positional encoding
method, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to
address this challenge. 2D-TPE enables each attention head to dynamically
select a permutation order of tokens within the context for attending to them,
where each permutation represents a distinct traversal mode for the table, such
as column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of
losing essential spatial information while preserving computational efficiency,
thus better preserving the table structure. Extensive experiments across five
benchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring
the importance of preserving the table structure for accurate table
comprehension. Comprehensive analysis further reveals the substantially better
scalability of 2D-TPE to large tables than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and
  Multi-Level Style Control <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot singing voice synthesis (SVS) with style transfer and style control
aims to generate high-quality singing voices with unseen timbres and styles
(including singing method, emotion, rhythm, technique, and pronunciation) from
audio and text prompts. However, the multifaceted nature of singing styles
poses a significant challenge for effective modeling, transfer, and control.
Furthermore, current SVS models often fail to generate singing voices rich in
stylistic nuances for unseen singers. To address these challenges, we introduce
TCSinger, the first zero-shot SVS model for style transfer across cross-lingual
speech and singing styles, along with multi-level style control. Specifically,
TCSinger proposes three primary modules: 1) the clustering style encoder
employs a clustering vector quantization model to stably condense style
information into a compact latent space; 2) the Style and Duration Language
Model (S\&D-LM) concurrently predicts style information and phoneme duration,
which benefits both; 3) the style adaptive decoder uses a novel mel-style
adaptive normalization method to generate singing voices with enhanced details.
Experimental results show that TCSinger outperforms all baseline models in
synthesis quality, singer similarity, and style controllability across various
tasks, including zero-shot style transfer, multi-level style control,
cross-lingual style transfer, and speech-to-singing style transfer. Singing
voice samples can be accessed at https://tcsinger.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18045v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18045v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the factuality of long-form large language model (LLM)-generated
text is an important challenge. Recently there has been a surge of interest in
factuality evaluation for English, but little is known about the factuality
evaluation of multilingual LLMs, specially when it comes to long-form
generation. %This paper systematically evaluates multilingual LLMs' factual
accuracy across languages and geographic regions. We introduce a simple
pipeline for multilingual factuality evaluation, by applying FActScore (Min et
al., 2023) for diverse languages. In addition to evaluating multilingual
factual generation, we evaluate the factual accuracy of long-form text
generation in topics that reflect regional diversity. We also examine the
feasibility of running the FActScore pipeline using non-English Wikipedia and
provide comprehensive guidelines on multilingual factual evaluation for
regionally diverse topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latte: Latent Attention for Linear Time <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Dolga, Marius Cobzarenco, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time complexity of the standard attention mechanism in transformers
scales quadratically with sequence length. We propose a probabilistic framework
for attention, enabling us to derive a novel low-rank linear
re-parameterisation of both bidirectional and causal cases, based on defining a
latent variable model. Our method can be seamlessly integrated as a drop-in
replacement for the standard attention mechanism. Additionally, this framework
provides a natural extension for combining local standard attention with our
global linear attention. This approach allows us to extend the context length
of existing large pre-trained models with only a few additional training steps.
The resulting ``Latte Transformer'' achieves performance comparable to standard
attention and other state-of-the-art models, while maintaining linear time and
memory complexity, along with constant-time next-token prediction during
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twists, Humps, and Pebbles: Multilingual Speech Recognition Models
  Exhibit Gender Performance Gaps <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automatic speech recognition (ASR) models are designed to be used
across many languages and tasks without substantial changes. However, this
broad language coverage hides performance gaps within languages, for example,
across genders. Our study systematically evaluates the performance of two
widely used multilingual ASR models on three datasets, encompassing 19
languages from eight language families and two speaking conditions. Our
findings reveal clear gender disparities, with the advantaged group varying
across languages and models. Surprisingly, those gaps are not explained by
acoustic or lexical properties. However, probing internal model states reveals
a correlation with gendered performance gap. That is, the easier it is to
distinguish speaker gender in a language using probes, the more the gap
reduces, favoring female speakers. Our results show that gender disparities
persist even in state-of-the-art models. Our findings have implications for the
improvement of multilingual ASR systems, underscoring the importance of
accessibility to training data and nuanced evaluation to predict and mitigate
gender gaps. We release all code and artifacts at
https://github.com/g8a9/multilingual-asr-gender-gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024. Code and artifacts at
  https://github.com/g8a9/multilingual-asr-gender-gap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Feedback in Scripted versus Spontaneous Dialogues: A
  Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ildikó Pilán, Laurent Prévot, Hendrik Buschmeier, Pierre Lison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scripted dialogues such as movie and TV subtitles constitute a widespread
source of training data for conversational NLP models. However, there are
notable linguistic differences between these dialogues and spontaneous
interactions, especially regarding the occurrence of communicative feedback
such as backchannels, acknowledgments, or clarification requests. This paper
presents a quantitative analysis of such feedback phenomena in both subtitles
and spontaneous conversations. Based on conversational data spanning eight
languages and multiple genres, we extract lexical statistics, classifications
from a dialogue act tagger, expert annotations and labels derived from a
fine-tuned Large Language Model (LLM). Our main empirical findings are that (1)
communicative feedback is markedly less frequent in subtitles than in
spontaneous dialogues and (2) subtitles contain a higher proportion of negative
feedback. We also show that dialogues generated by standard LLMs lie much
closer to scripted dialogues than spontaneous interactions in terms of
communicative feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version for SIGdial 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ miniCTX: Neural Theorem Proving with (Long-)Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiewen Hu, Thomas Zhu, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world formal theorem proving often depends on a wealth of context,
including definitions, lemmas, comments, file structure, and other information.
We introduce miniCTX, which tests a model's ability to prove formal
mathematical theorems that depend on new context that is not seen during
training. miniCTX contains theorems sourced from real Lean projects and
textbooks, each associated with a context that can span tens of thousands of
tokens. Models are tasked with proving a theorem given access to code from the
theorem's repository, which contains context that is needed for the proof. As a
baseline for miniCTX, we tested fine-tuning and prompting methods that
condition theorem proving on preceding context. Both approaches substantially
outperform traditional methods that rely solely on state information. We found
that this ability to use context is not captured by previous benchmarks such as
miniF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting
and annotating theorem proving data, making it easy to add new projects into
miniCTX to ensure that contexts are not seen during training. miniCTX offers a
challenging and realistic evaluation of neural theorem provers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Hardness of Code in <span class="highlight-title">Large Language Model</span>s -- A
  Probabilistic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common practice in large language model (LLM) usage for complex analytical
tasks such as code generation, is to sample a solution for the entire task
within the model's context window. Previous works have shown that subtask
decomposition within the model's context (chain of thought), is beneficial for
solving such tasks. In this work, we point a limitation of LLMs' ability to
perform several sub-tasks within the same context window - an in-context
hardness of composition, pointing to an advantage for distributing a decomposed
problem in a multi-agent system of LLMs. The hardness of composition is
quantified by a generation complexity metric, i.e., the number of LLM
generations required to sample at least one correct solution. We find a gap
between the generation complexity of solving a compositional problem within the
same context relative to distributing it among multiple agents, that increases
exponentially with the solution's length. We prove our results theoretically
and demonstrate them empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Automatic Metrics with Incremental Machine Translation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojun Wu, Shay B. Cohen, Rico Sennrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a dataset comprising commercial machine translations, gathered
weekly over six years across 12 translation directions. Since human A/B testing
is commonly used, we assume commercial systems improve over time, which enables
us to evaluate machine translation (MT) metrics based on their preference for
more recent translations. Our study not only confirms several prior findings,
such as the advantage of neural metrics over non-neural ones, but also explores
the debated issue of how MT quality affects metric reliability--an
investigation that smaller datasets in previous research could not sufficiently
explore. Overall, our research demonstrates the dataset's value as a testbed
for metric evaluation. We release our code at https://github.com/gjwubyron/Evo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Chain-of-Thought: Augmenting <span class="highlight-title">Large Language Model</span>s by Reasoning on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), while exhibiting exceptional performance,
suffer from hallucinations, especially on knowledge-intensive tasks. Existing
works propose to augment LLMs with individual text units retrieved from
external knowledge corpora to alleviate the issue. However, in many domains,
texts are interconnected (e.g., academic papers in a bibliographic graph are
linked by citations and co-authorships) which form a (text-attributed) graph.
The knowledge in such graphs is encoded not only in single texts/nodes but also
in their associated connections. To facilitate the research of augmenting LLMs
with graphs, we manually construct a Graph Reasoning Benchmark dataset called
GRBench, containing 1,740 questions that can be answered with the knowledge
from 10 domain graphs. Then, we propose a simple and effective framework called
Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging
LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of
three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We
conduct systematic experiments with three LLM backbones on GRBench, where
Graph-CoT outperforms the baselines consistently. The code is available at
https://github.com/PeterGriffinJin/Graph-CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Instruction-following Abilities of <span class="highlight-title">Large Language Model</span>s with
  Task-aware Curriculum Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning aims to align large language models (LLMs) with
open-domain instructions and human-preferred responses. While several studies
have explored autonomous approaches to distilling and annotating instructions
from powerful proprietary LLMs, such as ChatGPT, they often neglect the impact
of the distributions and characteristics of tasks, together with the varying
difficulty of instructions in training sets. This oversight can lead to
imbalanced knowledge capabilities and poor generalization powers of student
LLMs. To address these challenges, we introduce Task-Aware Curriculum Planning
for Instruction Refinement (TAPIR), a multi-round distillation framework that
utilizes an oracle LLM to select instructions that are difficult for a student
LLM to follow. To balance the student's capabilities, task distributions in
training sets are adjusted with responses automatically refined according to
their corresponding tasks. In addition, by incorporating curriculum planning,
our approach systematically escalates the difficulty levels of tasks,
progressively enhancing the student LLM's capabilities. We rigorously evaluate
TAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,
MT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that
student LLMs, trained with our method and less training data, outperform larger
instruction-tuned models and strong distillation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>emnlp 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic <span class="highlight-title">Survey</span> and Critical <span class="highlight-title">Review</span> on Evaluating Large Language
  Models: Challenges, Limitations, and Recommendations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tahmid Rahman Laskar, Sawsan Alqahtani, M Saiful Bari, Mizanur Rahman, Mohammad Abdullah Matin Khan, Haidar Khan, Israt Jahan, Amran Bhuiyan, Chee Wei Tan, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty, Jimmy Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently gained significant attention due
to their remarkable capabilities in performing diverse tasks across various
domains. However, a thorough evaluation of these models is crucial before
deploying them in real-world applications to ensure they produce reliable
performance. Despite the well-established importance of evaluating LLMs in the
community, the complexity of the evaluation process has led to varied
evaluation setups, causing inconsistencies in findings and interpretations. To
address this, we systematically review the primary challenges and limitations
causing these inconsistencies and unreliable evaluations in various steps of
LLM evaluation. Based on our critical review, we present our perspectives and
recommendations to ensure LLM evaluations are reproducible, reliable, and
robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span>s on Graphs: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as GPT4 and LLaMA, are creating
significant advancements in natural language processing, due to their strong
text encoding/decoding ability and newly found emergent capability (e.g.,
reasoning). While LLMs are mainly designed to process pure texts, there are
many real-world scenarios where text data is associated with rich structure
information in the form of graphs (e.g., academic networks, and e-commerce
networks) or scenarios where graph data is paired with rich textual information
(e.g., molecules with descriptions). Besides, although LLMs have shown their
pure text-based reasoning ability, it is underexplored whether such ability can
be generalized to graphs (i.e., graph-based reasoning). In this paper, we
provide a systematic review of scenarios and techniques related to large
language models on graphs. We first summarize potential scenarios of adopting
LLMs on graphs into three categories, namely pure graphs, text-attributed
graphs, and text-paired graphs. We then discuss detailed techniques for
utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM
as Aligner, and compare the advantages and disadvantages of different schools
of models. Furthermore, we discuss the real-world applications of such methods
and summarize open-source codes and benchmark datasets. Finally, we conclude
with potential future research directions in this fast-growing field. The
related source can be found at
https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tradeoffs Between Alignment and Helpfulness in Language Models with
  Representation Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16332v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16332v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model alignment has become an important component of AI safety,
allowing safe interactions between humans and language models, by enhancing
desired behaviors and inhibiting undesired ones. It is often done by tuning the
model or inserting preset aligning prompts. Recently, representation
engineering, a method which alters the model's behavior via changing its
representations post-training, was shown to be effective in aligning LLMs (Zou
et al., 2023a). Representation engineering yields gains in alignment oriented
tasks such as resistance to adversarial attacks and reduction of social biases,
but was also shown to cause a decrease in the ability of the model to perform
basic tasks. In this paper we study the tradeoff between the increase in
alignment and decrease in helpfulness of the model. We propose a theoretical
framework which provides bounds for these two quantities, and demonstrate their
relevance empirically. First, we find that under the conditions of our
framework, alignment can be guaranteed with representation engineering, and at
the same time that helpfulness is harmed in the process. Second, we show that
helpfulness is harmed quadratically with the norm of the representation
engineering vector, while the alignment increases linearly with it, indicating
a regime in which it is efficient to use representation engineering. We
validate our findings empirically, and chart the boundaries to the usefulness
of representation engineering for alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting In-Context Learning in Vision-Language Models for Videos
  Through Curated Data Distributional Properties <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17041v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17041v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major reason behind the recent success of large language models (LLMs) is
their \textit{in-context learning} capability, which makes it possible to
rapidly adapt them to downstream text-based tasks by prompting them with a
small number of relevant demonstrations. While large vision-language models
(VLMs) have recently been developed for tasks requiring both text and images,
they largely lack in-context learning over visual information, especially in
understanding and generating text about videos. In this work, we implement
\textbf{E}mergent \textbf{I}n-context \textbf{Le}arning on \textbf{V}ideos
(\eilev{}), a novel training paradigm that induces in-context learning over
video and text by capturing key properties of pre-training data found by prior
work to be essential for in-context learning in transformers. In our
experiments, we show that \eilev-trained models outperform other off-the-shelf
VLMs in few-shot video narration for novel, rare actions. Furthermore, we
demonstrate that these key properties of bursty distributions, skewed marginal
distributions, and dynamic meaning each contribute to varying degrees to VLMs'
in-context learning capability in narrating procedural videos. Our results,
analysis, and \eilev{}-trained models yield numerous insights about the
emergence of in-context learning over video and text, creating a foundation for
future work to optimize and scale VLMs for open-domain video understanding and
reasoning. Our code and demo are available at
\url{https://github.com/yukw777/EILEV}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, LaTeX; Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Instruction Tuning Make LLMs More Consistent? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15206v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15206v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Jiaang Li, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of instruction tuning is enabling zero-shot performance, but
instruction tuning has also been shown to improve chain-of-thought reasoning
and value alignment (Si et al., 2023). Here we consider the impact on
$\textit{consistency}$, i.e., the sensitivity of language models to small
perturbations in the input. We compare 10 instruction-tuned LLaMA models to the
original LLaMA-7b model and show that almost across-the-board they become more
consistent, both in terms of their representations and their predictions in
zero-shot and downstream tasks. We explain these improvements through
mechanistic analyses of factual recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We need to run extra experiments to ensure some of the claims in the
  paper are fully correct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGD: Multi-LLM Based Agent Debugger via Refinement and Ge<span class="highlight-title">ner</span>ation
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Jin, Zechao Sun, Huaming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown incredible potential in code
generation tasks, and recent research in prompt engineering have enhanced LLMs'
understanding of textual information. However, ensuring the accuracy of
generated code often requires extensive testing and validation by programmers.
While LLMs can typically generate code based on task descriptions, their
accuracy remains limited, especially for complex tasks that require a deeper
understanding of both the problem statement and the code generation process.
This limitation is primarily due to the LLMs' need to simultaneously comprehend
text and generate syntactically and semantically correct code, without having
the capability to automatically refine the code. In real-world software
development, programmers rarely produce flawless code in a single attempt based
on the task description alone, they rely on iterative feedback and debugging to
refine their programs. Inspired by this process, we introduce a novel
architecture of LLM-based agents for code generation and automatic debugging:
Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based
agent debugger that leverages three distinct LLM agents-Guide Agent, Debug
Agent, and Feedback Agent. RGD decomposes the code generation task into
multiple steps, ensuring a clearer workflow and enabling iterative code
refinement based on self-reflection and feedback. Experimental results
demonstrate that RGD exhibits remarkable code generation capabilities,
achieving state-of-the-art performance with a 9.8% improvement on the HumanEval
dataset and a 16.2% improvement on the MBPP dataset compared to the
state-of-the-art approaches and traditional direct prompting approaches. We
highlight the effectiveness of the RGD framework in enhancing LLMs' ability to
generate and refine code autonomously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic continued <span class="highlight-title">pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candès, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining on large-scale, unstructured internet text enables language
models to acquire a significant amount of world knowledge. However, this
knowledge acquisition is data-inefficient--to learn a given fact, models must
be trained on hundreds to thousands of diverse representations of it. This
poses a challenge when adapting a pretrained model to a small corpus of
domain-specific documents, where each fact may appear rarely or only once. We
propose to bridge this gap with synthetic continued pretraining: using the
small domain-specific corpus to synthesize a large corpus more amenable to
learning, and then performing continued pretraining on the synthesized corpus.
We instantiate this proposal with EntiGraph, a synthetic data augmentation
algorithm that extracts salient entities from the source documents and then
generates diverse text by drawing connections between the sampled entities.
Synthetic continued pretraining with EntiGraph enables a language model to
answer questions and follow generic instructions related to the source
documents without access to them. If, instead, the source documents are
available at inference time, we show that the knowledge acquired through our
approach compounds with retrieval-augmented generation. To better understand
these results, we build a simple mathematical model of EntiGraph, and show how
synthetic data augmentation can "rearrange" knowledge to enable more
data-efficient learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated organization of experimental results and methods
  introduction. Released the dataset and model weights artifact</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The SIFo Benchmark: Investigating the Sequential Instruction Following
  Ability of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, Christof Monz, Arianna Bisazza, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following multiple instructions is a crucial ability for large language
models (LLMs). Evaluating this ability comes with significant challenges: (i)
limited coherence between multiple instructions, (ii) positional bias where the
order of instructions affects model performance, and (iii) a lack of
objectively verifiable tasks. To address these issues, we introduce a benchmark
designed to evaluate models' abilities to follow multiple instructions through
sequential instruction following (SIFo) tasks. In SIFo, the successful
completion of multiple instructions is verifiable by examining only the final
instruction. Our benchmark evaluates instruction following using four tasks
(text modification, question answering, mathematics, and security rules), each
assessing different aspects of sequential instruction following. Our evaluation
of popular LLMs, both closed-source and open-source, shows that more recent and
larger models significantly outperform their older and smaller counterparts on
the SIFo tasks, validating the benchmark's effectiveness. All models struggle
with following sequences of instructions, hinting at an important lack of
robustness of today's language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Synopses of Movie Narratives: A <span class="highlight-title">Dataset</span> for Vision-Language
  Story Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Sun, Jianfei Yu, Boyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story video-text alignment, a core task in computational story understanding,
aims to align video clips with corresponding sentences in their descriptions.
However, progress on the task has been held back by the scarcity of manually
annotated video-text correspondence and the heavy concentration on English
narrations of Hollywood movies. To address these issues, in this paper, we
construct a large-scale multilingual video story dataset named Multilingual
Synopses of Movie Narratives (M-SYMON), containing 13,166 movie summary videos
from 7 languages, as well as manual annotation of fine-grained video-text
correspondences for 101.5 hours of video. Training on the human annotated data
from SyMoN outperforms the SOTA methods by 15.7 and 16.2 percentage points on
Clip Accuracy and Sentence IoU scores, respectively, demonstrating the
effectiveness of the annotations. As benchmarks for future research, we create
6 baseline approaches with different multilingual training strategies, compare
their performance in both intra-lingual and cross-lingual setups, exemplifying
the challenges of multilingual video-text alignment. The dataset is released
at: https://github.com/insundaycathy/M-SyMoN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Potential and Challenges of Evaluating Attitudes, Opinions, and
  Values in <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11096v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11096v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolei Ma, Xinpeng Wang, Tiancheng Hu, Anna-Carolina Haensch, Michael A. Hedderich, Barbara Plank, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have sparked wide interest in
validating and comprehending the human-like cognitive-behavioral traits LLMs
may capture and convey. These cognitive-behavioral traits include typically
Attitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within
LLMs remains opaque, and different evaluation methods may yield different
results. This has led to a lack of clarity on how different studies are related
to each other and how they can be interpreted. This paper aims to bridge this
gap by providing a comprehensive overview of recent works on the evaluation of
AOVs in LLMs. Moreover, we survey related approaches in different stages of the
evaluation pipeline in these works. By doing so, we address the potential and
challenges with respect to understanding the model, human-AI alignment, and
downstream application in social sciences. Finally, we provide practical
insights into evaluation methods, model enhancement, and interdisciplinary
collaboration, thereby contributing to the evolving landscape of evaluating
AOVs in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language models and brains align due to more than next-word prediction
  and word-level information <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Merlin, Mariya Toneva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models have been shown to significantly predict brain
recordings of people comprehending language. Recent work suggests that the
prediction of the next word is a key mechanism that contributes to this
alignment. What is not yet understood is whether prediction of the next word is
necessary for this observed alignment or simply sufficient, and whether there
are other shared mechanisms or information that are similarly important. In
this work, we take a step towards understanding the reasons for brain alignment
via two simple perturbations in popular pretrained language models. These
perturbations help us design contrasts that can control for different types of
information. By contrasting the brain alignment of these differently perturbed
models, we show that improvements in alignment with brain recordings are due to
more than improvements in next-word prediction and word-level information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lexically Grounded Subword Segmentation <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jindřich Libovický, Jindřich Helcl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present three innovations in tokenization and subword segmentation. First,
we propose to use unsupervised morphological analysis with Morfessor as
pre-tokenization. Second, we present an algebraic method for obtaining subword
embeddings grounded in a word embedding space. Based on that, we design a novel
subword segmentation algorithm that uses the embeddings, ensuring that the
procedure considers lexical meaning. Third, we introduce an efficient
segmentation algorithm based on a subword bigram model that can be initialized
with the lexically aware segmentation method to avoid using Morfessor and large
embedding tables at inference time. We evaluate the proposed approaches using
two intrinsic metrics and measure their performance on two downstream tasks:
part-of-speech tagging and machine translation. Our experiments show
significant improvements in the morphological plausibility of the segmentation
when evaluated using segmentation precision on morpheme boundaries and improved
R\'enyi efficiency in 8 languages. Although the proposed tokenization methods
do not have a large impact on automatic translation quality, we observe
consistent performance gains in the arguably more morphological task of
part-of-speech tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready, EMNLP Main conf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13663v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13663v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirui Qi, Gabriele Sarti, Raquel Fernández, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference. Code and data released at
  https://github.com/Betswish/MIRAGE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a
  Hybrid Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expanding the long-context capabilities of Multi-modal Large Language
Models~(MLLMs) is crucial for video understanding, high-resolution image
understanding, and multi-modal agents. This involves a series of systematic
optimizations, including model architecture, data construction and training
strategy, particularly addressing challenges such as \textit{degraded
performance with more images} and \textit{high computational costs}. In this
paper, we adapt the model architecture to a hybrid of Mamba and Transformer
blocks, approach data construction with both temporal and spatial dependencies
among multiple images and employ a progressive training strategy. The released
model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first
hybrid MLLM, which achieved a better balance between efficiency and
effectiveness. LongLLaVA not only achieves competitive results across various
benchmarks, but also maintains high throughput and low memory consumption.
Especially, it could process nearly a thousand images on a single A100 80GB
GPU, showing promising application prospects for a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Wizard: Task-Aware <span class="highlight-title">Prompt</span> Optimization Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshaan Agarwal, Joykirat Singh, Vivek Dani, Raghav Magazine, Tanuja Ganu, Akshay Nambi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed AI across diverse domains, with
prompting being central to their success in guiding model outputs. However,
manual prompt engineering is both labor-intensive and domain-specific,
necessitating the need for automated solutions. We introduce PromptWizard, a
novel, fully automated framework for discrete prompt optimization, utilizing a
self-evolving, self-adapting mechanism. Through a feedback-driven critique and
synthesis process, PromptWizard achieves an effective balance between
exploration and exploitation, iteratively refining both prompt instructions and
in-context examples to generate human-readable, task-specific prompts. This
guided approach systematically improves prompt quality, resulting in superior
performance across 45 tasks. PromptWizard excels even with limited training
data, smaller LLMs, and various LLM architectures. Additionally, our cost
analysis reveals a substantial reduction in API calls, token usage, and overall
cost, demonstrating PromptWizard's efficiency, scalability, and advantages over
existing prompt optimization strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Adversarial Vul<span class="highlight-title">ner</span>ability of Pairwise Evaluation Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawon Jeong, ChaeHun Park, Jimin Hong, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise evaluation using large language models (LLMs) is widely adopted for
evaluating generated outputs. However, the reliability of LLM evaluators is
often compromised by their biased preferences, such as favoring verbosity and
an authoritative tone. In this work, we find that the evaluation setup itself
can significantly amplify these biases, where pairwise evaluators exhibit more
undesirable tendencies than pointwise evaluators. Our analysis further reveals
that even when pairwise evaluators make incorrect judgments, they can still
accurately identify shortcomings in low-quality outputs. As a simple remedy, we
also propose incorporating pointwise reasoning into pairwise evaluation.
Experimental results show that our method improves the performance of pairwise
evaluators on adversarial samples across various models. We hope our findings
encourage further exploration into the reliability of LLM evaluators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ge<span class="highlight-title">ner</span>alists vs. Specialists: Evaluating <span class="highlight-title">Large Language Model</span>s for Urdu 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Abdul Hameed Azeemi, Agha Ali Raza, Awais Athar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b,
with special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that
have been fine-tuned on specific tasks. We focus on seven classification and
seven generation tasks to evaluate the performance of these models on Urdu
language. Urdu has 70 million native speakers, yet it remains underrepresented
in Natural Language Processing (NLP). Despite the frequent advancements in
Large Language Models (LLMs), their performance in low-resource languages,
including Urdu, still needs to be explored. We also conduct a human evaluation
for the generation tasks and compare the results with the evaluations performed
by GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose
models consistently outperform general-purpose models across various tasks. We
also find that the evaluation done by GPT-4-Turbo for generation tasks aligns
more closely with human evaluation compared to the evaluation the evaluation
done by Llama-3-8b. This paper contributes to the NLP community by providing
insights into the effectiveness of general and specific-purpose LLMs for
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Temporal Extrapolation of Multimodal <span class="highlight-title">Large Language Model</span>s
  with Temporal Grounding Bridge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Yang Liu, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in multimodal large language models (MLLMs), the challenge
of interpreting long-form videos in response to linguistic queries persists,
largely due to the inefficiency in temporal grounding and limited pre-trained
context window size. In this work, we introduce Temporal Grounding Bridge
(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding
capabilities and broadens their contextual scope. Our framework significantly
enhances the temporal capabilities of current MLLMs through three key
innovations: an efficient multi-span temporal grounding algorithm applied to
low-dimension temporal features projected from flow; a multimodal length
extrapolation training paradigm that utilizes low-dimension temporal features
to extend the training context window size; and a bootstrapping framework that
bridges our model with pluggable MLLMs without requiring annotation. We
validate TGB across seven video benchmarks and demonstrate substantial
performance improvements compared with prior MLLMs. Notably, our model,
initially trained on sequences of four frames, effectively handles sequences up
to 16 longer without sacrificing performance, highlighting its scalability and
effectiveness in real-world applications. Our code is publicly available at
https://github.com/bigai-nlco/VideoTGB
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wavelet<span class="highlight-title">GPT</span>: Wavelets Meet <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have ushered in a new wave of artificial
intelligence advancements impacting every scientific field and discipline. They
are trained on a simple objective: to predict the next token given the previous
context. We live in a world where most of the data around us, e.g., text,
audio, and music, has a multi-scale structure associated with it. This paper
infuses LLMs with traditional signal processing ideas, namely wavelets, during
pre-training to take advantage of the structure. Without adding \textbf{any
extra parameters} to a GPT-style LLM architecture, we achieve the same
pre-training performance almost twice as fast in text, raw audio, and symbolic
music. This is achieved by imposing a structure on intermediate embeddings.
When trained for the same number of training steps, we achieve significant
gains in performance, which is comparable to pre-training a larger neural
architecture. Our architecture allows every next token prediction access to
intermediate embeddings at different temporal resolutions in every Transformer
decoder block. This work will hopefully pave the way for incorporating
multi-rate signal processing ideas into traditional LLM pre-training. Further,
we showcase pushing model performance by improving internal structure instead
of just going after scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Hallucination in Large Language, Image, Video
  and Audio Foundation Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09589v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09589v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of foundation models (FMs) across language, image,
audio, and video domains has shown remarkable capabilities in diverse tasks.
However, the proliferation of FMs brings forth a critical challenge: the
potential to generate hallucinated outputs, particularly in high-stakes
applications. The tendency of foundation models to produce hallucinated content
arguably represents the biggest hindrance to their widespread adoption in
real-world scenarios, especially in domains where reliability and accuracy are
paramount. This survey paper presents a comprehensive overview of recent
developments that aim to identify and mitigate the problem of hallucination in
FMs, spanning text, image, video, and audio modalities. By synthesizing recent
advancements in detecting and mitigating hallucination across various
modalities, the paper aims to provide valuable insights for researchers,
developers, and practitioners. Essentially, it establishes a clear framework
encompassing definition, taxonomy, and detection strategies for addressing
hallucination in multimodal foundation models, laying the foundation for future
research in this pivotal area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information
  Funneling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, Wen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate whether attention-based information flow inside
large language models (LLMs) is aggregated through noticeable patterns for long
context processing. Our observations reveal that LLMs aggregate information
through Pyramidal Information Funneling where attention is scattering widely in
lower layers, progressively consolidating within specific contexts, and
ultimately focusing on critical tokens (a.k.a massive activation or attention
sink) in higher layers. Motivated by these insights, we developed PyramidKV, a
novel and effective KV cache compression method. This approach dynamically
adjusts the KV cache size across different layers, allocating more cache in
lower layers and less in higher ones, diverging from traditional methods that
maintain a uniform KV cache size. Our experimental evaluations, utilizing the
LongBench benchmark, show that PyramidKV matches the performance of models with
a full KV cache while retaining only 12% of the KV cache, thus significantly
reducing memory usage. In scenarios emphasizing memory efficiency, where only
0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache
compression techniques, achieving up to a 20.5 absolute accuracy improvement on
TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms
competing methods in maintaining long-context comprehension in LLMs; notably,
retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve
100% Acc. performance, matching that of a full KV cache.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Constructed Context Decompilation with Fined-grained Alignment
  Enhancement <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Feng, Dechuan Teng, Yang Xu, Honglin Mu, Xiao Xu, Libo Qin, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompilation transforms compiled code back into a high-level programming
language for analysis when source code is unavailable. Previous work has
primarily focused on enhancing decompilation performance by increasing the
scale of model parameters or training data for pre-training. Based on the
characteristics of the decompilation task, we propose two methods: (1) Without
fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method
recompiles the LLM's decompilation results to construct pairs for in-context
learning, helping the model improve decompilation performance. (2) Fine-grained
Alignment Enhancement (FAE), which meticulously aligns assembly code with
source code at the statement level by leveraging debugging information, is
employed during the fine-tuning phase to achieve further improvements in
decompilation. By integrating these two methods, we achieved a Re-Executability
performance improvement of approximately 3.90% on the Decompile-Eval benchmark,
establishing a new state-of-the-art performance of 52.41%. The code, data, and
models are available at https://github.com/AlongWY/sccdec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring language relations through syntactic distances and geographic
  proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan De Gregorio, Raúl Toral, David Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Languages are grouped into families that share common linguistic traits.
While this approach has been successful in understanding genetic relations
between diverse languages, more analyses are needed to accurately quantify
their relatedness, especially in less studied linguistic levels such as syntax.
Here, we explore linguistic distances using series of parts of speech (POS)
extracted from the Universal Dependencies dataset. Within an
information-theoretic framework, we show that employing POS trigrams maximizes
the possibility of capturing syntactic variations while being at the same time
compatible with the amount of available data. Linguistic connections are then
established by assessing pairwise distances based on the POS distributions.
Intriguingly, our analysis reveals definite clusters that correspond to well
known language families and groups, with exceptions explained by distinct
morphological typologies. Furthermore, we obtain a significant correlation
between language similarity and geographic distance, which underscores the
influence of spatial proximity on language kinships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Analysis of <span class="highlight-title">Large Language Model</span>s as Soft Reaso<span class="highlight-title">ner</span>s: The
  Case of Syllogistic Inferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of Large Language Models (LLMs) are becoming a
central focus of study in NLP. In this paper, we consider the case of
syllogistic reasoning, an area of deductive reasoning studied extensively in
logic and cognitive psychology. Previous research has shown that pre-trained
LLMs exhibit reasoning biases, such as $\textit{content effects}$, avoid
answering that $\textit{no conclusion follows}$, display human-like
difficulties, and struggle with multi-step reasoning. We contribute to this
research line by systematically investigating the effects of chain-of-thought
reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on
syllogistic reasoning, considering syllogisms with conclusions that support or
violate world knowledge, as well as ones with multiple premises. Crucially, we
go beyond the standard focus on accuracy, with an in-depth analysis of the
conclusions generated by the models. Our results suggest that the behavior of
pre-trained LLMs can be explained by heuristics studied in cognitive science
and that both ICL and SFT improve model performance on valid inferences,
although only the latter mitigates most reasoning biases without harming model
consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Speculative Sampling for GPU Hardware Accelerators <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Seanie Lee, Ilja Baumann, Philipp Seeberger, Korbinian Riedhammer, Tobias Bocklet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we optimize speculative sampling for parallel hardware
accelerators to improve sampling speed. We notice that substantial portions of
the intermediate matrices necessary for speculative sampling can be computed
concurrently. This allows us to distribute the workload across multiple GPU
threads, enabling simultaneous operations on matrix segments within thread
blocks. This results in profiling time improvements ranging from 6% to 13%
relative to the baseline implementation, without compromising accuracy. To
further accelerate speculative sampling, probability distributions
parameterized by softmax are approximated by sigmoid. This approximation
approach results in significantly greater relative improvements in profiling
time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct
extensive experiments on both automatic speech recognition and summarization
tasks to validate the effectiveness of our optimization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StablePT: Towards Stable <span class="highlight-title">Prompt</span>ing for Few-shot Learning via Input
  Separation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Liu, Chen Liu, Zhaohan Zhang, Chengzhengxu Li, Longtian Wang, Yu Lan, Chao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have shown their ability to become effective few-shot
learners with prompting, revolutionizing the paradigm of learning with data
scarcity. However, this approach largely depends on the quality of prompt
initialization, and always exhibits large variability among different runs.
Such property makes prompt tuning highly unreliable and vulnerable to poorly
constructed prompts, which limits its extension to more real-world
applications. To tackle this issue, we propose to treat the hard prompt and
soft prompt as separate inputs to mitigate noise brought by the prompt
initialization. Furthermore, we optimize soft prompts with contrastive learning
for utilizing class-aware information in the training process to maintain model
performance. Experimental results demonstrate that \sysname outperforms
state-of-the-art methods by 6.97% in accuracy and reduces the standard
deviation by 1.92 on average. Furthermore, extensive experiments underscore its
robustness and stability across 8 datasets covering various tasks. Codes are
available at https://github.com/lccc0528/Stable/tree/main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-02T00:00:00Z">2024-10-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locret: Enhancing Eviction in Long-Context LLM Inference with Trained
  Retaining Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable advances in supporting
long-context comprehension and processing tasks. However, scaling the
generation inference of LLMs to such long contexts incurs significant
additional computation load, and demands a substantial GPU memory footprint to
maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache
compression methods, such as quantization, face memory bottlenecks as context
length increases, while static-sized caches, such as eviction, suffer from
inefficient policies. These limitations restrict deployment on consumer-grade
devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a
framework for long-context LLM inference that introduces retaining heads to
evaluate the causal importance of KV cache units, allowing for more accurate
eviction within a fixed cache size. Locret is fine-tuned on top of the frozen
backbone LLM using a minimal amount of data from standard long-context SFT
datasets. During inference, we evict low-importance cache units along with a
chunked prefill pattern, significantly reducing peak GPU memory usage. We
conduct an extensive empirical study to evaluate Locret, where the experimental
results show that Locret outperforms the recent competitive approaches,
including InfLLM, Quantization, SirLLM, and MInference, in terms of memory
efficiency and the quality of generated contents -- Locret achieves over a 20x
and 8x KV cache compression ratio compared to the full KV cache for
Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined
with other methods, such as quantization and token merging. To our knowledge,
Locret is the first framework capable of deploying Llama-3.1-8B or similar
models on a single Nvidia 4090 GPU, enabling 128K long-context inference
without compromising generation quality, and requiring little additional system
optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Driven Feature Selection and Engineering for Genotype Data
  with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Lee, Shu Yang, Jae Young Baik, Xiaoxi Liu, Zhen Tan, Dawei Li, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting phenotypes with complex genetic bases based on a small,
interpretable set of variant features remains a challenging task.
Conventionally, data-driven approaches are utilized for this task, yet the high
dimensional nature of genotype data makes the analysis and prediction
difficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and
their success in processing complex biomedical concepts, we set to examine the
ability of LLMs in feature selection and engineering for tabular genotype data,
with a novel knowledge-driven framework. We develop FREEFORM, Free-flow
Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling,
designed with chain-of-thought and ensembling principles, to select and
engineer features with the intrinsic knowledge of LLMs. Evaluated on two
distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing
loss, we find this framework outperforms several data-driven methods,
particularly on low-shot regimes. FREEFORM is available as open-source
framework at GitHub: https://github.com/PennShenLab/FREEFORM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loki: An Open-Source Tool for Fact Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Li, Xudong Han, Hao Wang, Yuxia Wang, Minghan Wang, Rui Xing, Yilin Geng, Zenan Zhai, Preslav Nakov, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Loki, an open-source tool designed to address the growing
problem of misinformation. Loki adopts a human-centered approach, striking a
balance between the quality of fact-checking and the cost of human involvement.
It decomposes the fact-checking task into a five-step pipeline: breaking down
long texts into individual claims, assessing their check-worthiness, generating
queries, retrieving evidence, and verifying the claims. Instead of fully
automating the claim verification process, Loki provides essential information
at each step to assist human judgment, especially for general users such as
journalists and content moderators. Moreover, it has been optimized for
latency, robustness, and cost efficiency at a commercially usable level. Loki
is released under an MIT license and is available on GitHub. We also provide a
video presenting the system and its capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When a language model is optimized for reasoning, does it still show
  embers of autoregression? An analysis of OpenAI o1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D. Hardy, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In "Embers of Autoregression" (McCoy et al., 2023), we showed that several
large language models (LLMs) have some important limitations that are
attributable to their origins in next-word prediction. Here we investigate
whether these issues persist with o1, a new system from OpenAI that differs
from previous LLMs in that it is optimized for reasoning. We find that o1
substantially outperforms previous LLMs in many cases, with particularly large
improvements on rare variants of common tasks (e.g., forming acronyms from the
second letter of each word in a list, rather than the first letter). Despite
these quantitative improvements, however, o1 still displays the same
qualitative trends that we observed in previous systems. Specifically, o1 -
like previous LLMs - is sensitive to the probability of examples and tasks,
performing better and requiring fewer "thinking tokens" in high-probability
settings than in low-probability ones. These results show that optimizing a
language model for reasoning can mitigate but might not fully overcome the
language model's probability sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamGarden: A Desig<span class="highlight-title">ner</span> Assistant for Growing Games from a Single <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding assistants are increasingly leveraged in game design, both generating
code and making high-level plans. To what degree can these tools align with
developer workflows, and what new modes of human-computer interaction can
emerge from their use? We present DreamGarden, an AI system capable of
assisting with the development of diverse game environments in Unreal Engine.
At the core of our method is an LLM-driven planner, capable of breaking down a
single, high-level prompt -- a dream, memory, or imagined scenario provided by
a human user -- into a hierarchical action plan, which is then distributed
across specialized submodules facilitating concrete implementation. This system
is presented to the user as a garden of plans and actions, both growing
independently and responding to user intervention via seed prompts, pruning,
and feedback. Through a user study, we explore design implications of this
system, charting courses for future work in semi-autonomous assistants and
open-ended simulation design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages + appendix, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Yang, Jack Cole, Ke Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in artificial intelligence in recent years, such as Large
Language Models (LLMs), have fueled expectations for breakthroughs in genomic
foundation models (GFMs). The code of nature, hidden in diverse genomes since
the very beginning of life's evolution, holds immense potential for impacting
humans and ecosystems through genome modeling. Recent breakthroughs in GFMs,
such as Evo, have attracted significant investment and attention to genomic
modeling, as they address long-standing challenges and transform in-silico
genomic studies into automated, reliable, and efficient paradigms. In the
context of this flourishing era of consecutive technological revolutions in
genomics, GFM studies face two major challenges: the lack of GFM benchmarking
tools and the absence of open-source software for diverse genomics. These
challenges hinder the rapid evolution of GFMs and their wide application in
tasks such as understanding and synthesizing genomes, problems that have
persisted for decades. To address these challenges, we introduce GFMBench, a
framework dedicated to GFM-oriented benchmarking. GFMBench standardizes
benchmark suites and automates benchmarking for a wide range of open-source
GFMs. It integrates millions of genomic sequences across hundreds of genomic
tasks from four large-scale benchmarks, democratizing GFMs for a wide range of
in-silico genomic applications. Additionally, GFMBench is released as
open-source software, offering user-friendly interfaces and diverse tutorials,
applicable for AutoBench and complex tasks like RNA design and structure
prediction. To facilitate further advancements in genome modeling, we have
launched a public leaderboard showcasing the benchmark performance derived from
AutoBench. GFMBench represents a step toward standardizing GFM benchmarking and
democratizing GFM applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yangheng95/OmniGenomeBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings. Website:
  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in
  Neural Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove rich algebraic structures of the solution space for 2-layer neural
networks with quadratic activation and $L_2$ loss, trained on reasoning tasks
in Abelian group (e.g., modular addition). Such a rich structure enables
analytical construction of global optimal solutions from partial solutions that
only satisfy part of the loss, despite its high nonlinearity. We coin the
framework as CoGO (Composing Global Optimizers). Specifically, we show that the
weight space over different numbers of hidden nodes of the 2-layer network is
equipped with a semi-ring algebraic structure, and the loss function to be
optimized consists of monomial potentials, which are ring homomorphism,
allowing partial solutions to be composed into global ones by ring addition and
multiplication. Our experiments show that around $95\%$ of the solutions
obtained by gradient descent match exactly our theoretical constructions.
Although the global optimizers constructed only required a small number of
hidden nodes, our analysis on gradient dynamics shows that
over-parameterization asymptotically decouples training dynamics and is
beneficial. We further show that training dynamics favors simpler solutions
under weight decay, and thus high-order global optimizers such as perfect
memorization are unfavorable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeFine: Enhancing LLM Decision-Making with Factor Profiles and
  Analogical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are ideal for decision-making due to their ability to reason over long
contexts and identify critical factors. However, challenges arise when
processing transcripts of spoken speech describing complex scenarios. These
transcripts often contain ungrammatical or incomplete sentences, repetitions,
hedging, and vagueness. For example, during a company's earnings call, an
executive might project a positive revenue outlook to reassure investors,
despite significant uncertainty regarding future earnings. It is crucial for
LLMs to incorporate this uncertainty systematically when making decisions. In
this paper, we introduce DeFine, a new framework that constructs probabilistic
factor profiles from complex scenarios. DeFine then integrates these profiles
with analogical reasoning, leveraging insights from similar past experiences to
guide LLMs in making critical decisions in novel situations. Our framework
separates the tasks of quantifying uncertainty in complex scenarios and
incorporating it into LLM decision-making. This approach is particularly useful
in fields such as medical consultations, negotiations, and political debates,
where making decisions under uncertainty is vital.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Ge<span class="highlight-title">ner</span>alization Complexity for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown exceptional capabilities in
understanding complex queries and performing sophisticated tasks, their
generalization abilities are often deeply entangled with memorization,
necessitating more precise evaluation. To address this challenge, we introduce
Scylla, a dynamic evaluation framework that quantitatively measures the
generalization abilities of LLMs. Scylla disentangles generalization from
memorization via assessing model performance on both in-distribution (ID) and
out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.
Through extensive experiments, we uncover a non-monotonic relationship between
task complexity and the performance gap between ID and OOD data, which we term
the generalization valley. Specifically, this phenomenon reveals a critical
threshold - referred to as critical complexity - where reliance on
non-generalizable behavior peaks, indicating the upper bound of LLMs'
generalization capabilities. As model size increases, the critical complexity
shifts toward higher levels of task complexity, suggesting that larger models
can handle more complex reasoning tasks before over-relying on memorization.
Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs
including both open-sourced models such as LLaMA and Qwen families, and
close-sourced models like Claude and GPT, providing a more robust evaluation
and establishing a clearer understanding of LLMs' generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich images, where text serves as the central visual element guiding the
overall understanding, are prevalent in real-world applications, such as
presentation slides, scanned documents, and webpage snapshots. Tasks involving
multiple text-rich images are especially challenging, as they require not only
understanding the content of individual images but reasoning about
inter-relationships and logical flows across multiple visual inputs. Despite
the importance of these scenarios, current multimodal large language models
(MLLMs) struggle to handle such tasks due to two key challenges: (1) the
scarcity of high-quality instruction tuning datasets for text-rich multi-image
scenarios, and (2) the difficulty in balancing image resolution with visual
feature sequence length. To address these challenges, we propose \OurMethod, a
MLLM designed specifically for handling vision-language tasks involving
multiple text-rich images. First, we curated about one million high-quality
multimodal instruction-tuning data, tailored to text-rich, multi-image
scenarios. Second, we developed an adaptive high-resolution multi-image
encoding module to dynamically optimize the allocation of visual sequence
length based on the original aspect ratios and resolutions of the input images.
Experiments across a wide range of benchmarks demonstrate our model's superior
capabilities in text-rich, multi-image evaluations and competitive performance
in general domain evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/Jill0001/Leopard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recursive Abstractive Processing for Retrieval in Dynamic <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charbel Chucri, Rami Azouz, Joachim Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent retrieval-augmented models enhance basic methods by building a
hierarchical structure over retrieved text chunks through recursive embedding,
clustering, and summarization. The most relevant information is then retrieved
from both the original text and generated summaries. However, such approaches
face limitations with dynamic datasets, where adding or removing documents over
time complicates the updating of hierarchical representations formed through
clustering. We propose a new algorithm to efficiently maintain the
recursive-abstractive tree structure in dynamic datasets, without compromising
performance. Additionally, we introduce a novel post-retrieval method that
applies query-focused recursive abstractive processing to substantially improve
context quality. Our method overcomes the limitations of other approaches by
functioning as a black-box post-retrieval layer compatible with any retrieval
algorithm. Both algorithms are validated through extensive experiments on
real-world datasets, demonstrating their effectiveness in handling dynamic data
and improving retrieval performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASeR: Learning to Adaptively Select Reward Models with Multi-Armed
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Models (RMs) play a crucial role in aligning LLMs with human
preferences, enhancing their performance by ranking outputs during inference or
iterative training. However, the degree to which an RM generalizes to new tasks
is often not known a priori (e.g. some RMs may excel at scoring creative
writing vs. math reasoning). Therefore, using only one fixed RM while training
LLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs
simultaneously can be prohibitively computationally-intensive and challenging
due to conflicting signals from different RMs, potentially degrading
performance. To address these challenges, we introduce LASeR (Learning to
Adaptively Select Rewards), which iteratively trains LLMs using multiple RMs,
selecting and utilizing the most well-suited RM for each instance to rank
outputs and generate preference data, framed as a multi-armed bandit problem.
Our results on commonsense and math reasoning tasks demonstrate that LASeR can
boost iterative LLM optimization by optimizing for multiple RMs, improving the
absolute average accuracy of Llama-3-8B over three datasets by 2.67% over
training with ensemble RM scores while also showing superior training
efficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of
instruction-following prompts, we find that using Llama-3-8B LASeR leads to a
71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending
to long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an
average improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA
over random RM selection when used with best-of-n sampling. LASeR is robust to
noisy rewards and generalizes to multiple settings. Finally, LASeR's RM
selection changes depending on the underlying task or instance and we verify
the presence of conflicting preferences from multiple RMs that can be mitigated
using LASeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; First two authors contributed equally. Code:
  https://github.com/duykhuongnguyen/LASeR-MAB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Perception in Text Strings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Jia, Xiang Yue, Shanshan Huang, Ziheng Qin, Yizhu Liu, Bill Yuchen Lin, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding visual semantics embedded in consecutive characters is a
crucial capability for both large language models (LLMs) and multi-modal large
language models (MLLMs). This type of artifact possesses the unique
characteristic that identical information can be readily formulated in both
texts and images, making them a significant proxy for analyzing modern LLMs'
and MLLMs' capabilities in modality-agnostic vision understanding. In this
work, we select ASCII art as a representative artifact, where the lines and
brightness used to depict each concept are rendered by characters, and we frame
the problem as an ASCII art recognition task. We benchmark model performance on
this task by constructing an evaluation dataset with an elaborate
categorization tree and also collect a training set to elicit the models'
visual perception ability. Through a comprehensive analysis of dozens of
models, results reveal that although humans can achieve nearly 100% accuracy,
the state-of-the-art LLMs and MLLMs lag far behind. Models are capable of
recognizing concepts depicted in the ASCII arts given only text inputs
indicated by over 60% accuracy for some concepts, but most of them achieves
merely around 30% accuracy when averaged across all categories. When provided
with images as inputs, GPT-4o gets 82.68%, outperforming the strongest
open-source MLLM by 21.95%. Although models favor different kinds of ASCII art
depending on the modality provided, none of the MLLMs successfully benefit when
both modalities are supplied simultaneously. Moreover, supervised fine-tuning
helps improve models' accuracy especially when provided with the image
modality, but also highlights the need for better training techniques to
enhance the information fusion among modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComfyGen: <span class="highlight-title">Prompt</span>-Adaptive Workflows for Text-to-Image Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rinon Gal, Adi Haviv, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The practical use of text-to-image generation has evolved from simple,
monolithic models to complex workflows that combine multiple specialized
components. While workflow-based approaches can lead to improved image quality,
crafting effective workflows requires significant expertise, owing to the large
number of available components, their complex inter-dependence, and their
dependence on the generation prompt. Here, we introduce the novel task of
prompt-adaptive workflow generation, where the goal is to automatically tailor
a workflow to each user prompt. We propose two LLM-based approaches to tackle
this task: a tuning-based method that learns from user-preference data, and a
training-free method that uses the LLM to select existing flows. Both
approaches lead to improved image quality when compared to monolithic models or
generic, prompt-independent workflows. Our work shows that prompt-dependent
flow prediction offers a new pathway to improving text-to-image generation
quality, complementing existing research directions in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://comfygen-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Reward Models for Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are key in reinforcement learning from human feedback (RLHF)
systems, aligning the model behavior with human preferences. Particularly in
the math domain, there have been plenty of studies using reward models to align
policies for improving reasoning capabilities. Recently, as the importance of
reward models has been emphasized, RewardBench is proposed to understand their
behavior. However, we figure out that the math subset of RewardBench has
different representations between chosen and rejected completions, and relies
on a single comparison, which may lead to unreliable results as it only see an
isolated case. Therefore, it fails to accurately present the robustness of
reward models, leading to a misunderstanding of its performance and potentially
resulting in reward hacking. In this work, we introduce a new design for
reliable evaluation of reward models, and to validate this, we construct
RewardMATH, a benchmark that effectively represents the robustness of reward
models in mathematical reasoning tasks. We demonstrate that the scores on
RewardMATH strongly correlate with the results of optimized policy and
effectively estimate reward overoptimization, whereas the existing benchmark
shows almost no correlation. The results underscore the potential of our design
to enhance the reliability of evaluation, and represent the robustness of
reward model. We make our code and data publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Knowledge Concept Annotation and Question Representation
  Learning for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) is a popular approach for modeling students' learning
progress over time, which can enable more personalized and adaptive learning.
However, existing KT approaches face two major limitations: (1) they rely
heavily on expert-defined knowledge concepts (KCs) in questions, which is
time-consuming and prone to errors; and (2) KT methods tend to overlook the
semantics of both questions and the given KCs. In this work, we address these
challenges and present KCQRL, a framework for automated knowledge concept
annotation and question representation learning that can improve the
effectiveness of any existing KT model. First, we propose an automated KC
annotation process using large language models (LLMs), which generates question
solutions and then annotates KCs in each solution step of the questions.
Second, we introduce a contrastive learning approach to generate semantically
rich embeddings for questions and solution steps, aligning them with their
associated KCs via a tailored false negative elimination approach. These
embeddings can be readily integrated into existing KT models, replacing their
randomly initialized embeddings. We demonstrate the effectiveness of KCQRL
across 15 KT algorithms on two large real-world Math learning datasets, where
we achieve consistent performance improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-Demo <span class="highlight-title">Prompt</span>ing: Leveraging Ge<span class="highlight-title">ner</span>ated Outputs as Demonstrations for
  Enhanced Batch <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyu Feng, Mengze Hong, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch prompting is a common technique in large language models (LLMs) used to
process multiple inputs simultaneously, aiming to improve computational
efficiency. However, as batch sizes increase, performance degradation often
occurs due to the model's difficulty in handling lengthy context inputs.
Existing methods that attempt to mitigate these issues rely solely on batch
data arrangement and majority voting rather than improving the design of the
batch prompt itself. In this paper, we address these limitations by proposing
"Auto-Demo Prompting," a novel approach that leverages the question-output
pairs from earlier questions within a batch as demonstrations for subsequent
answer inference. We provide a formal theoretical analysis of how Auto-Demo
Prompting functions within the autoregressive generation process of LLMs,
illustrating how it utilizes prior outputs to optimize the model's internal
representations. Our method effectively bridges the gap between batch prompting
and few-shot prompting, enhancing performance with only a slight compromise in
token usage. Experimental results across five NLP tasks demonstrate its
effectiveness in mitigating performance degradation and occasionally
outperforming single prompts. Furthermore, it opens new avenues for applying
few-shot learning techniques, such as demonstration selection, within batch
prompting, making it a robust solution for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Theoretical Understanding of Synthetic Data in LLM
  Post-Training: A Reverse-Bottleneck Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Gan, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has become a pivotal resource in post-training tasks for large
language models (LLMs) due to the scarcity of high-quality, specific data.
While various methods have been developed to generate synthetic data, there
remains a discernible gap between the practical effects of synthetic data and
our theoretical comprehension. To address this challenge, we commence by
presenting a detailed modeling of the prevalent synthetic data generation
process. Building upon this modeling, we demonstrate that the generalization
capability of the post-trained model is critically determined by the
information gain derived from the generative model, as analyzed from a novel
reverse-bottleneck perspective. Moreover, we introduce the concept of
Generalization Gain via Mutual Information (GGMI) and elucidate the
relationship between generalization gain and information gain. This analysis
serves as a theoretical foundation for synthetic data generation and further
highlights its connection with the generalization capability of post-trained
models, offering an understanding about the design of synthetic data generation
techniques and the optimization of the post-training process. We open source
our code through an anonymous GitHub repository at
https://anonymous.4open.science/r/Understanding-Synthetic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Role of Relationship Alignment in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut, Yi-Chia Wang, Jane Dwivedi-Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development and deployment of Generative AI in social settings
raise important questions about how to optimally personalize them for users
while maintaining accuracy and realism. Based on a Facebook public post-comment
dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the
semantic tones across different combinations of a commenter's and poster's
gender, age, and friendship closeness and to replicate these differences in
LLM-generated comments.
  The study consists of two parts: Part I assesses differences in semantic
tones across social relationship categories, and Part II examines the
similarity between comments generated by Llama 3.0 (70B) and human comments
from Part I given public Facebook posts as input. Part I results show that
including social relationship information improves the ability of a model to
predict the semantic tone of human comments. However, Part II results show that
even without including social context information in the prompt, LLM-generated
comments and human comments are equally sensitive to social context, suggesting
that LLMs can comprehend semantics from the original post alone. When we
include all social relationship information in the prompt, the similarity
between human comments and LLM-generated comments decreases. This inconsistency
may occur because LLMs did not include social context information as part of
their training data. Together these results demonstrate the ability of LLMs to
comprehend semantics from the original post and respond similarly to human
comments, but also highlights their limitations in generalizing personalized
comments through prompting alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Contrastive Monte Carlo Tree Search Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning
algorithm for Large Language Models (LLMs), significantly improves both
reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM
reasoning works often overlooked its biggest drawback--slower speed compared to
CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on
various tasks with limited quantitative analysis or ablation studies of its
components from reasoning interpretability perspective. 3. The reward model is
the most crucial component in MCTS, however previous work has rarely conducted
in-depth study or improvement of MCTS's reward models. Thus, we conducted
extensive ablation studies and quantitative analysis on components of MCTS,
revealing the impact of each component on the MCTS reasoning performance of
LLMs. Building on this, (i) we designed a highly interpretable reward model
based on the principle of contrastive decoding and (ii) achieved an average
speed improvement of 51.9% per node using speculative decoding. Additionally,
(iii) we improved UCT node selection strategy and backpropagation used in
previous works, resulting in significant performance improvement. We
outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step
reasoning dataset using Llama-3.1-70B with SC-MCTS*.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exploration of Self-Supervised Mutual Information Alignment for
  Multi-Task Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Govande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for pluralistic alignment methods that can steer
language models towards individual attributes and preferences. One such method,
Self-Supervised Alignment with Mutual Information (SAMI), uses conditional
mutual information to encourage the connection between behavioral preferences
and model responses. We conduct two experiments exploring SAMI in multi-task
settings. First, we compare SAMI to Direct Preference Optimization (DPO) on a
multi-task benchmark (MT-Bench), using a stronger model to generate training
data for a weaker one across diverse categories (humanities, STEM, extraction,
coding, math, reasoning, and roleplay). Our results indicate that one iteration
of SAMI has a 57% win rate against DPO, with significant variation in
performance between task categories. Second, we examine SAMI's impact on
mathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While
SAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%
boost. However, SAMI shows interesting scaling trends. When given 10 attempts,
SAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining
SAMI with SFT yields an additional improvement of 1.3% in multi-attempt
settings, though single-attempt accuracy remains unchanged.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving
  Long-Range Reasoning Problems using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangsheng Wang, Xiao Zhang, Hao Liu, Songde Han, Huimin Ma, Tianyu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated limitations in handling
combinatorial optimization problems involving long-range reasoning, partially
due to causal hallucinations and huge search space. As for causal
hallucinations, i.e., the inconsistency between reasoning and corresponding
state transition, this paper introduces the Causal Relationship Enhancement
(CRE) mechanism combining cause-effect interventions and the Individual
Treatment Effect (ITE) to guarantee the solid causal rightness between each
step of reasoning and state transition. As for the long causal range and huge
search space limiting the performances of existing models featuring
single-direction search, a Dual-End Searching (DES) approach is proposed to
seek solutions by simultaneously starting from both the initial and goal states
on the causal probability tree. By integrating CRE and DES (CreDes), our model
has realized simultaneous multi-step reasoning, circumventing the
inefficiencies from cascading multiple one-step reasoning like the
Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly
outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning
tasks in terms of both accuracy and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-shaped and Inverted-U Scaling behind Emergent Abilities of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung-Yu Wu, Pei-Yu Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to exhibit emergent abilities in
some downstream tasks, where performance seems to stagnate at first and then
improve sharply and unpredictably with scale beyond a threshold. By dividing
questions in the datasets according to difficulty level by average performance,
we observe U-shaped scaling for hard questions, and inverted-U scaling followed
by steady improvement for easy questions. Moreover, the emergence threshold
roughly coincides with the point at which performance on easy questions reverts
from inverse scaling to standard scaling. Capitalizing on the observable though
opposing scaling trend on easy and hard questions, we propose a simple yet
effective pipeline, called Slice-and-Sandwich, to predict both the emergence
threshold and model performance beyond the threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactAlign: Long-form Factuality Alignment of <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated significant potential as the
next-generation information access engines. However, their reliability is
hindered by issues of hallucination and generating non-factual content. This is
particularly problematic in long-form responses, where assessing and ensuring
factual accuracy is complex. In this paper, we address this gap by proposing
FactAlign, a novel alignment framework designed to enhance the factuality of
LLMs' long-form responses while maintaining their helpfulness. We introduce
fKTO, a fine-grained, sentence-level alignment algorithm that extends the
Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent
advances in automatic factuality evaluation, FactAlign utilizes fine-grained
factuality assessments to guide the alignment process. Our experiments on
open-domain prompts and information-seeking questions demonstrate that
FactAlign significantly improves the factual accuracy of LLM responses while
also improving their helpfulness. Further analyses identify that FactAlign is
capable of training LLMs to provide more information without losing factual
precision, thus improving the factual F1 score. Our source code, datasets, and
trained models are publicly available at https://github.com/MiuLab/FactAlign
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit
  Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly applied to complex reasoning
tasks that require executing several complex steps before receiving any reward.
Properly assigning credit to these steps is essential for enhancing model
performance. Proximal Policy Optimization (PPO), a state-of-the-art
reinforcement learning (RL) algorithm used for LLM finetuning, employs value
networks to tackle credit assignment. However, value networks face challenges
in predicting the expected cumulative rewards accurately in complex reasoning
tasks, often leading to high-variance updates and suboptimal performance. In
this work, we systematically evaluate the efficacy of value networks and reveal
their significant shortcomings in reasoning-heavy LLM tasks, showing that they
barely outperform a random baseline when comparing alternative steps. To
address this, we propose VinePPO, a straightforward approach that leverages the
flexibility of language environments to compute unbiased Monte Carlo-based
estimates, bypassing the need for large value networks. Our method consistently
outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with
fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These
results emphasize the importance of accurate credit assignment in RL finetuning
of LLM and demonstrate VinePPO's potential as a superior alternative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trying to be human: Linguistic traces of stochastic empathy in language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bennett Kleinberg, Jari Zegers, Jonas Festor, Stefana Vida, Julian Präsent, Riccardo Loconte, Sanne Peereboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating between generated and human-written content is important for
navigating the modern world. Large language models (LLMs) are crucial drivers
behind the increased quality of computer-generated content. Reportedly, humans
find it increasingly difficult to identify whether an AI model generated a
piece of text. Our work tests how two important factors contribute to the human
vs AI race: empathy and an incentive to appear human. We address both aspects
in two experiments: human participants and a state-of-the-art LLM wrote
relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),
either instructed to be as human as possible or not. New samples of humans
(n=428 and n=408) then judged the texts' source. Our findings show that when
empathy is required, humans excel. Contrary to expectations, instructions to
appear human were only effective for the LLM, so the human advantage
diminished. Computational text analysis revealed that LLMs become more human
because they may have an implicit representation of what makes a text human and
effortlessly apply these heuristics. The model resorts to a conversational,
self-referential, informal tone with a simpler vocabulary to mimic stochastic
empathy. We discuss these findings in light of recent claims on the on-par
performance of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Context Gaps: Leveraging Coreference Resolution for Long
  Contextual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yanxin Shen, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in natural
language processing; however, they still face difficulties when tasked with
understanding lengthy contexts and executing effective question answering.
These challenges often arise due to the complexity and ambiguity present in
longer texts. To enhance the performance of LLMs in such scenarios, we
introduce the Long Question Coreference Adaptation (LQCA) method. This
innovative framework focuses on coreference resolution tailored to long
contexts, allowing the model to identify and manage references effectively. The
LQCA method encompasses four key steps: resolving coreferences within
sub-documents, computing the distances between mentions, defining a
representative mention for coreference, and answering questions through mention
replacement. By processing information systematically, the framework provides
easier-to-handle partitions for LLMs, promoting better understanding.
Experimental evaluations on a range of LLMs and datasets have yielded positive
results, with a notable improvements on OpenAI-o1-mini and GPT-4o models,
highlighting the effectiveness of leveraging coreference resolution to bridge
context gaps in question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Underreview version of LQCA, Bridge context gap for long context</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Long-range Language Modeling with Self-supervised Causal
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Zhihao Teng, Wei Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, retrieval-based language models (RLMs) have received much
attention. However, most of them leverage a pre-trained retriever with fixed
parameters, which may not adapt well to causal language models. In this work,
we propose Grouped Cross-Attention, a novel module enabling joint pre-training
of the retriever and causal LM, and apply it to long-context modeling. For a
given input sequence, we split it into chunks and use the current chunk to
retrieve past chunks for subsequent text generation. Our innovation allows the
retriever to learn how to retrieve past chunks that better minimize the
auto-regressive loss of subsequent tokens in an end-to-end manner. By
integrating top-$k$ retrieval, our model can be pre-trained efficiently from
scratch with context lengths up to 64K tokens. Our experiments show our model,
compared with long-range LM baselines, can achieve lower perplexity with
comparable or lower pre-training and inference costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeIDClinic: A Multi-Layered Framework for De-identification of Clinical
  Free-text Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Paul, Dhivin Shaji, Lifeng Han, Warren Del-Pinto, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  De-identification is important in protecting patients' privacy for healthcare
text analytics. The MASK framework is one of the best on the de-identification
shared task organised by n2c2/i2b2 challenges. This work enhances the MASK
framework by integrating ClinicalBERT, a deep learning model specifically
fine-tuned on clinical texts, alongside traditional de-identification methods
like dictionary lookup and rule-based approaches. The system effectively
identifies and either redacts or replaces sensitive identifiable entities
within clinical documents, while also allowing users to customise the masked
documents according to their specific needs. The integration of ClinicalBERT
significantly improves the performance of entity recognition, achieving 0.9732
F1-score, especially for common entities such as names, dates, and locations.
  A risk assessment feature has also been developed, which analyses the
uniqueness of context within documents to classify them into risk levels,
guiding further de-identification efforts. While the system demonstrates strong
overall performance, this work highlights areas for future improvement,
including handling more complex entity occurrences and enhancing the system's
adaptability to different clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On The Adaptation of Unlimiformer for Decoder-Only <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kian Ahrabian, Alon Benhaim, Barun Patra, Jay Pujara, Saksham Singhal, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the prominent issues stifling the current generation of large language
models is their limited context length. Recent proprietary models such as GPT-4
and Claude 2 have introduced longer context lengths, 8k/32k and 100k,
respectively; however, despite the efforts in the community, most common
models, such as LLama-2, have a context length of 4k or less. Unlimiformer
(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation
method that offloads cross-attention computations to a kNN index. However, its
main limitation is incompatibility with decoder-only transformers out of the
box. In this work, we explore practical considerations of adapting Unlimiformer
to decoder-only transformers and introduce a series of modifications to
overcome this limitation. Moreover, we expand the original experimental setup
on summarization to include a new task (i.e., free-form Q&A) and an
instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase
the effectiveness of these modifications on summarization, performing on par
with a model with 2x the context length. Moreover, we discuss limitations and
future directions for free-form Q&A and instruction-tuned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Thematic Framework for Analyzing Large-scale Self-reported Social
  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhusudan Basak, Omar Sharif, Sarah E. Lord, Jacob T. Borodovsky, Lisa A. Marsch, Sandra A. Springer, Edward Nunes, Charlie D. Brackett, Luke J. ArchiBald, Sarah M. Preum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: One of the key FDA-approved medications for Opioid Use Disorder
(OUD) is buprenorphine. Despite its popularity, individuals often report
various information needs regarding buprenorphine treatment on social media
platforms like Reddit. However, the key challenge is to characterize these
needs. In this study, we propose a theme-based framework to curate and analyze
large-scale data from social media to characterize self-reported treatment
information needs (TINs).
  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit
sub-community for buprenorphine products. Following the standard protocol, we
first identified and defined five main themes from the data and then coded
6,000 posts based on these themes, where one post can be labeled with
applicable one to three themes. Finally, we determined the most frequently
appearing sub-themes (topics) for each theme by analyzing samples from each
group.
  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two
themes, and 13.9% three themes. The most frequent topics for each theme or
theme combination came with several key findings - prevalent reporting of
psychological and physical effects during recovery, complexities in accessing
buprenorphine, and significant information gaps regarding medication
administration, tapering, and usage of substances during different stages of
recovery. Moreover, self-treatment strategies and peer-driven advice reveal
valuable insights and potential misconceptions.
  Conclusions: The findings obtained using our proposed framework can inform
better patient education and patient-provider communication, design systematic
interventions to address treatment-related misconceptions and rumors, and
streamline the generation of hypotheses for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent Detection in the Age of LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arora, Shreya Jain, Srujana Merugu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection is a critical component of task-oriented dialogue systems
(TODS) which enables the identification of suitable actions to address user
utterances at each dialog turn. Traditional approaches relied on
computationally efficient supervised sentence transformer encoder models, which
require substantial training data and struggle with out-of-scope (OOS)
detection. The emergence of generative large language models (LLMs) with
intrinsic world knowledge presents new opportunities to address these
challenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context
learning and chain-of-thought prompting for intent detection, and compare their
performance with contrastively fine-tuned sentence transformer (SetFit) models
to highlight prediction quality and latency tradeoff. We propose a hybrid
system using uncertainty based routing strategy to combine the two approaches
that along with negative data augmentation results in achieving the best of
both worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To
better understand LLM OOS detection capabilities, we perform controlled
experiments revealing that this capability is significantly influenced by the
scope of intent labels and the size of the label space. We also introduce a
two-step approach utilizing internal LLM representations, demonstrating
empirical gains in OOS detection accuracy and F1-score by >5% for the
Mistral-7B model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Upcycling Instruction Tuning from Dense to Mixture-of-Experts via
  Parameter Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Yu Sun, Hua Wu, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and
demonstrates outstanding performance in plentiful natural language processing
tasks. However, existing methods transforming LLMs from dense to MoE face
significant data requirements and typically rely on large-scale post-training.
In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient
approach for tuning a dense pre-trained model into a MoE instruction model.
Specifically, we first point out that intermediate checkpoints during
instruction tuning of the dense model are naturally suitable for specialized
experts, and then propose an expert expansion stage to flexibly achieve models
with flexible numbers of experts, where genetic algorithm and parameter merging
are introduced to ensure sufficient diversity of new extended experts. To
ensure that each specialized expert in the MoE model works as expected, we
select a small amount of seed data that each expert excels to pre-optimize the
router. Extensive experiments with various data scales and upcycling settings
demonstrate the outstanding performance and data efficiency of UpIT, as well as
stable improvement in expert or data scaling. Further analysis reveals the
importance of ensuring expert diversity in upcycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENTP: Encoder-only Next Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-token prediction models have predominantly relied on decoder-only
Transformers with causal attention, driven by the common belief that causal
attention is essential to prevent "cheating" by masking future tokens. We
challenge this widely accepted notion and argue that this design choice is
about efficiency rather than necessity. While decoder-only Transformers are
still a good choice for practical reasons, they are not the only viable option.
In this work, we introduce Encoder-only Next Token Prediction (ENTP). We
explore the differences between ENTP and decoder-only Transformers in
expressive power and complexity, highlighting potential advantages of ENTP. We
introduce the Triplet-Counting task and show, both theoretically and
experimentally, that while ENTP can perform this task easily, a decoder-only
Transformer cannot. Finally, we empirically demonstrate ENTP's superior
performance across various realistic tasks, such as length generalization and
in-context learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spoken Grammar Assessment Using LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunil Kumar Kopparapu, Chitralekha Bhat, Ashish Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language assessment (SLA) systems restrict themselves to evaluating
the pronunciation and oral fluency of a speaker by analysing the read and
spontaneous spoken utterances respectively. The assessment of language grammar
or vocabulary is relegated to written language assessment (WLA) systems. Most
WLA systems present a set of sentences from a curated finite-size database of
sentences thereby making it possible to anticipate the test questions and train
oneself. In this paper, we propose a novel end-to-end SLA system to assess
language grammar from spoken utterances thus making WLA systems redundant;
additionally, we make the assessment largely unteachable by employing a large
language model (LLM) to bring in variations in the test. We further demonstrate
that a hybrid automatic speech recognition (ASR) with a custom-built language
model outperforms the state-of-the-art ASR engine for spoken grammar
assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source
  Instruction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the
cutting-edge progress in mathematical reasoning with LLMs has become
\emph{closed-source} due to lack of access to training data. This lack of data
access limits researchers from understanding the impact of different choices
for synthesizing and utilizing the data. With the goal of creating a
high-quality finetuning (SFT) dataset for math reasoning, we conduct careful
ablation experiments on data synthesis using the recently released
\texttt{Llama3.1} family of models. Our experiments show that: (a) solution
format matters, with excessively verbose solutions proving detrimental to SFT
performance, (b) data generated by a strong teacher outperforms
\emph{on-policy} data generated by a weak student model, (c) SFT is robust to
low-quality solutions, allowing for imprecise data filtering, and (d) question
diversity is crucial for achieving data scaling gains. Based on these insights,
we create the OpenMathInstruct-2 dataset, which consists of 14M
question-solution pairs ($\approx$ 600K unique questions), making it nearly
eight times larger than the previous largest open-source math reasoning
dataset. Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2
outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\%
$\rightarrow$ 67.8\%). Finally, to accelerate the open-source efforts, we
release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrative Decoding: Improve Factuality via Implicit Self-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency-based approaches, which involve repeatedly sampling multiple
outputs and selecting the most consistent one as the final response, prove to
be remarkably effective in improving the factual accuracy of large language
models. Nonetheless, existing methods usually have strict constraints on the
task format, largely limiting their applicability. In this paper, we present
Integrative Decoding (ID), to unlock the potential of self-consistency in
open-ended generation tasks. ID operates by constructing a set of inputs, each
prepended with a previously sampled response, and then processes them
concurrently, with the next token being selected by aggregating of all their
corresponding predictions at each decoding step. In essence, this simple
approach implicitly incorporates self-consistency in the decoding objective.
Extensive evaluation shows that ID consistently enhances factuality over a wide
range of language models, with substantial improvements on the TruthfulQA
(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance
gains amplify progressively as the number of sampled responses increases,
indicating the potential of ID to scale up with repeated sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACE: A LLM-based Negotiation Coaching System <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W. Morris, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing prominence of LLMs has led to an increase in the development of
AI tutoring systems. These systems are crucial in providing underrepresented
populations with improved access to valuable education. One important area of
education that is unavailable to many learners is strategic bargaining related
to negotiation. To address this, we develop a LLM-based Assistant for Coaching
nEgotiation (ACE). ACE not only serves as a negotiation partner for users but
also provides them with targeted feedback for improvement. To build our system,
we collect a dataset of negotiation transcripts between MBA students. These
transcripts come from trained negotiators and emulate realistic bargaining
scenarios. We use the dataset, along with expert consultations, to design an
annotation scheme for detecting negotiation mistakes. ACE employs this scheme
to identify mistakes and provide targeted feedback to users. To test the
effectiveness of ACE-generated feedback, we conducted a user experiment with
two consecutive trials of negotiation and found that it improves negotiation
performances significantly compared to a system that doesn't provide feedback
and one which uses an alternative method of providing feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedQA-CS: Benchmarking <span class="highlight-title">Large Language Model</span>s Clinical Skills Using an
  AI-SCE Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Zihao Zhang, Chaolong Tang, Xingyu Bian, Youxia Zhao, Zhichao Yang, Junda Wang, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) and large language models (LLMs) in healthcare
require advanced clinical skills (CS), yet current benchmarks fail to evaluate
these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by
medical education's Objective Structured Clinical Examinations (OSCEs), to
address this gap. MedQA-CS evaluates LLMs through two instruction-following
tasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real
clinical scenarios. Our contributions include developing MedQA-CS, a
comprehensive evaluation framework with publicly available data and expert
annotations, and providing the quantitative and qualitative assessment of LLMs
as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a
more challenging benchmark for evaluating clinical skills than traditional
multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,
MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities
for both open- and closed-source LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Transfer Learning: Demonstration Synthesis by Transferring
  Similar Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzirui Wang, Xuangliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is an effective approach to help large language
models (LLMs) adapt to various tasks by providing demonstrations of the target
task. Considering the high cost of labeling demonstrations, many methods
propose synthesizing demonstrations from scratch using LLMs. However, the
quality of the demonstrations synthesized from scratch is limited by the
capabilities and knowledge of LLMs. To address this, inspired by transfer
learning, we propose In-Context Transfer Learning (ICTL), which synthesizes
target task demonstrations by transferring labeled demonstrations from similar
source tasks. ICTL consists of two steps: source sampling and target transfer.
First, we define an optimization objective, which minimizes transfer error to
sample source demonstrations similar to the target task. Then, we employ LLMs
to transfer the sampled source demonstrations to the target task, matching the
definition and format of the target task. Experiments on Super-NI show that
ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the
effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> HarmAug: Effective Data Augmentation for Knowledge Distillation of
  Safety Guard Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, <span class="highlight-author">Yoshua Bengio</span>, Juho Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety guard models that detect malicious queries aimed at large language
models (LLMs) are essential for ensuring the secure and responsible deployment
of LLMs in real-world applications. However, deploying existing safety guard
models with billions of parameters alongside LLMs on mobile devices is
impractical due to substantial memory requirements and latency. To reduce this
cost, we distill a large teacher safety guard model into a smaller one using a
labeled dataset of instruction-response pairs with binary harmfulness labels.
Due to the limited diversity of harmful instructions in the existing labeled
dataset, naively distilled models tend to underperform compared to larger
models. To bridge the gap between small and large models, we propose HarmAug, a
simple yet effective data augmentation method that involves jailbreaking an LLM
and prompting it to generate harmful instructions. Given a prompt such as,
"Make a single harmful instruction prompt that would elicit offensive content",
we add an affirmative prefix (e.g., "I have an idea for a prompt:") to the
LLM's response. This encourages the LLM to continue generating the rest of the
response, leading to sampling harmful instructions. Another LLM generates a
response to the harmful instruction, and the teacher model labels the
instruction-response pair. We empirically show that our HarmAug outperforms
other relevant baselines. Moreover, a 435-million-parameter safety guard model
trained with HarmAug achieves an F1 score comparable to larger models with over
7 billion parameters, and even outperforms them in AUPRC, while operating at
less than 25% of their computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniPot: Infinite Context Processing on Memory-Constrained LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling long input contexts remains a significant challenge for Large
Language Models (LLMs), particularly in resource-constrained environments such
as mobile devices. Our work aims to address this limitation by introducing
InfiniPot, a novel KV cache control framework designed to enable pre-trained
LLMs to manage extensive sequences within fixed memory constraints efficiently,
without requiring additional training. InfiniPot leverages Continual Context
Distillation (CCD), an iterative process that compresses and retains essential
information through novel importance metrics, effectively maintaining critical
data even without access to future context. Our comprehensive evaluations
indicate that InfiniPot significantly outperforms models trained for long
contexts in various NLP tasks, establishing its efficacy and versatility. This
work represents a substantial advancement toward making LLMs applicable to a
broader range of real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstaTrans: An Instruction-Aware Translation Framework for Non-English
  Instruction <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging to generate high-quality instruction datasets for
non-English languages due to tail phenomena, which limit performance on less
frequently observed data. To mitigate this issue, we propose translating
existing high-quality English instruction datasets as a solution, emphasizing
the need for complete and instruction-aware translations to maintain the
inherent attributes of these datasets. We claim that fine-tuning LLMs with
datasets translated in this way can improve their performance in the target
language. To this end, we introduces a new translation framework tailored for
instruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through
extensive experiments, we demonstrate the superiority of InstaTrans over other
competitors in terms of completeness and instruction-awareness of translation,
highlighting its potential to broaden the accessibility of LLMs across diverse
languages at a relatively low cost. Furthermore, we have validated that
fine-tuning LLMs with datasets translated by InstaTrans can effectively improve
their performance in the target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Latent Shifts of In-Context Learning Through Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josip Jukić, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has become essential in natural language
processing, particularly with autoregressive large language models capable of
learning from demonstrations provided within the prompt. However, ICL faces
challenges with stability and long contexts, especially as the number of
demonstrations grows, leading to poor generalization and inefficient inference.
To address these issues, we introduce STICL (Self-Training ICL), an approach
that disentangles the latent shifts of demonstrations from the latent shift of
the query through self-training. STICL employs a teacher model to generate
pseudo-labels and trains a student model using these labels, encoded in an
adapter module. The student model exhibits weak-to-strong generalization,
progressively refining its predictions over time. Our empirical results show
that STICL improves generalization and stability, consistently outperforming
traditional ICL methods and other disentangling strategies across both
in-domain and out-of-domain data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersonaMath: Enhancing Math Reasoning through Persona-Driven Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Luo, Run Luo, Longze Chen, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Chengming Li, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While closed-source Large Language Models (LLMs) demonstrate strong
mathematical problem-solving abilities, open-source models continue to struggle
with such tasks. To bridge this gap, we propose a data augmentation approach
and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we
train the PersonaMath models. Our approach consists of two stages: the first
stage is learning from Persona Diversification, and the second stage is
learning from Reflection. In the first stage, we regenerate detailed
chain-of-thought (CoT) solutions as instructions using a closed-source LLM and
introduce a novel persona-driven data augmentation technique to enhance the
dataset's quantity and diversity. In the second stage, we incorporate
reflection to fully leverage more challenging and valuable questions.
Evaluation of our PersonaMath models on MATH and GSM8K reveals that the
PersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on
MATH and 68.7% on GSM8K, surpassing all baseline methods and achieving
state-of-the-art performance. Notably, our dataset contains only 70.3K data
points-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model
outperforms these baselines, demonstrating the high quality and diversity of
our dataset, which enables more efficient model training. We open-source the
PersonaMathQA dataset, PersonaMath models, and our code for public usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,
  Lightweight Plugin for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Ruizhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have achieved robust
performance across diverse tasks, but fine-tuning these models for specific
domains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)
methods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a
small subset of parameters. However, existing methods for fusing multiple LoRAs
lack dynamic fusion based on contextual inputs and often increase inference
time due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight
Plugin that employs a mini-MLP module with only 5M parameters to dynamically
fuse multiple LoRAs at the sentence level using top-p sampling strategies. This
approach reduces inference time to less than twice that of single LoRA
inference by leveraging parallel computation. Evaluations across 26
tasks-including multiple-choice questions and question answering-demonstrate
that DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice
datasets and significant improvements in BLEU and ROUGE scores on QA datasets,
outperforming different LLMs backbones under composite task settings. DLP-LoRA
effectively balances performance and efficiency, making it a practical solution
for dynamic multi-task adaptation in LLMs. Our code is available at
https://github.com/MeCuping/DLP-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review, 18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Context Window of <span class="highlight-title">Large Language Model</span>s from a Distributional
  Perspective <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingsheng Wu. Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the rotary position embedding (RoPE) has become a common method for
extending the context window of RoPE-based large language models (LLMs).
However, existing scaling methods often rely on empirical approaches and lack a
profound understanding of the internal distribution within RoPE, resulting in
suboptimal performance in extending the context window length. In this paper,
we propose to optimize the context window extending task from the view of
rotary angle distribution. Specifically, we first estimate the distribution of
the rotary angles within the model and analyze the extent to which length
extension perturbs this distribution. Then, we present a novel extension
strategy that minimizes the disturbance between rotary angle distributions to
maintain consistency with the pre-training phase, enhancing the model's
capability to generalize to longer sequences. Experimental results compared to
the strong baseline methods demonstrate that our approach reduces by up to 72%
of the distributional disturbance when extending LLaMA2's context window to 8k,
and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,
our method achieves an average improvement of up to 4.33% over existing
state-of-the-art methods. Furthermore, Our method maintains the model's
performance on the Hugging Face Open LLM benchmark after context window
extension, with only an average performance fluctuation ranging from -0.12 to
+0.22.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, Accepted to EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Language Models Like Small Vocabularies: Probing the Linguistic
  Abilities of Grapheme- and Phoneme-Based Baby Llamas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Bunzeck, Daniel Duran, Leonie Schade, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models use subword-based tokenization algorithms like Byte
Pair Encoding, which put their validity as models of linguistic representations
into question. In this paper, we explore the potential of tokenization-free,
phoneme- and grapheme-based language models. We demonstrate that small models
based on the Llama architecture can achieve strong linguistic performance on
standard syntactic and novel lexical/phonetic benchmarks when trained with
character-level vocabularies. We further show that phoneme-based models without
any graphemic biases almost match grapheme-based models in standard tasks and
novel evaluations. Our findings suggest a promising direction for creating more
linguistically plausible language models that are better suited for
computational studies of language acquisition and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Little Goes a Long Way: Efficient Long Context Training and Inference
  with Partial Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and serving long-context large language models (LLMs) incurs
substantial overhead. To address this, two critical steps are often required: a
pretrained LLM typically undergoes a separate stage for context length
extension by training on long-context data, followed by architectural
modifications to reduce the overhead of KV cache during serving. This paper
argues that integrating length extension with a GPU-friendly KV cache reduction
architecture not only reduces training overhead during length extension, but
also achieves better long-context performance. This leads to our proposed
LongGen, which finetunes a pretrained LLM into an efficient architecture during
length extension. LongGen builds on three key insights: (1) Sparse attention
patterns, such as window attention (attending to recent tokens), attention sink
(initial ones), and blockwise sparse attention (strided token blocks) are
well-suited for building efficient long-context models, primarily due to their
GPU-friendly memory access patterns, enabling efficiency gains not just
theoretically but in practice as well. (2) It is essential for the model to
have direct access to all tokens. A hybrid architecture with 1/3 full attention
layers and 2/3 efficient ones achieves a balanced trade-off between efficiency
and long-context performance. (3) Lightweight training on 5B long-context data
is sufficient to extend the hybrid model's context length from 4K to 128K.
  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its
effectiveness across different scales. During training with 128K-long contexts,
LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,
compared to a full-attention baseline. During inference, LongGen reduces KV
cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding
speedup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-Driven <span class="highlight-title">Large Language Model</span>s for Mandarin Lyric Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Hsiang Liu, Yi-Wen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models have shown impressive in-context learning
abilities, performing well across various tasks with just a prompt. Previous
melody-to-lyric research has been limited by scarce high-quality aligned data
and unclear standard for creativeness. Most efforts focused on general themes
or emotions, which are less valuable given current language model capabilities.
In tonal contour languages like Mandarin, pitch contours are influenced by both
melody and tone, leading to variations in lyric-melody fit. Our study,
validated by the Mpop600 dataset, confirms that lyricists and melody writers
consider this fit during their composition process. In this research, we
developed a multi-agent system that decomposes the melody-to-lyric task into
sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody
alignment, and consistency. Listening tests were conducted via a
diffusion-based singing voice synthesizer to evaluate the quality of lyrics
generated by different agent groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, figures, Accepted at O-COCOSDA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic
  Music: A Focus on Musical Phrase Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language
Processing to build a vocabulary of subwords, which has been recently applied
to symbolic music. Given that symbolic music can differ significantly from
text, particularly with polyphony, we investigate how BPE behaves with
different types of musical content. This study provides a qualitative analysis
of BPE's behavior across various instrumentations and evaluates its impact on a
musical phrase segmentation task for both monophonic and polyphonic music. Our
findings show that the BPE training process is highly dependent on the
instrumentation and that BPE "supertokens" succeed in capturing abstract
musical content. In a musical phrase segmentation task, BPE notably improves
performance in a polyphonic setting, but enhances performance in monophonic
tunes only within a specific range of BPE merges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,
  co-located with ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Geometric Signatures of Compositionality Across a Language Model's
  Lifetime <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, <span class="highlight-author">Yoshua Bengio</span>, Emily Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositionality, the notion that the meaning of an expression is constructed
from the meaning of its parts and syntactic rules, permits the infinite
productivity of human language. For the first time, artificial language models
(LMs) are able to match human performance in a number of compositional
generalization tasks. However, much remains to be understood about the
representational mechanisms underlying these abilities. We take a high-level
geometric approach to this problem by relating the degree of compositionality
in a dataset to the intrinsic dimensionality of its representations under an
LM, a measure of feature complexity. We find not only that the degree of
dataset compositionality is reflected in representations' intrinsic
dimensionality, but that the relationship between compositionality and
geometric complexity arises due to learned linguistic features over training.
Finally, our analyses reveal a striking contrast between linear and nonlinear
dimensionality, showing that they respectively encode formal and semantic
aspects of linguistic composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Compositions: Exploring Modular Structures in <span class="highlight-title">Transformer</span>-Based
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Mondorf, Sondre Wold, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in interpretability research is to what extent neural
networks, particularly language models, implement reusable functions via
subnetworks that can be composed to perform more complex tasks. Recent
developments in mechanistic interpretability have made progress in identifying
subnetworks, often referred to as circuits, which represent the minimal
computational subgraph responsible for a model's behavior on specific tasks.
However, most studies focus on identifying circuits for individual tasks
without investigating how functionally similar circuits relate to each other.
To address this gap, we examine the modularity of neural networks by analyzing
circuits for highly compositional subtasks within a transformer-based language
model. Specifically, given a probabilistic context-free grammar, we identify
and compare circuits responsible for ten modular string-edit operations. Our
results indicate that functionally similar circuits exhibit both notable node
overlap and cross-task faithfulness. Moreover, we demonstrate that the circuits
identified can be reused and combined through subnetwork set operations to
represent more complex functional capabilities of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with
  Retrieval-Augmentation for Solving Challenging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art large language models (LLMs) exhibit impressive
problem-solving capabilities but may struggle with complex reasoning and
factual correctness. Existing methods harness the strengths of chain-of-thought
and retrieval-augmented generation (RAG) to decompose a complex problem into
simpler steps and apply retrieval to improve factual correctness. These methods
work well on straightforward reasoning tasks but often falter on challenging
tasks such as competitive programming and mathematics, due to frequent
reasoning errors and irrelevant knowledge retrieval. To address this, we
introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a
novel framework that leverages fine-tuned critic models to guide both reasoning
and retrieval processes through planning. CR-Planner solves a problem by
iteratively selecting and executing sub-goals. Initially, it identifies the
most promising sub-goal from reasoning, query generation, and retrieval, guided
by rewards given by a critic model named sub-goal critic. It then executes this
sub-goal through sampling and selecting the optimal output based on evaluations
from another critic model named execution critic. This iterative process,
informed by retrieved information and critic models, enables CR-Planner to
effectively navigate the solution space towards the final answer. We employ
Monte Carlo Tree Search to collect the data for training the critic models,
allowing for a systematic exploration of action sequences and their long-term
impacts. We validate CR-Planner on challenging domain-knowledge-intensive and
reasoning-heavy tasks, including competitive programming, theorem-driven math
reasoning, and complex domain retrieval problems. Our experiments demonstrate
that CR-Planner significantly outperforms baselines, highlighting its
effectiveness in addressing challenging problems by improving both reasoning
and retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Labyrinth of Links: Navigating the Associative Maze of Multi-modal
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Question-guided Knowledge Graph Re-scoring and Injection for Knowledge
  Graph Question Answering <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph question answering (KGQA) involves answering natural language
questions by leveraging structured information stored in a knowledge graph.
Typically, KGQA initially retrieve a targeted subgraph from a large-scale
knowledge graph, which serves as the basis for reasoning models to address
queries. However, the retrieved subgraph inevitably brings distraction
information for knowledge utilization, impeding the model's ability to perform
accurate reasoning. To address this issue, we propose a Question-guided
Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the
input question, thereby focusing specifically on pertinent factual knowledge.
Moreover, we introduce Knowformer, a parameter-efficient method for injecting
the re-scored knowledge graph into large language models to enhance their
ability to perform factual reasoning. Extensive experiments on multiple KGQA
benchmarks demonstrate the superiority of our method over existing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>findings of EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrowdCounter: A benchmark type-specific multi-target counterspeech
  <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Punyajoy Saha, Abhilash Datta, Abhik Jana, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterspeech presents a viable alternative to banning or suspending users
for hate speech while upholding freedom of expression. However, writing
effective counterspeech is challenging for moderators/users. Hence, developing
suggestion tools for writing counterspeech is the need of the hour. One
critical challenge in developing such a tool is the lack of quality and
diversity of the responses in the existing datasets. Hence, we introduce a new
dataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs
spanning six different counterspeech types (empathy, humor, questioning,
warning, shaming, contradiction), which is the first of its kind. The design of
our annotation platform itself encourages annotators to write type-specific,
non-redundant and high-quality counterspeech. We evaluate two frameworks for
generating counterspeech responses - vanilla and type-controlled prompts -
across four large language models. In terms of metrics, we evaluate the
responses using relevance, diversity and quality. We observe that Flan-T5 is
the best model in the vanilla framework across different models. Type-specific
prompts enhance the relevance of the responses, although they might reduce the
language quality. DialoGPT proves to be the best at following the instructions
and generating the type-specific counterspeech accurately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 14 tables, Code available
  https://github.com/hate-alert/CrowdCounter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PairDistill: Pairwise Relevance Distillation for Dense Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective information retrieval (IR) from vast datasets relies on advanced
techniques to extract relevant information in response to queries. Recent
advancements in dense retrieval have showcased remarkable efficacy compared to
traditional sparse retrieval methods. To further enhance retrieval performance,
knowledge distillation techniques, often leveraging robust cross-encoder
rerankers, have been extensively explored. However, existing approaches
primarily distill knowledge from pointwise rerankers, which assign absolute
relevance scores to documents, thus facing challenges related to inconsistent
comparisons. This paper introduces Pairwise Relevance Distillation
(PairDistill) to leverage pairwise reranking, offering fine-grained
distinctions between similarly relevant documents to enrich the training of
dense retrieval models. Our experiments demonstrate that PairDistill
outperforms existing methods, achieving new state-of-the-art results across
multiple benchmarks. This highlights the potential of PairDistill in advancing
dense retrieval techniques effectively. Our source code and trained models are
released at https://github.com/MiuLab/PairDistill
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Entropy Decay during Language Model <span class="highlight-title">Pretrain</span>ing Hinders New
  Knowledge Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate how a model's tendency to broadly integrate its
parametric knowledge evolves throughout pretraining, and how this behavior
affects overall performance, particularly in terms of knowledge acquisition and
forgetting. We introduce the concept of knowledge entropy, which quantifies the
range of memory sources the model engages with; high knowledge entropy
indicates that the model utilizes a wide range of memory sources, while low
knowledge entropy suggests reliance on specific sources with greater certainty.
Our analysis reveals a consistent decline in knowledge entropy as pretraining
advances. We also find that the decline is closely associated with a reduction
in the model's ability to acquire and retain knowledge, leading us to conclude
that diminishing knowledge entropy (smaller number of active memory sources)
impairs the model's knowledge acquisition and retention capabilities. We find
further support for this by demonstrating that increasing the activity of
inactive memory sources enhances the model's capacity for knowledge acquisition
and retention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCQPR: Proactive Conversational Question Planning with Reflection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shasha Guo, Lizi Liao, Jing Zhang, Cuiping Li, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Question Generation (CQG) enhances the interactivity of
conversational question-answering systems in fields such as education, customer
service, and entertainment. However, traditional CQG, focusing primarily on the
immediate context, lacks the conversational foresight necessary to guide
conversations toward specified conclusions. This limitation significantly
restricts their ability to achieve conclusion-oriented conversational outcomes.
In this work, we redefine the CQG task as Conclusion-driven Conversational
Question Generation (CCQG) by focusing on proactivity, not merely reacting to
the unfolding conversation but actively steering it towards a
conclusion-oriented question-answer pair. To address this, we propose a novel
approach, called Proactive Conversational Question Planning with self-Refining
(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte
Carlo Tree Search (MCTS) with the analytical capabilities of large language
models (LLMs), PCQPR predicts future conversation turns and continuously
refines its questioning strategies. This iterative self-refining mechanism
ensures the generation of contextually relevant questions strategically devised
to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR
significantly surpasses existing CQG methods, marking a paradigm shift towards
conclusion-oriented conversational question-answering systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assisted Data Annotation for Business Process Information Extraction
  from Textual Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Neuberger, Han van der Aa, Lars Ackermann, Daniel Buschek, Jannic Herrmann, Stefan Jablonski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning based generation of process models from natural language
text process descriptions provides a solution for the time-intensive and
expensive process discovery phase. Many organizations have to carry out this
phase, before they can utilize business process management and its benefits.
Yet, research towards this is severely restrained by an apparent lack of large
and high-quality datasets. This lack of data can be attributed to, among other
things, an absence of proper tool assistance for dataset creation, resulting in
high workloads and inferior data quality. We explore two assistance features to
support dataset creation, a recommendation system for identifying process
information in the text and visualization of the current state of already
identified process information as a graphical business process model. A
controlled user study with 31 participants shows that assisting dataset
creators with recommendations lowers all aspects of workload, up to $-51.0\%$,
and significantly improves annotation quality, up to $+38.9\%$. We make all
data and code available to encourage further research on additional novel
assistance strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging, such as model souping, is the practice of combining different
models with the same architecture together without further training. In this
work, we present a model merging methodology that addresses the difficulty of
fine-tuning Large Language Models (LLMs) for target tasks in non-English
languages, where task-specific data is often unavailable. We focus on
mathematical reasoning and without in-language math data, facilitate
cross-lingual transfer by composing language and math capabilities. Starting
from the same pretrained model, we fine-tune separate "experts" on math
instruction data in English and on generic instruction data in the target
language. We then replace the top and bottom transformer layers of the math
expert directly with layers from the language expert, which consequently
enhances math performance in the target language. The resulting merged models
outperform the individual experts and other merging methods on the math
benchmark, MGSM, by 10% across four major languages where math instruction data
is scarce. In addition, this layer swapping is simple, inexpensive, and
intuitive, as it is based on an interpretative analysis of the most important
parameter changes during the fine-tuning of each expert. The ability to
successfully re-compose LLMs for cross-lingual transfer in this manner opens up
future possibilities to combine model expertise, create modular solutions, and
transfer reasoning capabilities across languages all post hoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 main pages, 23 pages total, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Language Skills under Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exploration of language skills in language models (LMs) has always been
one of the central goals in mechanistic interpretability. However, existing
circuit analyses often fall short in representing the full functional scope of
these models, primarily due to the exclusion of Feed-Forward layers.
Additionally, isolating the effect of a single language skill from a text,
which inherently involves multiple entangled skills, poses a significant
challenge. To address these gaps, we introduce a novel concept, Memory Circuit,
a minimum unit that fully and independently manipulates the memory-reading
functionality of a language model, and disentangle the transformer model
precisely into a circuit graph which is an ensemble of paths connecting
different memory circuits. Based on this disentanglement, we identify salient
circuit paths, named as skill paths, responsible for three crucial language
skills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning
(ICL) Skill, leveraging causal effect estimation through interventions and
counterfactuals. Our experiments on various datasets confirm the correspondence
between our identified skill paths and language skills, and validate three
longstanding hypotheses: 1) Language skills are identifiable through circuit
dissection; 2) Simple language skills reside in shallow layers, whereas complex
language skills are found in deeper layers; 3) Complex language skills are
formed on top of simpler language skills. Our codes are available at:
https://github.com/Zodiark-ch/Language-Skill-of-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Aware Response Ge<span class="highlight-title">ner</span>ation Using Affect-Enriched Embeddings with
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a need for empathetic and coherent responses in automated
chatbot-facilitated psychotherapy sessions. This study addresses the challenge
of enhancing the emotional and contextual understanding of large language
models (LLMs) in psychiatric applications. We introduce a novel framework that
integrates multiple emotion lexicons, including NRC Emotion Lexicon, VADER,
WordNet, and SentiWordNet, with state-of-the-art LLMs such as LLAMA 2, Flan-T5,
ChatGPT 3.0, and ChatGPT 4.0. The primary dataset comprises over 2,000 therapy
session transcripts from the Counseling and Psychotherapy database, covering
discussions on anxiety, depression, trauma, and addiction. We segment the
transcripts into smaller chunks, enhancing them with lexical features and
computing embeddings using BERT, GPT-3, and RoBERTa to capture semantic and
emotional nuances. These embeddings are stored in a FAISS vector database,
enabling efficient similarity search and clustering based on cosine similarity.
Upon user query, the most relevant segments are retrieved and provided as
context to the LLMs, significantly improving the models' ability to generate
empathetic and contextually appropriate responses. Experimental evaluations
demonstrate that in-corporating emotion lexicons enhances empathy, coherence,
informativeness, and fluency scores. Our findings highlight the critical role
of emotional embeddings in improving LLM performance for psychotherapy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Hierarchical Text Classification: Inference and Metrics <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Plaud, Matthieu Labeau, Antoine Saillenfest, Thomas Bonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical text classification (HTC) is the task of assigning labels to a
text within a structured space organized as a hierarchy. Recent works treat HTC
as a conventional multilabel classification problem, therefore evaluating it as
such. We instead propose to evaluate models based on specifically designed
hierarchical metrics and we demonstrate the intricacy of metric choice and
prediction inference method. We introduce a new challenging dataset and we
evaluate fairly, recent sophisticated models, comparing them with a range of
simple but strong baselines, including a new theoretically motivated loss.
Finally, we show that those baselines are very often competitive with the
latest models. This highlights the importance of carefully considering the
evaluation methodology when proposing new methods for HTC. Code implementation
and dataset are available at \url{https://github.com/RomanPlaud/revisitingHTC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoNLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Endless Jailbreaks with Bijection Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian R. Y. Huang, Maximilian Li, Leonard Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive safety training, LLMs are vulnerable to adversarial inputs.
In this work, we introduce a simple but powerful attack paradigm, bijection
learning, that yields a practically endless set of jailbreak prompts. We
exploit language models' advanced reasoning capabilities to teach them
invertible languages (bijections) in context, pass encoded queries to the model
to bypass built-in safety mechanisms, and finally decode responses back into
English, yielding helpful replies to harmful requests. Our approach proves
effective on a wide range of frontier language models and harm categories.
Bijection learning is an automated and universal attack that grows stronger
with scale: larger models with more advanced reasoning capabilities are more
susceptible to bijection learning jailbreaks despite stronger safety
mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Copy Bias in In-Context Learning through Neuron Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameen Ali, Lior Wolf, Ivan Titov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive few-shot in-context
learning (ICL) abilities. Still, we show that they are sometimes prone to a
`copying bias', where they copy answers from provided examples instead of
learning the underlying patterns. In this work, we propose a novel and simple
method to mitigate such copying bias. First, we create a synthetic task and use
the Integrated Gradients method to identify neurons that prioritize copying
over generalization. We demonstrate that pruning these neurons consistently
improves performance across a diverse set of ICL tasks. We also show that our
method is applicable across various LLM architectures, including Transformers
and State-Space Models, without requiring modifications. In our analysis, we
adopt a task-recognition perspective on ICL and examine task vectors (Hendel et
al., 2023) induced by the model. We find that pruning enhances the quality of
these vectors, suggesting that the pruned neurons previously hindered effective
task recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Training Data Attribution for <span class="highlight-title">Large Language Model</span>s with
  Fitting Error Consideration <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of large language models (LLMs) poses challenges in
interpreting results, impacting issues such as data intellectual property
protection and hallucination tracing. Training data attribution (TDA) methods
are considered effective solutions to address these challenges. Most recent TDA
methods rely on influence functions, assuming the model achieves minimized
empirical risk. However, achieving this criterion is difficult, and sourcing
accuracy can be compromised by fitting errors during model training. In this
paper, we introduce a novel TDA method called Debias and Denoise Attribution
(DDA), which enhances influence functions by addressing fitting errors.
Specifically, the debias strategy seeks to improve the performance of influence
functions by eliminating the knowledge bias present in the base model before
fine-tuning, while the denoise strategy aims to reduce discrepancies in
influence scores arising from varying degrees of fitting during the training
process through smoothing techniques. Experimental results demonstrate that our
method significantly outperforms existing approaches, achieving an averaged AUC
of 91.64%. Moreover, DDA exhibits strong generality and scalability across
various sources and different-scale models like LLaMA2, QWEN2, and Mistral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Unveiling AI's Potential Through Tools, Techniques, and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pohsun Feng, Ziqian Bi, Yizhu Wen, Xuanhe Pan, Benji Peng, Ming Liu, Jiawei Xu, Keyu Chen, Junyu Liu, Caitlyn Heqi Yin, Sen Zhang, Jinlang Wang, Qian Niu, Ming Li, Tianyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This book serves as an introduction to deep learning and machine learning,
focusing on their applications in big data analytics. It covers essential
concepts, tools like ChatGPT and Claude, hardware recommendations, and
practical guidance on setting up development environments using libraries like
PyTorch and TensorFlow. Designed for beginners and advanced users alike, it
provides step-by-step instructions, hands-on projects, and insights into AI's
future, including AutoML and edge computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This book contains 156 pages and 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HelpSteer2-Preference: Complementing Ratings with Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Yi Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are critical for aligning models to follow instructions, and
are typically trained following one of two popular paradigms: Bradley-Terry
style or Regression style. However, there is a lack of evidence that either
approach is better than the other, when adequately matched for data. This is
primarily because these approaches require data collected in different (but
incompatible) formats, meaning that adequately matched data is not available in
existing public datasets. To tackle this problem, we release preference
annotations (designed for Bradley-Terry training) to complement existing
ratings (designed for Regression style training) in the HelpSteer2 dataset. To
improve data interpretability, preference annotations are accompanied with
human-written justifications. Using this data, we conduct the first
head-to-head comparison of Bradley-Terry and Regression models when adequately
matched for data. Based on insights derived from such a comparison, we propose
a novel approach to combine Bradley-Terry and Regression reward modeling. A
Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on
RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We
also demonstrate the effectiveness of this reward model at aligning models to
follow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at
https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the
trained Reward Model at
https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended
  Responses <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) tasks have been extensively studied in the field of
natural language processing (NLP). Answers to open-ended questions are highly
diverse and difficult to quantify, and cannot be simply evaluated as correct or
incorrect, unlike close-ended questions with definitive answers. While large
language models (LLMs) have demonstrated strong capabilities across various
tasks, they exhibit relatively weaker performance in evaluating answers to
open-ended questions. In this study, we propose a method that leverages LLMs
and the analytic hierarchy process (AHP) to assess answers to open-ended
questions. We utilized LLMs to generate multiple evaluation criteria for a
question. Subsequently, answers were subjected to pairwise comparisons under
each criterion with LLMs, and scores for each answer were calculated in the
AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and
GPT-4. Our results indicate that our approach more closely aligns with human
judgment compared to the four baselines. Additionally, we explored the impact
of the number of criteria, variations in models, and differences in datasets on
the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGD: Multi-LLM Based Agent Debugger via Refinement and Ge<span class="highlight-title">ner</span>ation
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Jin, Zechao Sun, Yiheng Yang, Huaming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown incredible potential in code
generation tasks, and recent research in prompt engineering have enhanced LLMs'
understanding of textual information. However, ensuring the accuracy of
generated code often requires extensive testing and validation by programmers.
While LLMs can typically generate code based on task descriptions, their
accuracy remains limited, especially for complex tasks that require a deeper
understanding of both the problem statement and the code generation process.
This limitation is primarily due to the LLMs' need to simultaneously comprehend
text and generate syntactically and semantically correct code, without having
the capability to automatically refine the code. In real-world software
development, programmers rarely produce flawless code in a single attempt based
on the task description alone, they rely on iterative feedback and debugging to
refine their programs. Inspired by this process, we introduce a novel
architecture of LLM-based agents for code generation and automatic debugging:
Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based
agent debugger that leverages three distinct LLM agents-Guide Agent, Debug
Agent, and Feedback Agent. RGD decomposes the code generation task into
multiple steps, ensuring a clearer workflow and enabling iterative code
refinement based on self-reflection and feedback. Experimental results
demonstrate that RGD exhibits remarkable code generation capabilities,
achieving state-of-the-art performance with a 9.8% improvement on the HumanEval
dataset and a 16.2% improvement on the MBPP dataset compared to the
state-of-the-art approaches and traditional direct prompting approaches. We
highlight the effectiveness of the RGD framework in enhancing LLMs' ability to
generate and refine code autonomously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic deductive coding in discourse analysis: an application of
  <span class="highlight-title">large language model</span>s in learning analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lishan Zhang, Han Wu, Xiaoshan Huang, Tengfei Duan, Hanxiang Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deductive coding is a common discourse analysis method widely used by
learning science and learning analytics researchers for understanding teaching
and learning interactions. It often requires researchers to manually label all
discourses to be analyzed according to a theoretically guided coding scheme,
which is time-consuming and labor-intensive. The emergence of large language
models such as GPT has opened a new avenue for automatic deductive coding to
overcome the limitations of traditional deductive coding. To evaluate the
usefulness of large language models in automatic deductive coding, we employed
three different classification methods driven by different artificial
intelligence technologies, including the traditional text classification method
with text feature engineering, BERT-like pretrained language model and GPT-like
pretrained large language model (LLM). We applied these methods to two
different datasets and explored the potential of GPT and prompt engineering in
automatic deductive coding. By analyzing and comparing the accuracy and Kappa
values of these three classification methods, we found that GPT with prompt
engineering outperformed the other two methods on both datasets with limited
number of training samples. By providing detailed prompt structures, the
reported work demonstrated how large language models can be used in the
implementation of automatic deductive coding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CASE: Efficient Curricular Data <span class="highlight-title">Pre-train</span>ing for Building Assistive
  Psychology Expert Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, TK Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit Sodhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited availability of psychologists necessitates efficient
identification of individuals requiring urgent mental healthcare. This study
explores the use of Natural Language Processing (NLP) pipelines to analyze text
data from online mental health forums used for consultations. By analyzing
forum posts, these pipelines can flag users who may require immediate
professional attention. A crucial challenge in this domain is data privacy and
scarcity. To address this, we propose utilizing readily available curricular
texts used in institutes specializing in mental health for pre-training the NLP
pipelines. This helps us mimic the training process of a psychologist. Our work
presents CASE-BERT that flags potential mental health disorders based on forum
text. CASE-BERT demonstrates superior performance compared to existing methods,
achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the
most commonly reported mental health disorders. Our code and data are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Conjuring: Multi-User Runtime Collaboration with AI in Building
  Virtual 3D Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amina Kobenova, Cyan DeVeaux, Samyak Parajuli, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence has shown promise in prompting virtual
worlds into existence, yet little attention has been given to understanding how
this process unfolds as social interaction. We present Social Conjurer, a
framework for AI-augmented dynamic 3D scene co-creation, where multiple users
collaboratively build and modify virtual worlds in real-time. Through an
expanded set of interactions, including social and tool-based engagements as
well as spatial reasoning, our framework facilitates the creation of rich,
diverse virtual environments. Findings from a preliminary user study (N=12)
provide insight into the user experience of this approach, how social contexts
shape the prompting of spatial environments, and perspective on social
applications of prompt-based 3D co-creation. In addition to highlighting the
potential of AI-supported multi-user world creation and offering new pathways
for AI-augmented creative processes in VR, this article presents a set of
implications for designing human-centered interfaces that incorporate AI models
into 3D content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding
  issues in arXiv compilation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliminating Position Bias of Language Models: A Mechanistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Position bias has proven to be a prevalent issue of modern language models
(LMs), where the models prioritize content based on its position within the
given context. This bias often leads to unexpected model failures and hurts
performance, robustness, and reliability across various applications. Our
mechanistic analysis attributes the position bias to two components employed in
nearly all state-of-the-art LMs: causal attention and relative positional
encodings. Based on the analyses, we propose to eliminate position bias (e.g.,
different retrieved documents' orders in QA affect performance) with a
training-free zero-shot approach. Our method changes the causal attention to
bidirectional attention between documents and utilizes model attention values
to decide the relative orders of documents instead of using the order provided
in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the
document level. By eliminating position bias, models achieve better performance
and reliability in downstream tasks, including LM-as-a-judge,
retrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE
is especially useful when adapting LMs for evaluating reasoning pairs: it
consistently provides 8 to 10 percentage points performance gains, making
Llama-3-70B-Instruct perform even better than GPT-4-0125-preview and
GPT-4o-2024-08-06 on the RewardBench reasoning set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Optimal LR Across Token Horizons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset
size and cluster size. It is economically infeasible to extensively tune
hyperparameter for the largest runs. Instead, approximately optimal
hyperparameters must be inferred or \textit{transferred} from smaller
experiments. Hyperparameter transfer across model sizes has been studied in
Yang et al. However, hyperparameter transfer across dataset size -- or token
horizon -- has not been studied yet. To remedy this we conduct a large scale
empirical study on how optimal learning rate (LR) depends on token horizon in
LLM training. We first demonstrate that the optimal LR changes significantly
with token horizon -- longer training necessitates smaller LR. Secondly we
demonstrate the the optimal LR follows a scaling law, and that the optimal LR
for longer horizons can be accurately estimated from shorter horizons via such
scaling laws. We also provide a rule-of-thumb for transferring LR across token
horizons with zero overhead over current practices. Lastly we provide evidence
that LLama-1 used too high LR, and estimate the performance hit from this. We
thus argue that hyperparameter transfer across data size is an important and
overlooked component of LLM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Preference Optimization: Toward Controllable
  Multi-Objective Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun, Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment in artificial intelligence pursues the consistency between model
responses and human preferences as well as values. In practice, the
multifaceted nature of human preferences inadvertently introduces what is known
as the "alignment tax" -a compromise where enhancements in alignment within one
objective (e.g.,harmlessness) can diminish performance in others
(e.g.,helpfulness). However, existing alignment techniques are mostly
unidirectional, leading to suboptimal trade-offs and poor flexibility over
various objectives. To navigate this challenge, we argue the prominence of
grounding LLMs with evident preferences. We introduce controllable preference
optimization (CPO), which explicitly specifies preference scores for different
objectives, thereby guiding the model to generate responses that meet the
requirements. Our experimental analysis reveals that the aligned models can
provide responses that match various preferences among the "3H" (helpfulness,
honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and
alignment goals, we surpass baseline methods in aligning with single
objectives, hence mitigating the impact of the alignment tax and achieving
Pareto improvements in multi-objective alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ README: Bridging Medical Jargon and Lay Understanding for Patient
  Education through Data-Centric NLP <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15561v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15561v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan, Sunjae Kwon, Zhichao Yang, README annotation team, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement in healthcare has shifted focus toward patient-centric
approaches, particularly in self-care and patient education, facilitated by
access to Electronic Health Records (EHR). However, medical jargon in EHRs
poses significant challenges in patient comprehension. To address this, we
introduce a new task of automatically generating lay definitions, aiming to
simplify complex medical terms into patient-friendly lay language. We first
created the README dataset, an extensive collection of over 50,000 unique
(medical term, lay definition) pairs and 300,000 mentions, each offering
context-aware lay definitions manually annotated by domain experts. We have
also engineered a data-centric Human-AI pipeline that synergizes data
filtering, augmentation, and selection to improve data quality. We then used
README as the training data for models and leveraged a Retrieval-Augmented
Generation method to reduce hallucinations and improve the quality of model
outputs. Our extensive automatic and human evaluations demonstrate that
open-source mobile-friendly models, when fine-tuned with high-quality data, are
capable of matching or even surpassing the performance of state-of-the-art
closed-source large language models like ChatGPT. This research represents a
significant stride in closing the knowledge gap in patient education and
advancing patient-centric healthcare solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Findings of the Association for Computational
  Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Dynamics of LLM Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Danica J. Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dynamics, which describes how the learning of specific training
examples influences the model's predictions on other examples, gives us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during different types of
finetuning, by analyzing the step-wise decomposition of how influence
accumulates among different potential responses. Our framework allows a uniform
interpretation of many interesting observations about the training of popular
algorithms for both instruction tuning and preference tuning. In particular, we
propose a hypothetical explanation of why specific types of hallucination are
strengthened after finetuning, e.g., the model might use phrases or facts in
the response for question B to answer question A, or the model might keep
repeating similar simple phrases when generating responses. We also extend our
framework and highlight a unique "squeezing effect" to explain a previously
observed phenomenon in off-policy direct preference optimization (DPO), where
running DPO for too long makes even the desired outputs less likely. This
framework also provides insights into where the benefits of on-policy DPO and
other variants come from. The analysis not only provides a novel perspective of
understanding LLM's finetuning but also inspires a simple, effective method to
improve alignment performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCAR: Efficient Instruction-Tuning for <span class="highlight-title">Large Language Model</span>s via Style
  Consistency-Aware Response Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10882v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10882v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that maintaining a consistent response style by
human experts and enhancing data quality in training sets can significantly
improve the performance of fine-tuned Large Language Models (LLMs) while
reducing the number of training examples needed. However, the precise
definition of style and the relationship between style, data quality, and LLM
performance remains unclear. This research identifies two key stylistic
elements in responses: linguistic form and semantic surprisal. We find that,
among training data of comparable quality, higher consistency in these response
elements leads to better LLM performance. Inspired by this, we introduce Style
Consistency-Aware Response Ranking (SCAR), which automatically prioritizes
instruction-response pairs in the training set based on their response
stylistic consistency. By selecting the most style-consistent examples,
sometimes as few as 0.7% of the full dataset, the fine-tuned LLMs can match or
even surpass the performance of models trained on the entire dataset in coding
and open-ended question-answering benchmarks. Code and data are available at
https://github.com/zhuang-li/SCAR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogicAsker: Evaluating and Improving the Logical Reasoning Ability of
  <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LogicAsker, a novel approach for evaluating and enhancing the
logical reasoning capabilities of large language models (LLMs) such as ChatGPT
and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code
generation, and machine translation, assessing their ability to reason has been
challenging. Traditional evaluations often prioritize accuracy on downstream
tasks over direct assessments of reasoning processes. LogicAsker addresses this
gap by employing a set of atomic reasoning skills grounded in propositional and
predicate logic to systematically examine and improve the reasoning prowess of
LLMs. Our methodology reveals significant gaps in LLMs' learning of logical
rules, with identified reasoning failures ranging from 29\% to 90\% across
different models. Moreover, we leverage these findings to construct targeted
demonstration examples and fine-tune data, notably enhancing logical reasoning
in models like GPT-4o by up to 5\%. To our knowledge, this is the first effort
to utilize test case outcomes to effectively refine LLMs' formal reasoning
capabilities. We make our code, data, and results publicly available
(https://github.com/yxwan123/LogicAsker) to facilitate further research and
replication of our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SysCaps: Language Interfaces for Simulation Surrogates of Complex
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are used to predict the behavior of complex energy systems
that are too expensive to simulate with traditional numerical methods. Our work
introduces the use of language descriptions, which we call "system captions" or
SysCaps, to interface with such surrogates. We argue that interacting with
surrogates through text, particularly natural language, makes these models more
accessible for both experts and non-experts. We introduce a lightweight
multimodal text and timeseries regression model and a training pipeline that
uses large language models (LLMs) to synthesize high-quality captions from
simulation metadata. Our experiments on two real-world simulators of buildings
and wind farms show that our SysCaps-augmented surrogates have better accuracy
on held-out systems than traditional methods while enjoying new generalization
abilities, such as handling semantically related descriptions of the same test
system. Additional experiments also highlight the potential of SysCaps to
unlock language-driven design space exploration and to regularize training
through prompt augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success across diverse
tasks, yet their inference processes are hindered by substantial time and
energy demands due to single-token generation at each decoding step. While
previous methods such as speculative decoding mitigate these inefficiencies by
producing multiple tokens per step, each token is still generated by its
single-token distribution, thereby enhancing speed without improving
effectiveness. In contrast, our work simultaneously enhances inference speed
and improves the output effectiveness. We consider multi-token joint decoding
(MTJD), which generates multiple tokens from their joint distribution at each
iteration, theoretically reducing perplexity and enhancing task performance.
However, MTJD suffers from the high cost of sampling from the joint
distribution of multiple tokens. Inspired by speculative decoding, we introduce
multi-token assisted decoding (MTAD), a novel framework designed to accelerate
MTJD. MTAD leverages a smaller auxiliary model to approximate the joint
distribution of a larger model, incorporating a verification mechanism that not
only ensures the accuracy of this approximation, but also improves the decoding
efficiency over conventional speculative decoding. Theoretically, we
demonstrate that MTAD closely approximates exact MTJD with bounded error.
Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B
parameters across various tasks reveal that MTAD reduces perplexity by 21.2%
and improves downstream performance compared to standard single-token sampling.
Furthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than
conventional speculative decoding methods. These results highlight MTAD's
ability to make multi-token joint decoding both effective and efficient,
promoting more sustainable and high-performance deployment of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool-Plan<span class="highlight-title">ner</span>: Task Planning with Clusters across Multiple Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated exceptional reasoning
capabilities, enabling them to solve various complex problems. Recently, this
ability has been applied to the paradigm of tool learning. Tool learning
involves providing examples of tool usage and their corresponding functions,
allowing LLMs to formulate plans and demonstrate the process of invoking and
executing each tool. LLMs can address tasks that they cannot complete
independently, thereby enhancing their potential across different tasks.
However, this approach faces two key challenges. First, redundant error
correction leads to unstable planning and long execution time. Additionally,
designing a correct plan among multiple tools is also a challenge in tool
learning. To address these issues, we propose Tool-Planner, a task-processing
framework based on toolkits. Tool-Planner groups tools based on the API
functions with the same function into a toolkit and allows LLMs to implement
planning across the various toolkits. When a tool error occurs, the language
model can reselect and adjust tools based on the toolkit. Experiments show that
our approach demonstrates a high pass and win rate across different datasets
and optimizes the planning scheme for tool learning in models such as GPT-4 and
Claude 3, showcasing the potential of our method. Our code is public at
\url{https://github.com/OceannTwT/Tool-Planner}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48pages second version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The implications of backdoor attacks on English-centric large language models
(LLMs) have been widely examined - such attacks can be achieved by embedding
malicious behaviors during training and activated under specific conditions
that trigger malicious outputs. Despite the increasing support for multilingual
capabilities in open-source and proprietary LLMs, the impact of backdoor
attacks on these systems remains largely under-explored. Our research focuses
on cross-lingual backdoor attacks against multilingual LLMs, particularly
investigating how poisoning the instruction-tuning data for one or two
languages can affect the outputs for languages whose instruction-tuning data
were not poisoned. Despite its simplicity, our empirical analysis reveals that
our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with
high attack success rates, surpassing 90% in more than 7 out of 12 languages
across various scenarios. Our findings also indicate that more powerful models
show increased susceptibility to transferable cross-lingual backdoor attacks,
which also applies to LLMs predominantly pre-trained on English data, such as
Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High
Transferability: the backdoor mechanism operates successfully in cross-lingual
response scenarios across 26 languages, achieving an average attack success
rate of 99%, and 2) Robustness: the proposed attack remains effective even
after defenses are applied. These findings expose critical security
vulnerabilities in multilingual LLMs and highlight the urgent need for more
robust, targeted defense strategies to address the unique challenges posed by
cross-lingual backdoor transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addition is All You Need for E<span class="highlight-title">ner</span>gy-efficient Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Luo, Wei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks spend most computation on floating point tensor
multiplications. In this work, we find that a floating point multiplier can be
approximated by one integer adder with high precision. We propose the
linear-complexity multiplication L-Mul algorithm that approximates floating
point number multiplication with integer addition operations. The new algorithm
costs significantly less computation resource than 8-bit floating point
multiplication but achieves higher precision. Compared to 8-bit floating point
multiplications, the proposed method achieves higher precision but consumes
significantly less bit-level computation. Since multiplying floating point
numbers requires substantially higher energy compared to integer addition
operations, applying the L-Mul operation in tensor processing hardware can
potentially reduce 95% energy cost by element-wise floating point tensor
multiplications and 80% energy cost of dot products. We calculated the
theoretical error expectation of L-Mul, and evaluated the algorithm on a wide
range of textual, visual, and symbolic tasks, including natural language
understanding, structural reasoning, mathematics, and commonsense question
answering. Our numerical analysis experiments agree with the theoretical error
estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable
precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa
outperforms float8_e5m2. Evaluation results on popular benchmarks show that
directly applying L-Mul to the attention mechanism is almost lossless. We
further show that replacing all floating point multiplications with 3-bit
mantissa L-Mul in a transformer model achieves equivalent precision as using
float8_e4m3 as accumulation precision in both fine-tuning and inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is "Typological Diversity" in NLP? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04222v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04222v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NLP research community has devoted increased attention to languages
beyond English, resulting in considerable improvements for multilingual NLP.
However, these improvements only apply to a small subset of the world's
languages. Aiming to extend this, an increasing number of papers aspires to
enhance generalizable multilingual performance across languages. To this end,
linguistic typology is commonly used to motivate language selection, on the
basis that a broad typological sample ought to imply generalization across a
broad range of languages. These selections are often described as being
'typologically diverse'. In this work, we systematically investigate NLP
research that includes claims regarding 'typological diversity'. We find there
are no set definitions or criteria for such claims. We introduce metrics to
approximate the diversity of language selection along several axes and find
that the results vary considerably across papers. Crucially, we show that
skewed language selection can lead to overestimated multilingual performance.
We recommend future work to include an operationalization of 'typological
diversity' that empirically justifies the diversity of language samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024: Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gemma 2: Improving Open Language Models at a Practical Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, Alek Andreev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Gemma 2, a new addition to the Gemma family of
lightweight, state-of-the-art open models, ranging in scale from 2 billion to
27 billion parameters. In this new version, we apply several known technical
modifications to the Transformer architecture, such as interleaving
local-global attentions (Beltagy et al., 2020a) and group-query attention
(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge
distillation (Hinton et al., 2015) instead of next token prediction. The
resulting models deliver the best performance for their size, and even offer
competitive alternatives to models that are 2-3 times bigger. We release all
our models to the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable and diverse evaluation of LLM medical knowledge mastery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mastering medical knowledge is crucial for medical-specific LLMs. However,
despite the existence of medical benchmarks like MedQA, a unified framework
that fully leverages existing knowledge bases to evaluate LLMs' mastery of
medical knowledge is still lacking. In the study, we propose a novel framework
PretexEval that dynamically generates reliable and diverse test samples to
evaluate LLMs for any given medical knowledge base. We notice that test samples
produced directly from knowledge bases by templates or LLMs may introduce
factual errors and also lack diversity. To address these issues, we introduce a
novel schema into our proposed evaluation framework that employs predicate
equivalence transformations to produce a series of variants for any given
medical knowledge point. Finally, these produced predicate variants are
converted into textual language, resulting in a series of reliable and diverse
test samples to evaluate whether LLMs fully master the given medical factual
knowledge point. Here, we use our proposed framework to systematically
investigate the mastery of medical factual knowledge of 12 well-known LLMs,
based on two knowledge bases that are crucial for clinical diagnosis and
treatment. The evaluation results illustrate that current LLMs still exhibit
significant deficiencies in fully mastering medical knowledge, despite
achieving considerable success on some famous public benchmarks. These new
findings provide valuable insights for developing medical-specific LLMs,
highlighting that current LLMs urgently need to strengthen their comprehensive
and in-depth mastery of medical knowledge before being applied to real-world
medical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many use cases require retrieving smaller portions of text, and dense
vector-based retrieval systems often perform better with shorter text segments,
as the semantics are less likely to be over-compressed in the embeddings.
Consequently, practitioners often split text documents into smaller chunks and
encode them separately. However, chunk embeddings created in this way can lose
contextual information from surrounding chunks, resulting in sub-optimal
representations. In this paper, we introduce a novel method called late
chunking, which leverages long context embedding models to first embed all
tokens of the long text, with chunking applied after the transformer model and
just before mean pooling - hence the term late in its naming. The resulting
chunk embeddings capture the full contextual information, leading to superior
results across various retrieval tasks. The method is generic enough to be
applied to a wide range of long-context embedding models and works without
additional training. To further increase the effectiveness of late chunking, we
propose a dedicated fine-tuning approach for embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3rd draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with
  Diverse Writing Styles <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaku Uchendu, Thai Le, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have enabled the generation
of open-ended high-quality texts, that are non-trivial to distinguish from
human-written texts. We refer to such LLM-generated texts as deepfake texts.
There are currently over 72K text generation models in the huggingface model
repo. As such, users with malicious intent can easily use these open-sourced
LLMs to generate harmful texts and dis/misinformation at scale. To mitigate
this problem, a computational method to determine if a given text is a deepfake
text or not is desired--i.e., Turing Test (TT). In particular, in this work, we
investigate the more general version of the problem, known as Authorship
Attribution (AA), in a multi-class setting--i.e., not only determining if a
given text is a deepfake text or not but also being able to pinpoint which LLM
is the author. We propose TopFormer to improve existing AA solutions by
capturing more linguistic patterns in deepfake texts by including a Topological
Data Analysis (TDA) layer in the Transformer-based model. We show the benefits
of having a TDA layer when dealing with imbalanced, and multi-style datasets,
by extracting TDA features from the reshaped $pooled\_output$ of our backbone
as input. This Transformer-based model captures contextual representations
(i.e., semantic and syntactic linguistic features), while TDA captures the
shape and structure of data (i.e., linguistic structures). Finally, TopFormer,
outperforms all baselines in all 3 datasets, achieving up to 7\% increase in
Macro F1 score. Our code and datasets are available at:
https://github.com/AdaUchendu/topformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The 27th European Conference on Artificial Intelligence
  (ECAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Heads are Better than One: Mixture of Modality Knowledge
  Experts for Entity Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning high-quality multi-modal entity representations is an important goal
of multi-modal knowledge graph (MMKG) representation learning, which can
enhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The
main challenge is to collaboratively model the structural information concealed
in massive triples and the multi-modal features of the entities. Existing
methods focus on crafting elegant entity-wise multi-modal fusion strategies,
yet they overlook the utilization of multi-perspective features concealed
within the modalities under diverse relational contexts. To address this issue,
we introduce a novel framework with Mixture of Modality Knowledge experts
(MoMoK for short) to learn adaptive multi-modal entity representations for
better MMKGC. We design relation-guided modality knowledge experts to acquire
relation-aware modality embeddings and integrate the predictions from
multi-modalities to achieve joint decisions. Additionally, we disentangle the
experts by minimizing their mutual information. Experiments on four public MMKG
benchmarks demonstrate the outstanding performance of MoMoK under complex
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Code and data will be released at
  https://github.com/zjukg/MoMoK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CUTE: Measuring LLMs' Understanding of Their Tokens <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Edman, Helmut Schmid, Alexander Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show remarkable performance on a wide variety of
tasks. Most LLMs split text into multi-character tokens and process them as
atomic units without direct access to individual characters. This raises the
question: To what extent can LLMs learn orthographic information? To answer
this, we propose a new benchmark, CUTE, which features a collection of tasks
designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs
on CUTE, finding that most of them seem to know the spelling of their tokens,
yet fail to use this information effectively to manipulate text, calling into
question how much of this knowledge is generalizable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Compression in Retrieval-Augmented Ge<span class="highlight-title">ner</span>ation for Large
  Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity or Relation Embeddings? An Analysis of Encoding Strategies for
  Relation Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Mtumbuka, Steven Schockaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essentially a text classification problem, which can
be tackled by fine-tuning a pre-trained language model (LM). However, a key
challenge arises from the fact that relation extraction cannot
straightforwardly be reduced to sequence or token classification. Existing
approaches therefore solve the problem in an indirect way: they fine-tune an LM
to learn embeddings of the head and tail entities, and then predict the
relationship from these entity embeddings. Our hypothesis in this paper is that
relation extraction models can be improved by capturing relationships in a more
direct way. In particular, we experiment with appending a prompt with a [MASK]
token, whose contextualised representation is treated as a relation embedding.
While, on its own, this strategy significantly underperforms the aforementioned
approach, we find that the resulting relation embeddings are highly
complementary to what is captured by embeddings of the head and tail entity. By
jointly considering both types of representations, we end up with a simple
model that outperforms the state-of-the-art across several relation extraction
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciEx: Benchmarking <span class="highlight-title">Large Language Model</span>s on Scientific Exams with Human
  Expert Grading and Automatic Grading <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), it is crucial to
have benchmarks which can evaluate the ability of LLMs on different domains.
One common use of LLMs is performing tasks on scientific topics, such as
writing algorithms, querying databases or giving mathematical proofs. Inspired
by the way university students are evaluated on such tasks, in this paper, we
propose SciEx - a benchmark consisting of university computer science exam
questions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)
multilingual, containing both English and German exams, and (2) multi-modal,
containing questions that involve images, and (3) contains various types of
freeform questions with different difficulty levels, due to the nature of
university exams. We evaluate the performance of various state-of-the-art LLMs
on our new benchmark. Since SciEx questions are freeform, it is not
straightforward to evaluate LLM performance. Therefore, we provide human expert
grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx
remain challenging for the current LLMs, where the best LLM only achieves
59.4\% exam grade on average. We also provide detailed comparisons between LLM
performance and student performance on SciEx. To enable future evaluation of
new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.
Our experiments show that, although they do not perform perfectly on solving
the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with
expert grading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KnowTuning: Knowledge-aware Fine-tuning for <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their success at many natural language processing (NLP) tasks, large
language models still struggle to effectively leverage knowledge for
knowledge-intensive tasks, manifesting limitations such as generating
incomplete, non-factual, or illogical answers. These limitations stem from
inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address
these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to
improve fine-grained and coarse-grained knowledge awareness of LLMs. We devise
a fine-grained knowledge augmentation stage to train LLMs to identify difficult
fine-grained knowledge in answers. We also propose a coarse-grained knowledge
comparison stage to train LLMs to distinguish between reliable and unreliable
knowledge, in three aspects: completeness, factuality, and logicality.
Extensive experiments on both generic and medical question answering (QA)
datasets confirm the effectiveness of KnowTuning, through automatic and human
evaluations, across various sizes of LLMs. We further verify that KnowTuning
generates more facts with less factual error rate under fine-grained facts
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has demonstrated the potential of AI agents to act as
companions that can provide constant emotional support for humans. In this
paper, we emphasize the necessity of autonomous adaptation in personal AI
companionship, an underexplored yet promising direction. Such adaptability is
crucial as it can facilitate more tailored interactions with users and allow
the agent to evolve in response to users' changing needs. However, imbuing
agents with autonomous adaptability presents unique challenges, including
identifying optimal adaptations to meet users' expectations and ensuring a
smooth transition during the adaptation process. To address them, we devise a
hierarchical framework, AutoPal, that enables controllable and authentic
adjustments to the agent's persona based on user interactions. A
personamatching dataset is constructed to facilitate the learning of optimal
persona adaptations. Extensive experiments demonstrate the effectiveness of
AutoPal and highlight the importance of autonomous adaptability in AI
companionship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientQAT: Efficient Quantization-Aware Training for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are crucial in modern natural language
processing and artificial intelligence. However, they face challenges in
managing their significant memory requirements. Although quantization-aware
training (QAT) offers a solution by reducing memory consumption through low-bit
representations with minimal accuracy loss, it is impractical due to
substantial training resources. To address this, we propose Efficient
Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.
EfficientQAT involves two consecutive phases: Block-wise training of all
parameters (Block-AP) and end-to-end training of quantization parameters
(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable
direct training of all parameters in a block-wise manner, reducing accuracy
loss in low-bit scenarios by enhancing the solution space during optimization.
E2E-QP then trains only the quantization parameters (step sizes) end-to-end,
further improving the performance of quantized models by considering
interactions among all sub-modules. Extensive experiments demonstrate that
EfficientQAT outperforms previous quantization methods across a range of
models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with
scales from 7B to 70B parameters at various quantization bits. For instance,
EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41
hours, with less than 3 points accuracy degradation compared to the full
precision (69.48 vs. 72.41). Code is available at
https://github.com/OpenGVLab/EfficientQAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An efficient and effective quantization technical to improve the
  performance of low-bits LMMs and LVLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Invisible: Captioning Videos with Metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are a common communication tool used in our day-to-day life. The
detection and generation of metaphors in textual form have been studied
extensively but metaphors in other forms have been under-explored. Recent
studies have shown that Vision-Language (VL) models cannot understand visual
metaphors in memes and adverts. As of now, no probing studies have been done
that involve complex language phenomena like metaphors with videos. Hence, we
introduce a new VL task of describing the metaphors present in the videos in
our work. To facilitate this novel task, we construct and release a manually
created dataset with 705 videos and 2115 human-written captions, along with a
new metric called Average Concept Distance (ACD), to automatically evaluate the
creativity of the metaphors generated. We also propose a novel low-resource
video metaphor captioning system: GIT-LLaVA, which obtains comparable
performance to SoTA video language models on the proposed task. We perform a
comprehensive analysis of existing video language models on this task and
publish our dataset, models, and benchmark results to enable further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging the Judges: A Systematic Investigation of Position Bias in
  Pairwise Comparative Assessments by LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07791v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07791v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, Soroush Vosoughi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge presents a promising alternative to human evaluators across
various tasks, but inherent biases, especially position bias - a tendency to
favor solutions based on their position in the prompt - have compromised its
effectiveness. Our study introduces a systematic framework to examine position
bias in pairwise comparisons, focusing on repetition stability, position
consistency, and preference fairness. This research significantly contributes
to the field by introducing new concepts for understanding position bias and
providing a multi-dimensional framework for evaluations. We conducted
experiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks
and approximately 40 solution-generating models - candidates, resulting in over
100,000 evaluation instances. Our findings confirm that position bias in
capable LLM judges is not due to random chances, along with notable variations
observed across judges and tasks. Moreover, position bias is weakly influenced
by the length of prompt components but significantly impacted by the quality
gap between solutions. These insights can help optimize judge model selections,
improve benchmark design, and inform future research on debiasing strategies,
ultimately enhancing the reliability of LLM judges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> Confidence Estimation via Black-Box Access 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, Soham Dan, Prasanna Sattigeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating uncertainty or confidence in the responses of a model can be
significant in evaluating trust not only in the responses, but also in the
model as a whole. In this paper, we explore the problem of estimating
confidence for responses of large language models (LLMs) with simply black-box
or query access to them. We propose a simple and extensible framework where, we
engineer novel features and train a (interpretable) model (viz. logistic
regression) on these features to estimate the confidence. We empirically
demonstrate that our simple framework is effective in estimating confidence of
Flan-ul2, Llama-13b and Mistral-7b on four benchmark Q\&A tasks as well as of
Pegasus-large and BART-large on two benchmark summarization tasks with it
surpassing baselines by even over $10\%$ (on AUROC) in some cases.
Additionally, our interpretable approach provides insight into features that
are predictive of confidence, leading to the interesting and useful discovery
that our confidence models built for one LLM generalize zero-shot across others
on a given dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2-Attention: Hardware-Aware Context Sharding Among Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse attention, which selectively attends to a subset of tokens in the
context was supposed to be efficient. However, its theoretical reduction in
FLOPs has rarely translated into wall-clock speed-up over its dense attention
counterparts due to the lack of hardware-aware optimizations like
FlashAttention. Meanwhile, it remains unclear whether sparse attention can
maintain the model's quality at a scale of today's large language models (LLMs)
and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library
that provides kernel optimization for sparse attention customizable at both
per-head and per-context-range levels. S2-Attention enables the exploration of
novel and high-performance sparse attention techniques, which we demonstrate
through extensive ablations across a wide range of sparse attention designs at
various model scales. From these insights, we present several basic guidelines
to design sparse attention that can achieve not only practical efficiency
improvements, but also strong downstream performance. To achieve high
parallelization and optimized memory IO, sparse attention should shard the
context heterogeneously across attention heads, where each head attends to a
different subset of tokens while collectively covering the full context.
Meanwhile, we find hybrid architectures combining sparse and dense attention
particularly beneficial in practice. S2-Attention achieves wall-clock speedup
of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with
strong downstream performance on-par with full attention and perfect retrieval
performance at a 128k context length. At inference, for 7B models, our model,
with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to
dense counterparts. S2-Attention is released with easy-to-customize APIs for
direct usage in Megatron and vLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span> vs RETRO: Exploring the Intersection of Retrieval and
  Parameter-Efficient Fine-Tuning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation
(RAG) have become popular methods for adapting large language models while
minimizing compute requirements. In this paper, we apply PEFT methods
(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer
(RETRO) and a baseline GPT model across several sizes, ranging from 823 million
to 48 billion parameters. We show that RETRO models outperform GPT models in
zero-shot settings due to their unique pre-training process but GPT models have
higher performance potential with PEFT. Additionally, our study indicates that
8B parameter models strike an optimal balance between cost and performance and
P-tuning lags behind other PEFT techniques. We further provide a comparative
analysis between applying PEFT to an Instruction-tuned RETRO model and base
RETRO model. This work presents the first comprehensive comparison of various
PEFT methods integrated with RAG, applied to both GPT and RETRO models,
highlighting their relative performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Multilingual Concepts of Human Value in <span class="highlight-title">Large Language Model</span>s:
  Is Value Alignment Consistent, Transferable and Controllable across
  Languages? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has revealed that certain abstract concepts are linearly
represented as directions in the representation space of LLMs, predominantly
centered around English. In this paper, we extend this investigation to a
multilingual context, with a specific focus on human values-related concepts
(i.e., value concepts) due to their significance for AI safety. Through our
comprehensive exploration covering 7 types of human values, 16 languages and 3
LLM series with distinct multilinguality (e.g., monolingual, bilingual and
multilingual), we first empirically confirm the presence of value concepts
within LLMs in a multilingual format. Further analysis on the cross-lingual
characteristics of these concepts reveals 3 traits arising from language
resource disparities: cross-lingual inconsistency, distorted linguistic
relationships, and unidirectional cross-lingual transfer between high- and
low-resource languages, all in terms of value concepts. Moreover, we validate
the feasibility of cross-lingual control over value alignment capabilities of
LLMs, leveraging the dominant language as a source language. Ultimately,
recognizing the significant impact of LLMs' multilinguality on our results, we
consolidate our findings and provide prudent suggestions on the composition of
multilingual data for LLMs pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 findings, code&dataset:
  https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Large Language Model</span>s Using Contrast Sets: An Experimental
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Sanwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of Natural Language Inference (NLI), especially in tasks
involving the classification of multiple input texts, the Cross-Entropy Loss
metric is widely employed as a standard for error measurement. However, this
metric falls short in effectively evaluating a model's capacity to understand
language entailments. In this study, we introduce an innovative technique for
generating a contrast set for the Stanford Natural Language Inference (SNLI)
dataset. Our strategy involves the automated substitution of verbs, adverbs,
and adjectives with their synonyms to preserve the original meaning of
sentences. This method aims to assess whether a model's performance is based on
genuine language comprehension or simply on pattern recognition. We conducted
our analysis using the ELECTRA-small model. The model achieved an accuracy of
89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%
on our contrast set, indicating a substantial 17% decline. This outcome led us
to conduct a detailed examination of the model's learning behaviors. Following
this, we improved the model's resilience by fine-tuning it with a
contrast-enhanced training dataset specifically designed for SNLI, which
increased its accuracy to 85.5% on the contrast sets. Our findings highlight
the importance of incorporating diverse linguistic expressions into datasets
for NLI tasks. We hope that our research will encourage the creation of more
inclusive datasets, thereby contributing to the development of NLI models that
are both more sophisticated and effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's Mine becomes Yours: Defining, Annotating and Detecting
  Context-Dependent Paraphrases in News Interview Dialogs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Wegmann, Tijs van den Broek, Dong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best practices for high conflict conversations like counseling or customer
support almost always include recommendations to paraphrase the previous
speaker. Although paraphrase classification has received widespread attention
in NLP, paraphrases are usually considered independent from context, and common
models and datasets are not applicable to dialog settings. In this work, we
investigate paraphrases in dialog (e.g., Speaker 1: "That book is mine."
becomes Speaker 2: "That book is yours."). We provide an operationalization of
context-dependent paraphrases, and develop a training for crowd-workers to
classify paraphrases in dialog. We introduce a dataset with utterance pairs
from NPR and CNN news interviews annotated for context-dependent paraphrases.
To enable analyses on label variation, the dataset contains 5,581 annotations
on 600 utterance pairs. We present promising results with in-context learning
and with token classification models for automatic paraphrase detection in
dialog.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as main conference paper to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical
  Reasoning in <span class="highlight-title">Large Language Model</span>s <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim, Wonseok Lee, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel learning approach designed to enhance both
mathematical reasoning and problem-solving abilities of Large Language Models
(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the
Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning
of mathematical reasoning ability is helpful for the amplification of
problem-solving ability. Thus, the initial learning with CoT is essential for
solving challenging mathematical problems. To this end, we propose a sequential
learning approach, named SAAS (Solving Ability Amplification Strategy), which
strategically transitions from CoT learning to PoT learning. Our empirical
study, involving an extensive performance comparison using several benchmarks,
demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The
results underscore the effectiveness of our sequential learning approach,
marking a significant advancement in the field of mathematical reasoning in
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Phase Accelerated <span class="highlight-title">Prompt</span> Optimization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muchen Yang, Moxin Li, Yongle Li, Zijun Chen, Chongming Gao, Junqi Zhang, Yangyang Li, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-free prompt optimization methods have made significant strides in
enhancing the performance of closed-source Large Language Models (LLMs) across
a wide range of tasks. However, existing approaches make light of the
importance of high-quality prompt initialization and the identification of
effective optimization directions, thus resulting in substantial optimization
steps to obtain satisfactory performance. In this light, we aim to accelerate
prompt optimization process to tackle the challenge of low convergence rate. We
propose a dual-phase approach which starts with generating high-quality initial
prompts by adopting a well-designed meta-instruction to delve into
task-specific information, and iteratively optimize the prompts at the sentence
level, leveraging previous tuning experience to expand prompt candidates and
accept effective ones. Extensive experiments on eight datasets demonstrate the
effectiveness of our proposed method, achieving a consistent accuracy gain over
baselines with less than five optimization steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Urdu Dependency Parsing and Treebank Development: A Syntactic and
  Morphological Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nudrat Habib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parsing is the process of analyzing a sentence's syntactic structure by
breaking it down into its grammatical components. and is critical for various
linguistic applications. Urdu is a low-resource, free word-order language and
exhibits complex morphology. Literature suggests that dependency parsing is
well-suited for such languages. Our approach begins with a basic feature model
encompassing word location, head word identification, and dependency relations,
followed by a more advanced model integrating part-of-speech (POS) tags and
morphological attributes (e.g., suffixes, gender). We manually annotated a
corpus of news articles of varying complexity. Using Maltparser and the
NivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an
unlabeled attachment score (UAS) of 84%, demonstrating the feasibility of
dependency parsing for Urdu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexism in German Online Newspaper Comments with Open-Source
  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks
  1 and 2, Closed Track) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sexism in online media comments is a pervasive challenge that often manifests
subtly, complicating moderation efforts as interpretations of what constitutes
sexism can vary among individuals. We study monolingual and multilingual
open-source text embeddings to reliably detect sexism and misogyny in
German-language online comments from an Austrian newspaper. We observed
classifiers trained on text embeddings to mimic closely the individual
judgements of human annotators. Our method showed robust performance in the
GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1
score of 0.597 (4th place, as reported on Codabench). It also accurately
predicted the distribution of human annotations in GerMS-Detect Subtask 2, with
an average Jensen-Shannon distance of 0.301 (2nd place). The computational
efficiency of our approach suggests potential for scalable applications across
various languages and linguistic contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document-Level In-Context Few-Shot Relation Extraction via <span class="highlight-title">Pre-Train</span>ed
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11085v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11085v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level relation extraction aims at inferring structured human
knowledge from textual documents. State-of-the-art methods for this task use
pre-trained language models (LMs) via fine-tuning, yet fine-tuning is
computationally expensive and cannot adapt to new relation types or new LMs. As
a remedy, we leverage the generalization capabilities of pre-trained LMs and
present a novel framework for document-level in-context few-shot relation
extraction. Our framework has three strengths: it eliminates the need (1) for
named entity recognition and (2) for human annotations of documents, and (3) it
can be updated to new LMs without re-training. We evaluate our framework using
DocRED, the largest publicly available dataset for document-level relation
extraction, and demonstrate that our framework achieves state-of-the-art
performance. We further show that our framework actually performs much better
than the original labels from the development set of DocRED. Finally, we
conduct an extensive benchmark demonstrating the effectiveness of our
framework, achieving state-of-the-art results across six relation extraction
datasets and outperforming more than 30 baseline methods. Unlike our framework,
the baseline methods have large computational overhead (e.g., from
fine-tuning). To the best of our knowledge, we are the first to reformulate the
document-level relation extraction task as a tailored in-context few-shot
learning paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongGenBench: Benchmarking Long-Form Ge<span class="highlight-title">ner</span>ation in Long Context LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02076v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02076v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evaluating the long-context capabilities of large language models (LLMs),
benchmarks such as "Needle-in-a-Haystack" (NIAH), Ruler, and Needlebench are
commonly used. While these benchmarks measure how well models understand
long-context input sequences, they do not effectively gauge the quality of
long-form text generation--a critical aspect for applications such as design
proposals and creative writing. To address this gap, we have introduced a new
long-form text evaluation benchmark, LongGenBench, which tests models' ability
to identify specific events within generated long text sequences. In this
benchmark, we prompt long-context LMs to create long-form text that must
include particular events or constraints and evaluate their ability to
incorporate these elements. We evaluated ten long-context LMs across four
distinct scenarios, three types of prompt instructions, and two different
generation-length settings (16K and 32K). Although these models perform well on
NIAH benchmarks, none demonstrated satisfactory performance on the
LongGenBench, raising concerns about their ability to generate coherent
long-form text that follows instructions. Additionally, as the length of the
generated text increases, all models exhibit a significant drop in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress; Github: https://github.com/mozhu621/LongGenBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-based Preference Optimization in Abstractive Summarization without
  Human Feedback <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaepill Choi, Kyubyung Chae, Jiwoo Song, Yohan Jo, Taesup Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In abstractive summarization, the challenge of producing concise and accurate
summaries arises from the vast amount of information contained in the source
document. Consequently, although Large Language Models (LLMs) can generate
fluent text, they often introduce inaccuracies by hallucinating content not
found in the original source. While supervised fine-tuning methods that
maximize likelihood contribute to this issue, they do not consistently enhance
the faithfulness of the summaries. Preference-based optimization methods, such
as Direct Preference Optimization (DPO), can further refine the model to align
with human preferences. However, these methods still heavily depend on costly
human feedback. In this work, we introduce a novel and straightforward approach
called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved
summarization abilities without any human feedback. By leveraging the model's
inherent summarization capabilities, we create a preference dataset that is
fully generated by the model using different decoding strategies. Our
experiments on standard summarization datasets and various metrics demonstrate
that our proposed MPO significantly enhances the quality of generated summaries
without relying on human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-as-a-Judge & Reward Model: What They Can and Cannot Do 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
in diverse contexts, such as non-English prompts, factual verification, or
challenging questions, remains unexplored. In this paper, we conduct a
comprehensive analysis of automated evaluators, reporting several key findings
on their behavior. First, we discover that English evaluation capabilities
significantly influence language-specific evaluation capabilities, often more
than the language proficiency itself, enabling evaluators trained in English to
easily transfer their skills to other languages. Second, we identify critical
shortcomings, where LLMs fail to detect and penalize errors, such as factual
inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we find that state-of-the-art evaluators struggle with
challenging prompts, in either English or Korean, underscoring their
limitations in assessing or generating complex reasoning questions. We release
the dataset and codes used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An LLM Feature-based Framework for Dialogue Constructiveness Assessment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lexin Zhou, Youmna Farag, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on dialogue constructiveness assessment focuses on (i) analysing
conversational factors that influence individuals to take specific actions, win
debates, change their perspectives or broaden their open-mindedness and (ii)
predicting constructiveness outcomes following dialogues for such use cases.
These objectives can be achieved by training either interpretable feature-based
models (which often involve costly human annotations) or neural models such as
pre-trained language models (which have empirically shown higher task accuracy
but lack interpretability). In this paper we propose an LLM feature-based
framework for dialogue constructiveness assessment that combines the strengths
of feature-based and neural approaches, while mitigating their downsides. The
framework first defines a set of dataset-independent and interpretable
linguistic features, which can be extracted by both prompting an LLM and simple
heuristics. Such features are then used to train LLM feature-based models. We
apply this framework to three datasets of dialogue constructiveness and find
that our LLM feature-based models outperform or performs at least as well as
standard feature-based models and neural models. We also find that the LLM
feature-based model learns more robust prediction rules instead of relying on
superficial shortcuts, which often trouble neural models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging the Context through Multi-Round Interactions for Jailbreaking
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which
aim to extract harmful information by subtly modifying the attack query. As
defense mechanisms evolve, directly obtaining harmful information becomes
increasingly challenging for Jailbreaking attacks. In this work, inspired from
Chomsky's transformational-generative grammar theory and human practices of
indirect context to elicit harmful information, we focus on a new attack form,
called Contextual Interaction Attack. We contend that the prior
context\u2014the information preceding the attack query\u2014plays a pivotal
role in enabling strong Jailbreaking attacks. Specifically, we propose a first
multi-turn approach that leverages benign preliminary questions to interact
with the LLM. Due to the autoregressive nature of LLMs, which use previous
conversation rounds as context during generation, we guide the model's
question-response pair to construct a context that is semantically aligned with
the attack query to execute the attack. We conduct experiments on seven
different LLMs and demonstrate the efficacy of this attack, which is black-box
and can also transfer across LLMs. We believe this can lead to further
developments and understanding of security in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Domain Content Ge<span class="highlight-title">ner</span>ation with Domain-Specific Small Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Maloo, Abhinav Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating domain-specific content using small language models poses
challenges, especially when dealing with multiple distinct datasets with
minimal overlap. In this study, we explore methods to enable a small language
model to produce coherent and relevant outputs for two different domains:
stories (Dataset A) and recipes (Dataset B). Our initial experiments show that
training individual models on each dataset yields satisfactory results, with
each model generating appropriate content within its domain. We find that
utilizing custom tokenizers tailored to each dataset significantly enhances
generation quality compared to using a generic tokenizer. Attempts to adapt a
single model to both domains using Low-Rank Adaptation (LoRA) or standard
fine-tuning do not yield substantial results, often failing to produce
meaningful outputs. Moreover, full fine-tuning without freezing the model's
existing weights leads to catastrophic forgetting, where the model loses
previously learned information and only retains knowledge from the new data. To
overcome these challenges, we employ a knowledge expansion strategy: training
only with additional parameters. This approach enables the model to generate
both stories and recipes upon request, effectively handling multiple domains
without suffering from catastrophic forgetting. Our findings demonstrate that
knowledge expansion with frozen layers is an effective method for small
language models to generate domain-specific content across distinct datasets.
This work contributes to the development of efficient multi-domain language
models and provides insights into managing catastrophic forgetting in
small-scale architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do We Need Domain-Specific Embedding Models? An Empirical Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advancements in Large
Language Models (LLMs) have further enhanced the performance of embedding
models, which are trained on massive amounts of text covering almost every
domain. These models are often benchmarked on general-purpose datasets like
Massive Text Embedding Benchmark (MTEB), where they demonstrate superior
performance. However, a critical question arises: Is the development of
domain-specific embedding models necessary when general-purpose models are
trained on vast corpora that already include specialized domain texts? In this
paper, we empirically investigate this question, choosing the finance domain as
an example. We introduce the Finance Massive Text Embedding Benchmark
(FinMTEB), a counterpart to MTEB that consists of financial domain-specific
text datasets. We evaluate the performance of seven state-of-the-art embedding
models on FinMTEB and observe a significant performance drop compared to their
performance on MTEB. To account for the possibility that this drop is driven by
FinMTEB's higher complexity, we propose four measures to quantify dataset
complexity and control for this factor in our analysis. Our analysis provides
compelling evidence that state-of-the-art embedding models struggle to capture
domain-specific linguistic and semantic patterns. Moreover, we find that the
performance of general-purpose embedding models on MTEB is not correlated with
their performance on FinMTEB, indicating the need for domain-specific embedding
benchmarks for domain-specific embedding models. This study sheds light on
developing domain-specific embedding models in the LLM era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DataSculpt: Crafting Data Landscapes for Long-Context LLMs through
  Multi-Objective Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated significant
improvements across a variety of tasks, one of which is the long-context
capability. The key to improving long-context performance lies in effective
data organization and management strategies that integrate data from multiple
domains and optimize the context window during training. Through extensive
experimental analysis, we identified three key challenges in designing
effective data management strategies that enable the model to achieve
long-context capability without sacrificing performance in other tasks: (1) a
shortage of long documents across multiple domains, (2) effective construction
of context windows, and (3) efficient organization of large-scale datasets. To
address these challenges, we introduce DataSculpt, a novel data management
framework designed for long-context training. We first formulate the
organization of training data as a multi-objective combinatorial optimization
problem, focusing on attributes including relevance, homogeneity, integrity,
and efficiency. Specifically, our approach utilizes a coarse-to-fine
methodology to optimize training data organization both efficiently and
effectively. We begin by clustering the data based on semantic similarity
(coarse), followed by a multi-objective greedy search within each cluster to
score and concatenate documents into various context windows (fine). Our
comprehensive evaluations demonstrate that DataSculpt significantly enhances
long-context training performance, resulting in improvements of 18.09% in
retrieval augmentation, 21.23% in summarization, 21.27% in reading
comprehension, and a 3.81% increase in code completion, while also maintaining
overall model proficiency with a 4.88% improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Reliable Are Automatic Evaluation Methods for Instruction-Tuned
  LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10770v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10770v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Doostmohammadi, Oskar Holmström, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Work on instruction-tuned Large Language Models (LLMs) has used automatic
methods based on text overlap and LLM judgments as cost-effective alternatives
to human evaluation. In this paper, we perform a meta-evaluation of such
methods and assess their reliability across a broad range of tasks. In
evaluating how well automatic methods align with human evaluations, correlation
metrics are the most commonly employed method despite their inherent
limitations when dealing with ties and different scales. To address these
shortcomings, we use Pairwise Accuracy as an alternative to standard
correlation measures. We observe that while automatic evaluation methods can
approximate human ratings under specific conditions, their validity is highly
context-dependent. Specifically, the simple ROUGE-L metric correlates very well
with human ratings for short-answer English tasks but is unreliable in
free-form generation tasks and cross-lingual scenarios. The effectiveness of
the more advanced method of using GPT-4 as a judge diminishes significantly if
reference answers are not included in the prompt, which is the scenario where
this method has the potential to provide the most value compared to other
metrics. Our findings enhance the understanding of how automatic methods should
be applied and interpreted when developing and evaluating instruction-tuned
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moshi: a speech-text foundation model for real-time dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, Neil Zeghidour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Moshi, a speech-text foundation model and full-duplex spoken
dialogue framework. Current systems for spoken dialogue rely on pipelines of
independent components, namely voice activity detection, speech recognition,
textual dialogue and text-to-speech. Such frameworks cannot emulate the
experience of real conversations. First, their complexity induces a latency of
several seconds between interactions. Second, text being the intermediate
modality for dialogue, non-linguistic information that modifies meaning -- such
as emotion or non-speech sounds -- is lost in the interaction. Finally, they
rely on a segmentation into speaker turns, which does not take into account
overlapping speech, interruptions and interjections. Moshi solves these
independent issues altogether by casting spoken dialogue as speech-to-speech
generation. Starting from a text language model backbone, Moshi generates
speech as tokens from the residual quantizer of a neural audio codec, while
modeling separately its own speech and that of the user into parallel streams.
This allows for the removal of explicit speaker turns, and the modeling of
arbitrary conversational dynamics. We moreover extend the hierarchical
semantic-to-acoustic token generation of previous work to first predict
time-aligned text tokens as a prefix to audio tokens. Not only this "Inner
Monologue" method significantly improves the linguistic quality of generated
speech, but we also illustrate how it can provide streaming speech recognition
and text-to-speech. Our resulting model is the first real-time full-duplex
spoken large language model, with a theoretical latency of 160ms, 200ms in
practice, and is available at https://github.com/kyutai-labs/moshi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out
  Document <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11184v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11184v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Yang, Seunghyun Yoon, Byeongjeong Kim, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through the advent of pre-trained language models, there have been notable
advancements in abstractive summarization systems. Simultaneously, a
considerable number of novel methods for evaluating factual consistency in
abstractive summarization systems has been developed. But these evaluation
approaches incorporate substantial limitations, especially on refinement and
interpretability. In this work, we propose highly effective and interpretable
factual inconsistency detection method metric Factual Inconsistency Detection
by Zoom-in Summary and Zoom-out Document for abstractive summarization systems
that is based on fine-grained atomic facts decomposition. Moreover, we align
atomic facts decomposed from the summary with the source document through
adaptive granularity expansion. These atomic facts represent a more
fine-grained unit of information, facilitating detailed understanding and
interpretability of the summary's factual inconsistency. Experimental results
demonstrate that our proposed factual consistency checking system significantly
outperforms existing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routoo: Learning to Route to <span class="highlight-title">Large Language Model</span>s Effectively 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Mohammadshahi, Arshad Rafiq Shaikh, Majid Yazdani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs with superior response quality--particularly larger or closed-source
models--often come with higher inference costs, making their deployment
inefficient and costly. Meanwhile, developing foundational LLMs from scratch is
becoming increasingly resource-intensive and impractical for many applications.
To address the challenge of balancing quality and cost, we introduce Routoo, an
architecture designed to optimize the selection of LLMs for specific prompts
based on performance, cost, and efficiency. Routoo provides controllability
over the trade-off between inference cost and quality, enabling significant
reductions in inference costs for a given quality requirement. Routoo comprises
two key components: a performance predictor and cost-aware selector. The
performance predictor is a lightweight LLM that estimates the expected
performance of various underlying LLMs on a given prompt without executing
them. The cost-aware selector module then selects the most suitable model based
on these predictions and constraints such as cost and latency, significantly
reducing inference costs for the same quality. We evaluated Routoo using the
MMLU benchmark across 57 domains employing open-source models. Our results show
that Routoo matches the performance of the Mixtral 8x7b model while reducing
inference costs by one-third. Additionally, by allowing increased costs, Routoo
surpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an
accuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly
matches GPT4's performance at half the cost and exceeds it with a 25% cost
reduction. These outcomes highlight Routoo's potential to significantly reduce
inference costs without compromising quality, and even to establish new
state-of-the-art results by leveraging the collective capabilities of multiple
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian WeakS-to-Strong from Text Classification to Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyun Cui, Ziyang Zhang, Wen Wu, Guangzhi Sun, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in large language models raise the question of how alignment
techniques will adapt as models become increasingly complex and humans will
only be able to supervise them weakly. Weak-to-Strong mimics such a scenario
where weak model supervision attempts to harness the full capabilities of a
much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by
exploring an ensemble of weak models which simulate the variability in human
opinions. Confidence scores are estimated using a Bayesian approach to guide
the WeakS-to-Strong generalization. Furthermore, we extend the application of
WeakS-to-Strong from text classification tasks to text generation tasks where
more advanced strategies are investigated for supervision. Moreover, direct
preference optimization is applied to advance the student model's preference
learning, beyond the basic learning framework of teacher forcing. Results
demonstrate the effectiveness of the proposed approach for the reliability of a
strong student model, showing potential for superalignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nebula: A discourse aware Minecraft Builder <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Chaturvedi, Kate Thompson, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We finetune an LLM to
predict actions based on prior context; our model, Nebula, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Effective Negotiators? Systematic Evaluation of the
  Multifaceted Capabilities of LLMs in Negotiation Dialogues <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale M. Lucas, Jonathan Gratch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A successful negotiation requires a range of capabilities, including
comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer
the partner's motives, strategic reasoning, and effective communication, making
it challenging for automated systems. Despite the remarkable performance of
LLMs in various NLP tasks, there is no systematic evaluation of their
capabilities in negotiation. Such an evaluation is critical for advancing AI
negotiation agents and negotiation research, ranging from designing dialogue
systems to providing pedagogical feedback and scaling up data collection
practices. This work aims to systematically analyze the multifaceted
capabilities of LLMs across diverse dialogue scenarios throughout the stages of
a typical negotiation interaction. Our analysis highlights GPT-4's superior
performance in many tasks while identifying specific challenges, such as making
subjective assessments and generating contextually appropriate, strategically
advantageous responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Instruction Tuning of LLMs with Domain Coverage Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhou Wang, Yaxin Du, Zhuzhong Qian, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited
cross-client private data together with server-side public data for instruction
augmentation, ultimately boosting model performance within specific domains. To
date, the factors affecting FedDIT remain unclear, and existing instruction
augmentation methods primarily focus on the centralized setting without
considering distributed environments. Our experiments reveal that the
cross-client domain coverage, rather than data heterogeneity, drives model
performance in FedDIT. In response, we propose FedDCA, which optimizes domain
coverage through greedy client center selection and retrieval-based
augmentation. For client-side computational efficiency and system scalability,
FedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with
server-side feature alignment. Extensive experiments across four distinct
domains (code, medical, financial, and mathematical) substantiate the
effectiveness of both methods. Additionally, we investigate privacy
preservation against memory extraction attacks utilizing various amounts of
public data. Results show that there is no significant correlation between the
volume of public data and the privacy-preserving capability. However, as the
fine-tuning rounds increase, the risk of privacy leakage reduces or converges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Llamipa: An Incremental Discourse Parser <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides the first discourse parsing experiments with a large
language model(LLM) finetuned on corpora annotated in the style of SDRT
(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,
2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),
that leverages discourse context, leading to substantial performance gains over
approaches that use encoder-only models to provide local, context-sensitive
representations of discourse units. Furthermore, it can process discourse data
incrementally, which is essential for the eventual use of discourse information
in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show Me What's Wrong!: Combining Charts and Text to Guide Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatriz Feliciano, Rita Costa, Jean Alves, Javier Liébana, Diogo Duarte, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome
but vital task across different domains. In the context of financial fraud
detection, analysts must quickly identify suspicious activity among
transactional data. This is an iterative process made of complex exploratory
tasks such as recognizing patterns, grouping, and comparing. To mitigate the
information overload inherent to these steps, we present a tool combining
automated information highlights, Large Language Model generated textual
insights, and visual analytics, facilitating exploration at different levels of
detail. We perform a segmentation of the data per analysis area and visually
represent each one, making use of automated visual cues to signal which require
more attention. Upon user selection of an area, our system provides textual and
graphical summaries. The text, acting as a link between the high-level and
detailed views of the chosen segment, allows for a quick understanding of
relevant details. A thorough exploration of the data comprising the selection
can be done through graphical representations. The feedback gathered in a study
performed with seven domain experts suggests our tool effectively supports and
guides exploratory analysis, easing the identification of suspicious
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Just ASR + LLM? A Study on Speech <span class="highlight-title">Large Language Model</span>s' Ability to
  Identify and Understand Speaker in Spoken Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04927v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04927v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Wu, Xulin Fan, Bo-Ru Lu, Xilin Jiang, Nima Mesgarani, Mark Hasegawa-Johnson, Mari Ostendorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have observed a rapid advancement in speech language
models (SpeechLLMs), catching up with humans' listening and reasoning
abilities. SpeechLLMs have demonstrated impressive spoken dialog
question-answering (SQA) performance in benchmarks like Gaokao, the English
listening test of the college entrance exam in China, which seemingly requires
understanding both the spoken content and voice characteristics of speakers in
a conversation. However, after carefully examining Gaokao's questions, we find
the correct answers to many questions can be inferred from the conversation
transcript alone, i.e.\ without speaker segmentation and identification. Our
evaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and
our proposed "What Do You Like?" dataset shows a significantly higher accuracy
in these context-based questions than in identity-critical questions, which can
only be answered reliably with correct speaker identification. The results and
analysis suggest that when solving SQA, the current SpeechLLMs exhibit limited
speaker awareness from the audio and behave similarly to an LLM reasoning from
the conversation transcription without sound. We propose that tasks focused on
identity-critical questions could offer a more accurate evaluation framework of
SpeechLLMs in SQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pruning Multilingual <span class="highlight-title">Large Language Model</span>s for Multilingual Inference <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwichan Kim, Jun Suzuki, Tosho Hirasawa, Mamoru Komachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (MLLMs), trained on multilingual balanced
data, demonstrate better zero-shot learning performance in non-English
languages compared to large language models trained on English-dominant data.
However, the disparity in performance between English and non-English languages
remains a challenge yet to be fully addressed. A distinctive characteristic of
MLLMs is their high-quality translation capabilities, indicating an acquired
proficiency in aligning between languages. This study explores how to enhance
the zero-shot performance of MLLMs in non-English languages by leveraging their
alignment capability between English and non-English languages. To achieve
this, we first analyze the behavior of MLLMs when performing translation and
reveal that there are large magnitude features that play a critical role in the
translation process. Inspired by these findings, we retain the weights
associated with operations involving the large magnitude features and prune
other weights to force MLLMs to rely on these features for tasks beyond
translation. We empirically demonstrate that this pruning strategy can enhance
the MLLMs' performance in non-English language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Data Long-Term Preservation in Cultural Heritage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Amico, Achille Felicetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The report explores the challenges and strategies for preserving 3D digital
data in cultural heritage. It discusses the issue of technological
obsolescence, emphasising the need for ustainable storage solutions and ongoing
data management strategies. Key topics include understanding technological
obsolescence, the lifecycle of digital content, digital continuity, data
management plans (DMP), FAIR principles, and the use of public repositories.
The report also covers the importance of metadata in long-term digital
preservation, including types of metadata and strategies for building valuable
metadata. It examines the evolving standards and interoperability in 3D format
preservation and the importance of managing metadata and paradata. The document
provides a comprehensive overview of the challenges and solutions for
preserving 3D cultural heritage data in the long term.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing Cross-lingual Open Domain Question Answering with
  Large-scale Synthetic Supervision <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Jiang, Tom Drummond, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual open domain question answering (CLQA) is a complex problem,
comprising cross-lingual retrieval from a multilingual knowledge base, followed
by answer generation in the query language. Both steps are usually tackled by
separate models, requiring substantial annotated datasets, and typically
auxiliary resources, like machine translation systems to bridge between
languages. In this paper, we show that CLQA can be addressed using a single
encoder-decoder model. To effectively train this model, we propose a
self-supervised method based on exploiting the cross-lingual link structure
within Wikipedia. We demonstrate how linked Wikipedia pages can be used to
synthesise supervisory signals for cross-lingual retrieval, through a form of
cloze query, and generate more natural questions to supervise answer
generation. Together, we show our approach, \texttt{CLASS}, outperforms
comparable methods on both supervised and zero-shot language adaptation
settings, including those using machine translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bone: Block Affine Transformation as Parameter Efficient Fine-tuning
  Methods for <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has achieved remarkable training results by
freezing the original weights and training only low-rank matrices, establishing
itself as the predominant fine-tuning method for LLMs. In pursuit of
performance closer to full-parameter training, a series of LoRA variants have
emerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements
complicate the initial setup of model training and increase initialization
time. More importantly, they overlook the internal interactions of the original
weight information. To address these issues, we introduce a novel theory,
``Weight Guide'' aimed at continuously guiding trainable matrices through the
original weights during training to enhance the utilization of weight
information. Based on this theory, we designed a new PEFT technique called Bone
(\textbf{B}l\textbf{o}ck Affi\textbf{ne}), which not only enhances the
utilization of original weight information but also emphasizes the internal
connections between weights, leading to faster convergence and better data
fitting. Experimental comparisons across two different LLM architectures
(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone
structure can achieve rapid convergence and superior data fitting without the
need for complex initialization. For example, when fine-tuning LLaMA2-7B on the
MetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved
fine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by
5.84\% and 1.96\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mahed Mousavi, Simone Alghisi, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs acquire knowledge from massive data snapshots collected at different
timestamps. Their knowledge is then commonly evaluated using static benchmarks.
However, factual knowledge is generally subject to time-sensitive changes, and
static benchmarks cannot address those cases. We present an approach to
dynamically evaluate the knowledge in LLMs and their time-sensitiveness against
Wikidata, a publicly available up-to-date knowledge graph. We evaluate the
time-sensitive knowledge in twenty-four private and open-source LLMs, as well
as the effectiveness of four editing methods in updating the outdated facts.
Our results show that 1) outdatedness is a critical problem across
state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with
slight variations of the question prompt; and 3) the performance of the
state-of-the-art knowledge editing algorithms is very limited, as they can not
reduce the cases of outdatedness and output inconsistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Power of GANs in Non-Autoregressive Text Ge<span class="highlight-title">ner</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Ren, Yi Cai, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have been studied in text generation
to tackle the exposure bias problem. Despite their remarkable development, they
adopt autoregressive structures so suffering from high latency in both training
and inference stages. Although GANs have potential to support efficient
generation by adopting non-autoregressive (NAR) structures, their explorations
in NAR models are extremely limited. In this work, we conduct pioneering study
of building language GANs based on NAR structures. We identify two issues that
constrain the performance of GAN-based NAR models. Firstly, existing methods of
incorporating latent variables provide highly similar representations which
cannot describe the diversity of different words in sentences. We tackle this
problem by proposing Position-Aware Self-Modulation, providing more diverse and
effective representations. Secondly, the attention mechanism in Transformer
cannot accurately build word dependencies in the unstable training of GANs, and
we adopt Dependency Feed Forward Network to enhance the model capacity in
dependency modeling. Armed with these two facilities, we propose a GAN-based
NAR model, Adversarial Non-autoregressive Transformer (ANT). The experimental
results demonstrate that ANT can achieve comparable performance with mainstream
models in a single forward pass and has great potential in various applications
like latent interpolation and semi-supervised learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Language Barriers: Cross-Lingual Continual <span class="highlight-title">Pre-Train</span>ing at
  Scale <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, Ming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have made significant strides
towards Artificial General Intelligence. However, training these models from
scratch requires substantial computational resources and vast amounts of text
data. In this paper, we explore an alternative approach to constructing an LLM
for a new language by continually pretraining (CPT) from existing pretrained
LLMs, instead of using randomly initialized parameters. Based on parallel
experiments on 40 model sizes ranging from 40M to 5B parameters, we find that
1) CPT converges faster and saves significant resources in a scalable manner;
2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)
with a joint data-parameter scaling term; 3) The compute-optimal data-parameter
allocation for CPT markedly differs based on our estimated scaling factors; 4)
The effectiveness of transfer at scale is influenced by training duration and
linguistic properties, while robust to data replaying, a method that
effectively mitigates catastrophic forgetting in CPT. We hope our findings
provide deeper insights into the transferability of LLMs at scale for the
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aspect-Based Sentiment Analysis Techniques: A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dineth Jayakody, Koshila Isuranda, A V A Malkith, Nisansa de Silva, Sachintha Rajith Ponnamperuma, G G N Sandamali, K L K Sudheera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the dawn of the digitalisation era, customer feedback and online
reviews are unequivocally major sources of insights for businesses.
Consequently, conducting comparative analyses of such sources has become the de
facto modus operandi of any business that wishes to give itself a competitive
edge over its peers and improve customer loyalty. Sentiment analysis is one
such method instrumental in gauging public interest, exposing market trends,
and analysing competitors. While traditional sentiment analysis focuses on
overall sentiment, as the needs advance with time, it has become important to
explore public opinions and sentiments on various specific subjects, products
and services mentioned in the reviews on a finer-granular level. To this end,
Aspect-based Sentiment Analysis (ABSA), supported by advances in Artificial
Intelligence (AI) techniques which have contributed to a paradigm shift from
simple word-level analysis to tone and context-aware analyses, focuses on
identifying specific aspects within the text and determining the sentiment
associated with each aspect. In this study, we compare several deep-NN methods
for ABSA on two benchmark datasets (Restaurant14 and Laptop-14) and found that
FAST LSA obtains the best overall results of 87.6% and 82.6% accuracy but does
not pass LSA+DeBERTa which reports 90.33% and 86.21% accuracy respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Event Causality Identification via Heuristic Semantic
  Dependency Inquiry Network <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Qiang Gao, Hongmei Wu, Li Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Causality Identification (ECI) focuses on extracting causal relations
between events in texts. Existing methods for ECI primarily rely on causal
features and external knowledge. However, these approaches fall short in two
dimensions: (1) causal features between events in a text often lack explicit
clues, and (2) external knowledge may introduce bias, while specific problems
require tailored analyses. To address these issues, we propose SemDI - a simple
and effective Semantic Dependency Inquiry Network for ECI. SemDI captures
semantic dependencies within the context using a unified encoder. Then, it
utilizes a Cloze Analyzer to generate a fill-in token based on comprehensive
context understanding. Finally, this fill-in token is used to inquire about the
causal relation between two events. Extensive experiments demonstrate the
effectiveness of SemDI, surpassing state-of-the-art methods on three widely
used benchmarks. Code is available at https://github.com/hrlics/SemDI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 camera-ready version. Code is released at
  https://github.com/hrlics/SemDI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extract, Define, Canonicalize: An LLM-based Framework for Knowledge
  Graph Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that, in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schemas
easily exceed the LLMs' context window length. Furthermore, there are scenarios
where a fixed pre-defined schema is not available and we would like the method
to construct a high-quality KG with a succinct self-generated schema. To
address these problems, we propose a three-phase framework named
Extract-Define-Canonicalize (EDC): open information extraction followed by
schema definition and post-hoc canonicalization. EDC is flexible in that it can
be applied to settings where a pre-defined target schema is available and when
it is not; in the latter case, it constructs a schema automatically and applies
self-canonicalization. To further improve performance, we introduce a trained
component that retrieves schema elements relevant to the input text; this
improves the LLMs' extraction performance in a retrieval-augmented
generation-like manner. We demonstrate on three KGC benchmarks that EDC is able
to extract high-quality triplets without any parameter tuning and with
significantly larger schemas compared to prior works. Code for EDC is available
at https://github.com/clear-nus/edc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, Proceedings of the 2024 Conference on Empirical
  Methods in Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneGen: Efficient One-Pass Unified Ge<span class="highlight-title">ner</span>ation and Retrieval for LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent advancements in Large Language Models (LLMs), which have
significantly enhanced the generative capabilities for various NLP tasks, LLMs
still face limitations in directly handling retrieval tasks. However, many
practical applications demand the seamless integration of both retrieval and
generation. This paper introduces a novel and efficient One-pass Generation and
retrieval framework (OneGen), designed to improve LLMs' performance on tasks
that require both generation and retrieval. The proposed framework bridges the
traditionally separate training approaches for generation and retrieval by
incorporating retrieval tokens generated autoregressively. This enables a
single LLM to handle both tasks simultaneously in a unified forward pass. We
conduct experiments on two distinct types of composite tasks, RAG and Entity
Linking, to validate the pluggability, effectiveness, and efficiency of OneGen
in training and inference. Furthermore, our results show that integrating
generation and retrieval within the same context preserves the generative
capabilities of LLMs while improving retrieval performance. To the best of our
knowledge, OneGen is the first to enable LLMs to conduct vector retrieval
during the generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings; code is available at
  https://github.com/zjunlp/OneGen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UI-JEPA: Towards Active Perception of User Intent through Onscreen User
  Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, Etai Littwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating user intent from a sequence of user interface (UI) actions is a
core challenge in comprehensive UI understanding. Recent advancements in
multimodal large language models (MLLMs) have led to substantial progress in
this area, but their demands for extensive model parameters, computing power,
and high latency makes them impractical for scenarios requiring lightweight,
on-device solutions with low latency or heightened privacy. Additionally, the
lack of high-quality datasets has hindered the development of such lightweight
models. To address these challenges, we propose UI-JEPA, a novel framework that
employs masking strategies to learn abstract UI embeddings from unlabeled data
through self-supervised learning, combined with an LLM decoder fine-tuned for
user intent prediction. We also introduce two new UI-grounded multimodal
datasets, "Intent in the Wild" (IIW) and "Intent in the Tame" (IIT), designed
for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos
across 219 intent categories, while IIT contains 914 videos across 10
categories. We establish the first baselines for these datasets, showing that
representations learned using a JEPA-style objective, combined with an LLM
decoder, can achieve user intent predictions that match the performance of
state-of-the-art large MLLMs, but with significantly reduced annotation and
deployment resources. Measured by intent similarity scores, UI-JEPA outperforms
GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged
across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x
reduction in computational cost and a 6.6x improvement in latency in the IIW
dataset. These results underscore the effectiveness of UI-JEPA, highlighting
its potential for lightweight, high-performance UI understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Task-Specific Directions in Parameter Efficient
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate impressive performance on downstream tasks,
yet requiring extensive resource consumption when fully fine-tuning all
parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)
strategies, such as LoRA, have been developed. In this paper, we delve into the
concept of task-specific directions (TSDs)-critical for transitioning large
models from pretrained states to task-specific enhancements in PEFT. We propose
a framework to clearly define these directions and explore their properties,
and practical utilization challenges. We then introduce a novel approach,
LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning
process, thereby enhancing model performance on targeted tasks. Extensive
experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and
in-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The
code is available at https://github.com/Chongjie-Si/Subspace-Tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revisions ongoing. Codes in
  https://github.com/Chongjie-Si/Subspace-Tuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph
  Embedding <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Zhu, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary aim of Knowledge Graph embeddings (KGE) is to learn
low-dimensional representations of entities and relations for predicting
missing facts. While rotation-based methods like RotatE and QuatE perform well
in KGE, they face two challenges: limited model flexibility requiring
proportional increases in relation size with entity dimension, and difficulties
in generalizing the model for higher-dimensional rotations. To address these
issues, we introduce OrthogonalE, a novel KGE model employing matrices for
entities and block-diagonal orthogonal matrices with Riemannian optimization
for relations. This approach enhances the generality and flexibility of KGE
models. The experimental results indicate that our new KGE model, OrthogonalE,
is both general and flexible, significantly outperforming state-of-the-art KGE
models while substantially reducing the number of relation parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 findings (Long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance
  Regularization <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahed Masoudian, Markus Frohmann, Navid Rekabsaz, Markus Schedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models frequently inherit societal biases from their training data.
Numerous techniques have been proposed to mitigate these biases during both the
pre-training and fine-tuning stages. However, fine-tuning a pre-trained
debiased language model on a downstream task can reintroduce biases into the
model. Additionally, existing debiasing methods for downstream tasks either (i)
require labels of protected attributes (e.g., age, race, or political views)
that are often not available or (ii) rely on indicators of bias, which
restricts their applicability to gender debiasing since they rely on
gender-specific words. To address this, we introduce a novel debiasing
regularization technique based on the class-wise variance of embeddings.
Crucially, our method does not require attribute labels and targets any
attribute, thus addressing the shortcomings of existing debiasing methods. Our
experiments on encoder language models and three datasets demonstrate that our
method outperforms existing strong debiasing baselines that rely on target
attribute labels while maintaining performance on the target task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-10T05:31:22.251057233Z">
            2024-10-10 05:31:22 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
