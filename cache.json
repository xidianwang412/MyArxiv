{"2024-10-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.01805v1","updated":"2024-10-02T17:59:52Z","published":"2024-10-02T17:59:52Z","title":"Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads","summary":"  Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.\n","authors":["Yuxiang Huang","Binhang Yuan","Xu Han","Chaojun Xiao","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01805v1.pdf","comment":"Preprints"},{"id":"http://arxiv.org/abs/2410.01795v1","updated":"2024-10-02T17:53:08Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01794v1","updated":"2024-10-02T17:52:41Z","published":"2024-10-02T17:52:41Z","title":"Loki: An Open-Source Tool for Fact Verification","summary":"  We introduce Loki, an open-source tool designed to address the growing\nproblem of misinformation. Loki adopts a human-centered approach, striking a\nbalance between the quality of fact-checking and the cost of human involvement.\nIt decomposes the fact-checking task into a five-step pipeline: breaking down\nlong texts into individual claims, assessing their check-worthiness, generating\nqueries, retrieving evidence, and verifying the claims. Instead of fully\nautomating the claim verification process, Loki provides essential information\nat each step to assist human judgment, especially for general users such as\njournalists and content moderators. Moreover, it has been optimized for\nlatency, robustness, and cost efficiency at a commercially usable level. Loki\nis released under an MIT license and is available on GitHub. We also provide a\nvideo presenting the system and its capabilities.\n","authors":["Haonan Li","Xudong Han","Hao Wang","Yuxia Wang","Minghan Wang","Rui Xing","Yilin Geng","Zenan Zhai","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2410.01794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01792v1","updated":"2024-10-02T17:50:19Z","published":"2024-10-02T17:50:19Z","title":"When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1","summary":"  In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.\n","authors":["R. Thomas McCoy","Shunyu Yao","Dan Friedman","Mathew D. Hardy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2410.01792v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.01791v1","updated":"2024-10-02T17:49:07Z","published":"2024-10-02T17:49:07Z","title":"DreamGarden: A Designer Assistant for Growing Games from a Single Prompt","summary":"  Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.\n","authors":["Sam Earle","Samyak Parajuli","Andrzej Banburski-Fahey"],"pdf_url":"https://arxiv.org/pdf/2410.01791v1.pdf","comment":"21 pages + appendix, 11 figures"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01784v1","updated":"2024-10-02T17:40:44Z","published":"2024-10-02T17:40:44Z","title":"OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic\n  Foundation Models","summary":"  The advancements in artificial intelligence in recent years, such as Large\nLanguage Models (LLMs), have fueled expectations for breakthroughs in genomic\nfoundation models (GFMs). The code of nature, hidden in diverse genomes since\nthe very beginning of life's evolution, holds immense potential for impacting\nhumans and ecosystems through genome modeling. Recent breakthroughs in GFMs,\nsuch as Evo, have attracted significant investment and attention to genomic\nmodeling, as they address long-standing challenges and transform in-silico\ngenomic studies into automated, reliable, and efficient paradigms. In the\ncontext of this flourishing era of consecutive technological revolutions in\ngenomics, GFM studies face two major challenges: the lack of GFM benchmarking\ntools and the absence of open-source software for diverse genomics. These\nchallenges hinder the rapid evolution of GFMs and their wide application in\ntasks such as understanding and synthesizing genomes, problems that have\npersisted for decades. To address these challenges, we introduce GFMBench, a\nframework dedicated to GFM-oriented benchmarking. GFMBench standardizes\nbenchmark suites and automates benchmarking for a wide range of open-source\nGFMs. It integrates millions of genomic sequences across hundreds of genomic\ntasks from four large-scale benchmarks, democratizing GFMs for a wide range of\nin-silico genomic applications. Additionally, GFMBench is released as\nopen-source software, offering user-friendly interfaces and diverse tutorials,\napplicable for AutoBench and complex tasks like RNA design and structure\nprediction. To facilitate further advancements in genome modeling, we have\nlaunched a public leaderboard showcasing the benchmark performance derived from\nAutoBench. GFMBench represents a step toward standardizing GFM benchmarking and\ndemocratizing GFM applications.\n","authors":["Heng Yang","Jack Cole","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2410.01784v1.pdf","comment":"https://github.com/yangheng95/OmniGenomeBench"},{"id":"http://arxiv.org/abs/2409.02449v2","updated":"2024-10-02T17:40:25Z","published":"2024-09-04T05:08:23Z","title":"What is lost in Normalization? Exploring Pitfalls in Multilingual ASR\n  Model Evaluations","summary":"  This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.\n","authors":["Kavya Manohar","Leena G Pillai"],"pdf_url":"https://arxiv.org/pdf/2409.02449v2.pdf","comment":"Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.00274v2","updated":"2024-10-02T17:34:41Z","published":"2024-09-30T23:02:51Z","title":"Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds","summary":"  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n","authors":["Amina Kobenova","Cyan DeVeaux","Samyak Parajuli","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2410.00274v2.pdf","comment":"27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01772v1","updated":"2024-10-02T17:29:34Z","published":"2024-10-02T17:29:34Z","title":"DeFine: Enhancing LLM Decision-Making with Factor Profiles and\n  Analogical Reasoning","summary":"  LLMs are ideal for decision-making due to their ability to reason over long\ncontexts and identify critical factors. However, challenges arise when\nprocessing transcripts of spoken speech describing complex scenarios. These\ntranscripts often contain ungrammatical or incomplete sentences, repetitions,\nhedging, and vagueness. For example, during a company's earnings call, an\nexecutive might project a positive revenue outlook to reassure investors,\ndespite significant uncertainty regarding future earnings. It is crucial for\nLLMs to incorporate this uncertainty systematically when making decisions. In\nthis paper, we introduce DeFine, a new framework that constructs probabilistic\nfactor profiles from complex scenarios. DeFine then integrates these profiles\nwith analogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in novel situations. Our framework\nseparates the tasks of quantifying uncertainty in complex scenarios and\nincorporating it into LLM decision-making. This approach is particularly useful\nin fields such as medical consultations, negotiations, and political debates,\nwhere making decisions under uncertainty is vital.\n","authors":["Yebowen Hu","Xiaoyang Wang","Wenlin Yao","Yiming Lu","Daoan Zhang","Hassan Foroosh","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01769v1","updated":"2024-10-02T17:25:37Z","published":"2024-10-02T17:25:37Z","title":"Quantifying Generalization Complexity for Large Language Models","summary":"  While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.\n","authors":["Zhenting Qi","Hongyin Luo","Xuliang Huang","Zhuokai Zhao","Yibo Jiang","Xiangjun Fan","Himabindu Lakkaraju","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.01769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01100v2","updated":"2024-10-02T17:09:53Z","published":"2024-07-01T09:06:57Z","title":"Eliminating Position Bias of Language Models: A Mechanistic Approach","summary":"  Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.\n","authors":["Ziqi Wang","Hanlin Zhang","Xiner Li","Kuan-Hao Huang","Chi Han","Shuiwang Ji","Sham M. Kakade","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.01100v2.pdf","comment":"26 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01744v1","updated":"2024-10-02T16:55:01Z","published":"2024-10-02T16:55:01Z","title":"LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose \\OurMethod, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v1.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2402.19085v2","updated":"2024-10-02T16:54:33Z","published":"2024-02-29T12:12:30Z","title":"Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment","summary":"  Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nPareto improvements in multi-objective alignment.\n","authors":["Yiju Guo","Ganqu Cui","Lifan Yuan","Ning Ding","Zexu Sun","Bowen Sun","Huimin Chen","Ruobing Xie","Jie Zhou","Yankai Lin","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19085v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2312.15561v4","updated":"2024-10-02T16:52:30Z","published":"2023-12-24T23:01:00Z","title":"README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP","summary":"  The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.\n","authors":["Zonghai Yao","Nandyala Siddharth Kantu","Guanghao Wei","Hieu Tran","Zhangqi Duan","Sunjae Kwon","Zhichao Yang","README annotation team","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.15561v4.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01736v1","updated":"2024-10-02T16:47:35Z","published":"2024-10-02T16:47:35Z","title":"Recursive Abstractive Processing for Retrieval in Dynamic Datasets","summary":"  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n","authors":["Charbel Chucri","Rami Azouz","Joachim Ott"],"pdf_url":"https://arxiv.org/pdf/2410.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10882v5","updated":"2024-10-02T16:46:54Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking","summary":"  Recent studies have shown that maintaining a consistent response style by\nhuman experts and enhancing data quality in training sets can significantly\nimprove the performance of fine-tuned Large Language Models (LLMs) while\nreducing the number of training examples needed. However, the precise\ndefinition of style and the relationship between style, data quality, and LLM\nperformance remains unclear. This research identifies two key stylistic\nelements in responses: linguistic form and semantic surprisal. We find that,\namong training data of comparable quality, higher consistency in these response\nelements leads to better LLM performance. Inspired by this, we introduce Style\nConsistency-Aware Response Ranking (SCAR), which automatically prioritizes\ninstruction-response pairs in the training set based on their response\nstylistic consistency. By selecting the most style-consistent examples,\nsometimes as few as 0.7% of the full dataset, the fine-tuned LLMs can match or\neven surpass the performance of models trained on the entire dataset in coding\nand open-ended question-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v5.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2410.01735v1","updated":"2024-10-02T16:46:38Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v1.pdf","comment":"20 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2410.01733v1","updated":"2024-10-02T16:46:01Z","published":"2024-10-02T16:46:01Z","title":"Visual Perception in Text Strings","summary":"  Understanding visual semantics embedded in consecutive characters is a\ncrucial capability for both large language models (LLMs) and multi-modal large\nlanguage models (MLLMs). This type of artifact possesses the unique\ncharacteristic that identical information can be readily formulated in both\ntexts and images, making them a significant proxy for analyzing modern LLMs'\nand MLLMs' capabilities in modality-agnostic vision understanding. In this\nwork, we select ASCII art as a representative artifact, where the lines and\nbrightness used to depict each concept are rendered by characters, and we frame\nthe problem as an ASCII art recognition task. We benchmark model performance on\nthis task by constructing an evaluation dataset with an elaborate\ncategorization tree and also collect a training set to elicit the models'\nvisual perception ability. Through a comprehensive analysis of dozens of\nmodels, results reveal that although humans can achieve nearly 100% accuracy,\nthe state-of-the-art LLMs and MLLMs lag far behind. Models are capable of\nrecognizing concepts depicted in the ASCII arts given only text inputs\nindicated by over 60% accuracy for some concepts, but most of them achieves\nmerely around 30% accuracy when averaged across all categories. When provided\nwith images as inputs, GPT-4o gets 82.68%, outperforming the strongest\nopen-source MLLM by 21.95%. Although models favor different kinds of ASCII art\ndepending on the modality provided, none of the MLLMs successfully benefit when\nboth modalities are supplied simultaneously. Moreover, supervised fine-tuning\nhelps improve models' accuracy especially when provided with the image\nmodality, but also highlights the need for better training techniques to\nenhance the information fusion among modalities.\n","authors":["Qi Jia","Xiang Yue","Shanshan Huang","Ziheng Qin","Yizhu Liu","Bill Yuchen Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.01733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01731v1","updated":"2024-10-02T16:43:24Z","published":"2024-10-02T16:43:24Z","title":"ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation","summary":"  The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.\n","authors":["Rinon Gal","Adi Haviv","Yuval Alaluf","Amit H. Bermano","Daniel Cohen-Or","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2410.01731v1.pdf","comment":"Project website: https://comfygen-paper.github.io/"},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01727v1","updated":"2024-10-02T16:37:19Z","published":"2024-10-02T16:37:19Z","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.01727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01724v1","updated":"2024-10-02T16:34:40Z","published":"2024-10-02T16:34:40Z","title":"Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for\n  Enhanced Batch Prompting","summary":"  Batch prompting is a common technique in large language models (LLMs) used to\nprocess multiple inputs simultaneously, aiming to improve computational\nefficiency. However, as batch sizes increase, performance degradation often\noccurs due to the model's difficulty in handling lengthy context inputs.\nExisting methods that attempt to mitigate these issues rely solely on batch\ndata arrangement and majority voting rather than improving the design of the\nbatch prompt itself. In this paper, we address these limitations by proposing\n\"Auto-Demo Prompting,\" a novel approach that leverages the question-output\npairs from earlier questions within a batch as demonstrations for subsequent\nanswer inference. We provide a formal theoretical analysis of how Auto-Demo\nPrompting functions within the autoregressive generation process of LLMs,\nillustrating how it utilizes prior outputs to optimize the model's internal\nrepresentations. Our method effectively bridges the gap between batch prompting\nand few-shot prompting, enhancing performance with only a slight compromise in\ntoken usage. Experimental results across five NLP tasks demonstrate its\neffectiveness in mitigating performance degradation and occasionally\noutperforming single prompts. Furthermore, it opens new avenues for applying\nfew-shot learning techniques, such as demonstration selection, within batch\nprompting, making it a robust solution for real-world applications.\n","authors":["Longyu Feng","Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v2","updated":"2024-10-02T16:30:34Z","published":"2024-01-01T13:53:53Z","title":"LogicAsker: Evaluating and Improving the Logical Reasoning Ability of\n  Large Language Models","summary":"  We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.19653v2","updated":"2024-10-02T16:23:12Z","published":"2024-05-30T03:12:04Z","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","summary":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call \"system captions\" or\nSysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","authors":["Patrick Emami","Zhaonan Li","Saumya Sinha","Truc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.19653v2.pdf","comment":"21 pages. Under review"},{"id":"http://arxiv.org/abs/2410.01708v1","updated":"2024-10-02T16:16:02Z","published":"2024-10-02T16:16:02Z","title":"Examining the Role of Relationship Alignment in Large Language Models","summary":"  The rapid development and deployment of Generative AI in social settings\nraise important questions about how to optimally personalize them for users\nwhile maintaining accuracy and realism. Based on a Facebook public post-comment\ndataset, this study evaluates the ability of Llama 3.0 (70B) to predict the\nsemantic tones across different combinations of a commenter's and poster's\ngender, age, and friendship closeness and to replicate these differences in\nLLM-generated comments.\n  The study consists of two parts: Part I assesses differences in semantic\ntones across social relationship categories, and Part II examines the\nsimilarity between comments generated by Llama 3.0 (70B) and human comments\nfrom Part I given public Facebook posts as input. Part I results show that\nincluding social relationship information improves the ability of a model to\npredict the semantic tone of human comments. However, Part II results show that\neven without including social context information in the prompt, LLM-generated\ncomments and human comments are equally sensitive to social context, suggesting\nthat LLMs can comprehend semantics from the original post alone. When we\ninclude all social relationship information in the prompt, the similarity\nbetween human comments and LLM-generated comments decreases. This inconsistency\nmay occur because LLMs did not include social context information as part of\ntheir training data. Together these results demonstrate the ability of LLMs to\ncomprehend semantics from the original post and respond similarly to human\ncomments, but also highlights their limitations in generalizing personalized\ncomments through prompting alone.\n","authors":["Kristen M. Altenburger","Hongda Jiang","Robert E. Kraut","Yi-Chia Wang","Jane Dwivedi-Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01707v1","updated":"2024-10-02T16:15:31Z","published":"2024-10-02T16:15:31Z","title":"Interpretable Contrastive Monte Carlo Tree Search Reasoning","summary":"  We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*.\n","authors":["Zitian Gao","Boye Niu","Xuzheng He","Haotian Xu","Hongzhang Liu","Aiwei Liu","Xuming Hu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2410.01707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01704v1","updated":"2024-10-02T16:15:04Z","published":"2024-10-02T16:15:04Z","title":"An Exploration of Self-Supervised Mutual Information Alignment for\n  Multi-Task Settings","summary":"  There is a growing need for pluralistic alignment methods that can steer\nlanguage models towards individual attributes and preferences. One such method,\nSelf-Supervised Alignment with Mutual Information (SAMI), uses conditional\nmutual information to encourage the connection between behavioral preferences\nand model responses. We conduct two experiments exploring SAMI in multi-task\nsettings. First, we compare SAMI to Direct Preference Optimization (DPO) on a\nmulti-task benchmark (MT-Bench), using a stronger model to generate training\ndata for a weaker one across diverse categories (humanities, STEM, extraction,\ncoding, math, reasoning, and roleplay). Our results indicate that one iteration\nof SAMI has a 57% win rate against DPO, with significant variation in\nperformance between task categories. Second, we examine SAMI's impact on\nmathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While\nSAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%\nboost. However, SAMI shows interesting scaling trends. When given 10 attempts,\nSAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining\nSAMI with SFT yields an additional improvement of 1.3% in multi-attempt\nsettings, though single-attempt accuracy remains unchanged.\n","authors":["Soham Govande"],"pdf_url":"https://arxiv.org/pdf/2410.01704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09722v2","updated":"2024-10-02T16:14:09Z","published":"2024-07-12T23:29:54Z","title":"Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.\n","authors":["Zongyue Qin","Ziniu Hu","Zifan He","Neha Prakriya","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01696v1","updated":"2024-10-02T16:05:01Z","published":"2024-10-02T16:05:01Z","title":"CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving\n  Long-Range Reasoning Problems using LLMs","summary":"  Large language models (LLMs) have demonstrated limitations in handling\ncombinatorial optimization problems involving long-range reasoning, partially\ndue to causal hallucinations and huge search space. As for causal\nhallucinations, i.e., the inconsistency between reasoning and corresponding\nstate transition, this paper introduces the Causal Relationship Enhancement\n(CRE) mechanism combining cause-effect interventions and the Individual\nTreatment Effect (ITE) to guarantee the solid causal rightness between each\nstep of reasoning and state transition. As for the long causal range and huge\nsearch space limiting the performances of existing models featuring\nsingle-direction search, a Dual-End Searching (DES) approach is proposed to\nseek solutions by simultaneously starting from both the initial and goal states\non the causal probability tree. By integrating CRE and DES (CreDes), our model\nhas realized simultaneous multi-step reasoning, circumventing the\ninefficiencies from cascading multiple one-step reasoning like the\nChain-of-Thought (CoT). Experiments demonstrate that CreDes significantly\noutperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning\ntasks in terms of both accuracy and time efficiency.\n","authors":["Kangsheng Wang","Xiao Zhang","Hao Liu","Songde Han","Huimin Ma","Tianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v1","updated":"2024-10-02T16:03:49Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where performance seems to stagnate at first and then\nimprove sharply and unpredictably with scale beyond a threshold. By dividing\nquestions in the datasets according to difficulty level by average performance,\nwe observe U-shaped scaling for hard questions, and inverted-U scaling followed\nby steady improvement for easy questions. Moreover, the emergence threshold\nroughly coincides with the point at which performance on easy questions reverts\nfrom inverse scaling to standard scaling. Capitalizing on the observable though\nopposing scaling trend on easy and hard questions, we propose a simple yet\neffective pipeline, called Slice-and-Sandwich, to predict both the emergence\nthreshold and model performance beyond the threshold.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.01691v1","updated":"2024-10-02T16:03:13Z","published":"2024-10-02T16:03:13Z","title":"FactAlign: Long-form Factuality Alignment of Large Language Models","summary":"  Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01691v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.03807v2","updated":"2024-10-02T16:00:39Z","published":"2024-06-06T07:30:14Z","title":"Tool-Planner: Task Planning with Clusters across Multiple Tools","summary":"  Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\n\\url{https://github.com/OceannTwT/Tool-Planner}\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yuwei Zhang","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2406.03807v2.pdf","comment":"48pages second version"},{"id":"http://arxiv.org/abs/2410.01679v1","updated":"2024-10-02T15:49:30Z","published":"2024-10-02T15:49:30Z","title":"VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment","summary":"  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n","authors":["Amirhossein Kazemnejad","Milad Aghajohari","Eva Portelance","Alessandro Sordoni","Siva Reddy","Aaron Courville","Nicolas Le Roux"],"pdf_url":"https://arxiv.org/pdf/2410.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19597v2","updated":"2024-10-02T15:47:40Z","published":"2024-04-30T14:43:57Z","title":"TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning","summary":"  The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.\n","authors":["Xuanli He","Jun Wang","Qiongkai Xu","Pasquale Minervini","Pontus Stenetorp","Benjamin I. P. Rubinstein","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2404.19597v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.01675v1","updated":"2024-10-02T15:46:40Z","published":"2024-10-02T15:46:40Z","title":"Trying to be human: Linguistic traces of stochastic empathy in language\n  models","summary":"  Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.\n","authors":["Bennett Kleinberg","Jari Zegers","Jonas Festor","Stefana Vida","Julian Präsent","Riccardo Loconte","Sanne Peereboom"],"pdf_url":"https://arxiv.org/pdf/2410.01675v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.01671v1","updated":"2024-10-02T15:39:55Z","published":"2024-10-02T15:39:55Z","title":"Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yanxin Shen","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2410.01671v1.pdf","comment":"Underreview version of LQCA, Bridge context gap for long context"},{"id":"http://arxiv.org/abs/2410.00907v2","updated":"2024-10-02T15:34:12Z","published":"2024-10-01T17:53:28Z","title":"Addition is All You Need for Energy-efficient Language Models","summary":"  Large neural networks spend most computation on floating point tensor\nmultiplications. In this work, we find that a floating point multiplier can be\napproximated by one integer adder with high precision. We propose the\nlinear-complexity multiplication L-Mul algorithm that approximates floating\npoint number multiplication with integer addition operations. The new algorithm\ncosts significantly less computation resource than 8-bit floating point\nmultiplication but achieves higher precision. Compared to 8-bit floating point\nmultiplications, the proposed method achieves higher precision but consumes\nsignificantly less bit-level computation. Since multiplying floating point\nnumbers requires substantially higher energy compared to integer addition\noperations, applying the L-Mul operation in tensor processing hardware can\npotentially reduce 95% energy cost by element-wise floating point tensor\nmultiplications and 80% energy cost of dot products. We calculated the\ntheoretical error expectation of L-Mul, and evaluated the algorithm on a wide\nrange of textual, visual, and symbolic tasks, including natural language\nunderstanding, structural reasoning, mathematics, and commonsense question\nanswering. Our numerical analysis experiments agree with the theoretical error\nestimation, which indicates that L-Mul with 4-bit mantissa achieves comparable\nprecision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa\noutperforms float8_e5m2. Evaluation results on popular benchmarks show that\ndirectly applying L-Mul to the attention mechanism is almost lossless. We\nfurther show that replacing all floating point multiplications with 3-bit\nmantissa L-Mul in a transformer model achieves equivalent precision as using\nfloat8_e4m3 as accumulation precision in both fine-tuning and inference.\n","authors":["Hongyin Luo","Wei Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04222v4","updated":"2024-10-02T15:27:36Z","published":"2024-02-06T18:29:39Z","title":"What is \"Typological Diversity\" in NLP?","summary":"  The NLP research community has devoted increased attention to languages\nbeyond English, resulting in considerable improvements for multilingual NLP.\nHowever, these improvements only apply to a small subset of the world's\nlanguages. Aiming to extend this, an increasing number of papers aspires to\nenhance generalizable multilingual performance across languages. To this end,\nlinguistic typology is commonly used to motivate language selection, on the\nbasis that a broad typological sample ought to imply generalization across a\nbroad range of languages. These selections are often described as being\n'typologically diverse'. In this work, we systematically investigate NLP\nresearch that includes claims regarding 'typological diversity'. We find there\nare no set definitions or criteria for such claims. We introduce metrics to\napproximate the diversity of language selection along several axes and find\nthat the results vary considerably across papers. Crucially, we show that\nskewed language selection can lead to overestimated multilingual performance.\nWe recommend future work to include an operationalization of 'typological\ndiversity' that empirically justifies the diversity of language samples.\n","authors":["Esther Ploeger","Wessel Poelman","Miryam de Lhoneux","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2402.04222v4.pdf","comment":"EMNLP 2024: Main Conference"},{"id":"http://arxiv.org/abs/2408.00118v3","updated":"2024-10-02T15:22:49Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Léonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ramé","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogozińska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Plucińska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Görner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Cogan","Sarah Perrin","Sébastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01651v1","updated":"2024-10-02T15:18:34Z","published":"2024-10-02T15:18:34Z","title":"Efficient Long-range Language Modeling with Self-supervised Causal\n  Retrieval","summary":"  Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.\n","authors":["Xiang Hu","Zhihao Teng","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.01651v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.14302v2","updated":"2024-10-02T15:17:35Z","published":"2024-09-22T03:13:38Z","title":"Reliable and diverse evaluation of LLM medical knowledge mastery","summary":"  Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.\n","authors":["Yuxuan Zhou","Xien Liu","Chen Ning","Xiao Zhang","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.14302v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.01648v1","updated":"2024-10-02T15:16:02Z","published":"2024-10-02T15:16:02Z","title":"DeIDClinic: A Multi-Layered Framework for De-identification of Clinical\n  Free-text Data","summary":"  De-identification is important in protecting patients' privacy for healthcare\ntext analytics. The MASK framework is one of the best on the de-identification\nshared task organised by n2c2/i2b2 challenges. This work enhances the MASK\nframework by integrating ClinicalBERT, a deep learning model specifically\nfine-tuned on clinical texts, alongside traditional de-identification methods\nlike dictionary lookup and rule-based approaches. The system effectively\nidentifies and either redacts or replaces sensitive identifiable entities\nwithin clinical documents, while also allowing users to customise the masked\ndocuments according to their specific needs. The integration of ClinicalBERT\nsignificantly improves the performance of entity recognition, achieving 0.9732\nF1-score, especially for common entities such as names, dates, and locations.\n  A risk assessment feature has also been developed, which analyses the\nuniqueness of context within documents to classify them into risk levels,\nguiding further de-identification efforts. While the system demonstrates strong\noverall performance, this work highlights areas for future improvement,\nincluding handling more complex entity occurrences and enhancing the system's\nadaptability to different clinical settings.\n","authors":["Angel Paul","Dhivin Shaji","Lifeng Han","Warren Del-Pinto","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2410.01648v1.pdf","comment":"ongoing work"},{"id":"http://arxiv.org/abs/2410.01637v1","updated":"2024-10-02T15:08:12Z","published":"2024-10-02T15:08:12Z","title":"On The Adaptation of Unlimiformer for Decoder-Only Transformers","summary":"  One of the prominent issues stifling the current generation of large language\nmodels is their limited context length. Recent proprietary models such as GPT-4\nand Claude 2 have introduced longer context lengths, 8k/32k and 100k,\nrespectively; however, despite the efforts in the community, most common\nmodels, such as LLama-2, have a context length of 4k or less. Unlimiformer\n(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation\nmethod that offloads cross-attention computations to a kNN index. However, its\nmain limitation is incompatibility with decoder-only transformers out of the\nbox. In this work, we explore practical considerations of adapting Unlimiformer\nto decoder-only transformers and introduce a series of modifications to\novercome this limitation. Moreover, we expand the original experimental setup\non summarization to include a new task (i.e., free-form Q&A) and an\ninstruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase\nthe effectiveness of these modifications on summarization, performing on par\nwith a model with 2x the context length. Moreover, we discuss limitations and\nfuture directions for free-form Q&A and instruction-tuned models.\n","authors":["Kian Ahrabian","Alon Benhaim","Barun Patra","Jay Pujara","Saksham Singhal","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2410.01637v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.04701v2","updated":"2024-10-02T15:07:09Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.\n","authors":["Michael Günther","Isabelle Mohr","Daniel James Williams","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v2.pdf","comment":"11 pages, 3rd draft"},{"id":"http://arxiv.org/abs/2309.12934v3","updated":"2024-10-02T15:04:59Z","published":"2023-09-22T15:32:49Z","title":"TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with\n  Diverse Writing Styles","summary":"  Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as deepfake texts.\nThere are currently over 72K text generation models in the huggingface model\nrepo. As such, users with malicious intent can easily use these open-sourced\nLLMs to generate harmful texts and dis/misinformation at scale. To mitigate\nthis problem, a computational method to determine if a given text is a deepfake\ntext or not is desired--i.e., Turing Test (TT). In particular, in this work, we\ninvestigate the more general version of the problem, known as Authorship\nAttribution (AA), in a multi-class setting--i.e., not only determining if a\ngiven text is a deepfake text or not but also being able to pinpoint which LLM\nis the author. We propose TopFormer to improve existing AA solutions by\ncapturing more linguistic patterns in deepfake texts by including a Topological\nData Analysis (TDA) layer in the Transformer-based model. We show the benefits\nof having a TDA layer when dealing with imbalanced, and multi-style datasets,\nby extracting TDA features from the reshaped $pooled\\_output$ of our backbone\nas input. This Transformer-based model captures contextual representations\n(i.e., semantic and syntactic linguistic features), while TDA captures the\nshape and structure of data (i.e., linguistic structures). Finally, TopFormer,\noutperforms all baselines in all 3 datasets, achieving up to 7\\% increase in\nMacro F1 score. Our code and datasets are available at:\nhttps://github.com/AdaUchendu/topformer\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2309.12934v3.pdf","comment":"Accepted at The 27th European Conference on Artificial Intelligence\n  (ECAI 2024)"},{"id":"http://arxiv.org/abs/2410.01633v1","updated":"2024-10-02T15:04:21Z","published":"2024-10-02T15:04:21Z","title":"A Thematic Framework for Analyzing Large-scale Self-reported Social\n  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product","summary":"  Background: One of the key FDA-approved medications for Opioid Use Disorder\n(OUD) is buprenorphine. Despite its popularity, individuals often report\nvarious information needs regarding buprenorphine treatment on social media\nplatforms like Reddit. However, the key challenge is to characterize these\nneeds. In this study, we propose a theme-based framework to curate and analyze\nlarge-scale data from social media to characterize self-reported treatment\ninformation needs (TINs).\n  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit\nsub-community for buprenorphine products. Following the standard protocol, we\nfirst identified and defined five main themes from the data and then coded\n6,000 posts based on these themes, where one post can be labeled with\napplicable one to three themes. Finally, we determined the most frequently\nappearing sub-themes (topics) for each theme by analyzing samples from each\ngroup.\n  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two\nthemes, and 13.9% three themes. The most frequent topics for each theme or\ntheme combination came with several key findings - prevalent reporting of\npsychological and physical effects during recovery, complexities in accessing\nbuprenorphine, and significant information gaps regarding medication\nadministration, tapering, and usage of substances during different stages of\nrecovery. Moreover, self-treatment strategies and peer-driven advice reveal\nvaluable insights and potential misconceptions.\n  Conclusions: The findings obtained using our proposed framework can inform\nbetter patient education and patient-provider communication, design systematic\ninterventions to address treatment-related misconceptions and rumors, and\nstreamline the generation of hypotheses for future research.\n","authors":["Madhusudan Basak","Omar Sharif","Sarah E. Lord","Jacob T. Borodovsky","Lisa A. Marsch","Sandra A. Springer","Edward Nunes","Charlie D. Brackett","Luke J. ArchiBald","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2410.01633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01627v1","updated":"2024-10-02T15:01:55Z","published":"2024-10-02T15:01:55Z","title":"Intent Detection in the Age of LLMs","summary":"  Intent detection is a critical component of task-oriented dialogue systems\n(TODS) which enables the identification of suitable actions to address user\nutterances at each dialog turn. Traditional approaches relied on\ncomputationally efficient supervised sentence transformer encoder models, which\nrequire substantial training data and struggle with out-of-scope (OOS)\ndetection. The emergence of generative large language models (LLMs) with\nintrinsic world knowledge presents new opportunities to address these\nchallenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context\nlearning and chain-of-thought prompting for intent detection, and compare their\nperformance with contrastively fine-tuned sentence transformer (SetFit) models\nto highlight prediction quality and latency tradeoff. We propose a hybrid\nsystem using uncertainty based routing strategy to combine the two approaches\nthat along with negative data augmentation results in achieving the best of\nboth worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To\nbetter understand LLM OOS detection capabilities, we perform controlled\nexperiments revealing that this capability is significantly influenced by the\nscope of intent labels and the size of the label space. We also introduce a\ntwo-step approach utilizing internal LLM representations, demonstrating\nempirical gains in OOS detection accuracy and F1-score by >5% for the\nMistral-7B model.\n","authors":["Gaurav Arora","Shreya Jain","Srujana Merugu"],"pdf_url":"https://arxiv.org/pdf/2410.01627v1.pdf","comment":"Accepted at EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01610v1","updated":"2024-10-02T14:48:22Z","published":"2024-10-02T14:48:22Z","title":"Upcycling Instruction Tuning from Dense to Mixture-of-Experts via\n  Parameter Merging","summary":"  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.\n","authors":["Tingfeng Hui","Zhenyu Zhang","Shuohuan Wang","Yu Sun","Hua Wu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2410.01610v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2405.16869v2","updated":"2024-10-02T14:42:10Z","published":"2024-05-27T06:36:17Z","title":"Multiple Heads are Better than One: Mixture of Modality Knowledge\n  Experts for Entity Representation Learning","summary":"  Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Binbin Hu","Ziqi Liu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.16869v2.pdf","comment":"Work in progress. Code and data will be released at\n  https://github.com/zjukg/MoMoK"},{"id":"http://arxiv.org/abs/2410.01600v1","updated":"2024-10-02T14:39:13Z","published":"2024-10-02T14:39:13Z","title":"ENTP: Encoder-only Next Token Prediction","summary":"  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n","authors":["Ethan Ewer","Daewon Chae","Thomas Zeng","Jinkyu Kim","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15452v2","updated":"2024-10-02T14:35:40Z","published":"2024-09-23T18:27:03Z","title":"CUTE: Measuring LLMs' Understanding of Their Tokens","summary":"  Large Language Models (LLMs) show remarkable performance on a wide variety of\ntasks. Most LLMs split text into multi-character tokens and process them as\natomic units without direct access to individual characters. This raises the\nquestion: To what extent can LLMs learn orthographic information? To answer\nthis, we propose a new benchmark, CUTE, which features a collection of tasks\ndesigned to test the orthographic knowledge of LLMs. We evaluate popular LLMs\non CUTE, finding that most of them seem to know the spelling of their tokens,\nyet fail to use this information effectively to manipulate text, calling into\nquestion how much of this knowledge is generalizable.\n","authors":["Lukas Edman","Helmut Schmid","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2409.15452v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2409.13385v2","updated":"2024-10-02T14:30:28Z","published":"2024-09-20T10:36:49Z","title":"Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.\n","authors":["Sourav Verma"],"pdf_url":"https://arxiv.org/pdf/2409.13385v2.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2312.11062v2","updated":"2024-10-02T14:26:30Z","published":"2023-12-18T09:58:19Z","title":"Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction","summary":"  Relation extraction is essentially a text classification problem, which can\nbe tackled by fine-tuning a pre-trained language model (LM). However, a key\nchallenge arises from the fact that relation extraction cannot\nstraightforwardly be reduced to sequence or token classification. Existing\napproaches therefore solve the problem in an indirect way: they fine-tune an LM\nto learn embeddings of the head and tail entities, and then predict the\nrelationship from these entity embeddings. Our hypothesis in this paper is that\nrelation extraction models can be improved by capturing relationships in a more\ndirect way. In particular, we experiment with appending a prompt with a [MASK]\ntoken, whose contextualised representation is treated as a relation embedding.\nWhile, on its own, this strategy significantly underperforms the aforementioned\napproach, we find that the resulting relation embeddings are highly\ncomplementary to what is captured by embeddings of the head and tail entity. By\njointly considering both types of representations, we end up with a simple\nmodel that outperforms the state-of-the-art across several relation extraction\nbenchmarks.\n","authors":["Frank Mtumbuka","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2312.11062v2.pdf","comment":"Accepted in the Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.10421v3","updated":"2024-10-02T14:20:50Z","published":"2024-06-14T21:52:21Z","title":"SciEx: Benchmarking Large Language Models on Scientific Exams with Human\n  Expert Grading and Automatic Grading","summary":"  With the rapid development of Large Language Models (LLMs), it is crucial to\nhave benchmarks which can evaluate the ability of LLMs on different domains.\nOne common use of LLMs is performing tasks on scientific topics, such as\nwriting algorithms, querying databases or giving mathematical proofs. Inspired\nby the way university students are evaluated on such tasks, in this paper, we\npropose SciEx - a benchmark consisting of university computer science exam\nquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)\nmultilingual, containing both English and German exams, and (2) multi-modal,\ncontaining questions that involve images, and (3) contains various types of\nfreeform questions with different difficulty levels, due to the nature of\nuniversity exams. We evaluate the performance of various state-of-the-art LLMs\non our new benchmark. Since SciEx questions are freeform, it is not\nstraightforward to evaluate LLM performance. Therefore, we provide human expert\ngrading of the LLM outputs on SciEx. We show that the free-form exams in SciEx\nremain challenging for the current LLMs, where the best LLM only achieves\n59.4\\% exam grade on average. We also provide detailed comparisons between LLM\nperformance and student performance on SciEx. To enable future evaluation of\nnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.\nOur experiments show that, although they do not perform perfectly on solving\nthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with\nexpert grading.\n","authors":["Tu Anh Dinh","Carlos Mullov","Leonard Bärmann","Zhaolin Li","Danni Liu","Simon Reiß","Jueun Lee","Nathan Lerzer","Fabian Ternava","Jianfeng Gao","Tobias Röddiger","Alexander Waibel","Tamim Asfour","Michael Beigl","Rainer Stiefelhagen","Carsten Dachsbacher","Klemens Böhm","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2406.10421v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.11176v3","updated":"2024-10-02T14:20:29Z","published":"2024-02-17T02:54:32Z","title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models","summary":"  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n","authors":["Yougang Lyu","Lingyong Yan","Shuaiqiang Wang","Haibo Shi","Dawei Yin","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2402.11176v3.pdf","comment":"EMNLP 2024 main paper"},{"id":"http://arxiv.org/abs/2410.01579v1","updated":"2024-10-02T14:15:13Z","published":"2024-10-02T14:15:13Z","title":"Spoken Grammar Assessment Using LLM","summary":"  Spoken language assessment (SLA) systems restrict themselves to evaluating\nthe pronunciation and oral fluency of a speaker by analysing the read and\nspontaneous spoken utterances respectively. The assessment of language grammar\nor vocabulary is relegated to written language assessment (WLA) systems. Most\nWLA systems present a set of sentences from a curated finite-size database of\nsentences thereby making it possible to anticipate the test questions and train\noneself. In this paper, we propose a novel end-to-end SLA system to assess\nlanguage grammar from spoken utterances thus making WLA systems redundant;\nadditionally, we make the assessment largely unteachable by employing a large\nlanguage model (LLM) to bring in variations in the test. We further demonstrate\nthat a hybrid automatic speech recognition (ASR) with a custom-built language\nmodel outperforms the state-of-the-art ASR engine for spoken grammar\nassessment.\n","authors":["Sunil Kumar Kopparapu","Chitralekha Bhat","Ashish Panda"],"pdf_url":"https://arxiv.org/pdf/2410.01579v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v2","updated":"2024-10-02T13:59:40Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01556v1","updated":"2024-10-02T13:52:55Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01555v1","updated":"2024-10-02T13:52:09Z","published":"2024-10-02T13:52:09Z","title":"ACE: A LLM-based Negotiation Coaching System","summary":"  The growing prominence of LLMs has led to an increase in the development of\nAI tutoring systems. These systems are crucial in providing underrepresented\npopulations with improved access to valuable education. One important area of\neducation that is unavailable to many learners is strategic bargaining related\nto negotiation. To address this, we develop a LLM-based Assistant for Coaching\nnEgotiation (ACE). ACE not only serves as a negotiation partner for users but\nalso provides them with targeted feedback for improvement. To build our system,\nwe collect a dataset of negotiation transcripts between MBA students. These\ntranscripts come from trained negotiators and emulate realistic bargaining\nscenarios. We use the dataset, along with expert consultations, to design an\nannotation scheme for detecting negotiation mistakes. ACE employs this scheme\nto identify mistakes and provide targeted feedback to users. To test the\neffectiveness of ACE-generated feedback, we conducted a user experiment with\ntwo consecutive trials of negotiation and found that it improves negotiation\nperformances significantly compared to a system that doesn't provide feedback\nand one which uses an alternative method of providing feedback.\n","authors":["Ryan Shea","Aymen Kallala","Xin Lucy Liu","Michael W. Morris","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01555v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01553v1","updated":"2024-10-02T13:47:17Z","published":"2024-10-02T13:47:17Z","title":"MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an\n  AI-SCE Framework","summary":"  Artificial intelligence (AI) and large language models (LLMs) in healthcare\nrequire advanced clinical skills (CS), yet current benchmarks fail to evaluate\nthese comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by\nmedical education's Objective Structured Clinical Examinations (OSCEs), to\naddress this gap. MedQA-CS evaluates LLMs through two instruction-following\ntasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real\nclinical scenarios. Our contributions include developing MedQA-CS, a\ncomprehensive evaluation framework with publicly available data and expert\nannotations, and providing the quantitative and qualitative assessment of LLMs\nas reliable judges in CS evaluation. Our experiments show that MedQA-CS is a\nmore challenging benchmark for evaluating clinical skills than traditional\nmultiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,\nMedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities\nfor both open- and closed-source LLMs.\n","authors":["Zonghai Yao","Zihao Zhang","Chaolong Tang","Xingyu Bian","Youxia Zhao","Zhichao Yang","Junda Wang","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2406.04886v2","updated":"2024-10-02T13:40:10Z","published":"2024-06-07T12:32:44Z","title":"Unveiling the Invisible: Captioning Videos with Metaphors","summary":"  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Sumit Shekhar"],"pdf_url":"https://arxiv.org/pdf/2406.04886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01548v1","updated":"2024-10-02T13:37:54Z","published":"2024-10-02T13:37:54Z","title":"In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks","summary":"  In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.\n","authors":["Dingzirui Wang","Xuangliang Zhang","Qiguang Chen","Longxu Dou","Xiao Xu","Rongyu Cao","Yingwei Ma","Qingfu Zhu","Wanxiang Che","Binhua Li","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01532v1","updated":"2024-10-02T13:24:56Z","published":"2024-10-02T13:24:56Z","title":"Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for\n  Large Language Models","summary":"  Advancements in Natural Language Processing (NLP), have led to the emergence\nof Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which\nexcel across a range of tasks but require extensive fine-tuning to align their\noutputs with human expectations. A widely used method for achieving this\nalignment is Reinforcement Learning from Human Feedback (RLHF), which, despite\nits success, faces challenges in accurately modelling human preferences. In\nthis paper, we introduce GazeReward, a novel framework that integrates implicit\nfeedback -- and specifically eye-tracking (ET) data -- into the Reward Model\n(RM). In addition, we explore how ET-based features can provide insights into\nuser preferences. Through ablation studies we test our framework with different\nintegration methods, LLMs, and ET generator models, demonstrating that our\napproach significantly improves the accuracy of the RM on established human\npreference datasets. This work advances the ongoing discussion on optimizing AI\nalignment with human values, exploring the potential of cognitive data for\nshaping future NLP research.\n","authors":["Angela Lopez-Cardona","Carlos Segura","Alexandros Karatzoglou","Sergi Abadal","Ioannis Arapakis"],"pdf_url":"https://arxiv.org/pdf/2410.01532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01524v1","updated":"2024-10-02T13:12:13Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01518v1","updated":"2024-10-02T13:09:41Z","published":"2024-10-02T13:09:41Z","title":"InfiniPot: Infinite Context Processing on Memory-Constrained LLMs","summary":"  Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.\n","authors":["Minsoo Kim","Kyuhong Shim","Jungwook Choi","Simyung Chang"],"pdf_url":"https://arxiv.org/pdf/2410.01518v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.07791v5","updated":"2024-10-02T13:08:43Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs","summary":"  LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.\n","authors":["Lin Shi","Chiyu Ma","Wenhua Liang","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01512v1","updated":"2024-10-02T13:02:23Z","published":"2024-10-02T13:02:23Z","title":"InstaTrans: An Instruction-Aware Translation Framework for Non-English\n  Instruction Datasets","summary":"  It is challenging to generate high-quality instruction datasets for\nnon-English languages due to tail phenomena, which limit performance on less\nfrequently observed data. To mitigate this issue, we propose translating\nexisting high-quality English instruction datasets as a solution, emphasizing\nthe need for complete and instruction-aware translations to maintain the\ninherent attributes of these datasets. We claim that fine-tuning LLMs with\ndatasets translated in this way can improve their performance in the target\nlanguage. To this end, we introduces a new translation framework tailored for\ninstruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through\nextensive experiments, we demonstrate the superiority of InstaTrans over other\ncompetitors in terms of completeness and instruction-awareness of translation,\nhighlighting its potential to broaden the accessibility of LLMs across diverse\nlanguages at a relatively low cost. Furthermore, we have validated that\nfine-tuning LLMs with datasets translated by InstaTrans can effectively improve\ntheir performance in the target language.\n","authors":["Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.01512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01508v1","updated":"2024-10-02T13:00:21Z","published":"2024-10-02T13:00:21Z","title":"Disentangling Latent Shifts of In-Context Learning Through Self-Training","summary":"  In-context learning (ICL) has become essential in natural language\nprocessing, particularly with autoregressive large language models capable of\nlearning from demonstrations provided within the prompt. However, ICL faces\nchallenges with stability and long contexts, especially as the number of\ndemonstrations grows, leading to poor generalization and inefficient inference.\nTo address these issues, we introduce STICL (Self-Training ICL), an approach\nthat disentangles the latent shifts of demonstrations from the latent shift of\nthe query through self-training. STICL employs a teacher model to generate\npseudo-labels and trains a student model using these labels, encoded in an\nadapter module. The student model exhibits weak-to-strong generalization,\nprogressively refining its predictions over time. Our empirical results show\nthat STICL improves generalization and stability, consistently outperforming\ntraditional ICL methods and other disentangling strategies across both\nin-domain and out-of-domain data.\n","authors":["Josip Jukić","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2410.01508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01504v1","updated":"2024-10-02T12:57:12Z","published":"2024-10-02T12:57:12Z","title":"PersonaMath: Enhancing Math Reasoning through Persona-Driven Data\n  Augmentation","summary":"  While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models continue to struggle\nwith such tasks. To bridge this gap, we propose a data augmentation approach\nand introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we\ntrain the PersonaMath models. Our approach consists of two stages: the first\nstage is learning from Persona Diversification, and the second stage is\nlearning from Reflection. In the first stage, we regenerate detailed\nchain-of-thought (CoT) solutions as instructions using a closed-source LLM and\nintroduce a novel persona-driven data augmentation technique to enhance the\ndataset's quantity and diversity. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on\nMATH and 68.7% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 70.3K data\npoints-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.\n","authors":["Jing Luo","Run Luo","Longze Chen","Liang Zhu","Chang Ao","Jiaming Li","Yukun Chen","Xin Cheng","Wen Yang","Jiayuan Su","Chengming Li","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2410.01504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v3","updated":"2024-10-02T12:47:50Z","published":"2024-07-25T00:27:07Z","title":"S2-Attention: Hardware-Aware Context Sharding Among Attention Heads","summary":"  Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Liliang Ren","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.01490v1","updated":"2024-10-02T12:40:11Z","published":"2024-10-02T12:40:11Z","title":"Extending Context Window of Large Language Models from a Distributional\n  Perspective","summary":"  Scaling the rotary position embedding (RoPE) has become a common method for\nextending the context window of RoPE-based large language models (LLMs).\nHowever, existing scaling methods often rely on empirical approaches and lack a\nprofound understanding of the internal distribution within RoPE, resulting in\nsuboptimal performance in extending the context window length. In this paper,\nwe propose to optimize the context window extending task from the view of\nrotary angle distribution. Specifically, we first estimate the distribution of\nthe rotary angles within the model and analyze the extent to which length\nextension perturbs this distribution. Then, we present a novel extension\nstrategy that minimizes the disturbance between rotary angle distributions to\nmaintain consistency with the pre-training phase, enhancing the model's\ncapability to generalize to longer sequences. Experimental results compared to\nthe strong baseline methods demonstrate that our approach reduces by up to 72%\nof the distributional disturbance when extending LLaMA2's context window to 8k,\nand reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,\nour method achieves an average improvement of up to 4.33% over existing\nstate-of-the-art methods. Furthermore, Our method maintains the model's\nperformance on the Hugging Face Open LLM benchmark after context window\nextension, with only an average performance fluctuation ranging from -0.12 to\n+0.22.\n","authors":["Yingsheng Wu. Yuxuan Gu","Xiaocheng Feng","Weihong Zhong","Dongliang Xu","Qing Yang","Hongtao Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.01490v1.pdf","comment":"14 pages, 8 figures, Accepted to EMNLP2024"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01487v1","updated":"2024-10-02T12:36:08Z","published":"2024-10-02T12:36:08Z","title":"Small Language Models Like Small Vocabularies: Probing the Linguistic\n  Abilities of Grapheme- and Phoneme-Based Baby Llamas","summary":"  Current language models use subword-based tokenization algorithms like Byte\nPair Encoding, which put their validity as models of linguistic representations\ninto question. In this paper, we explore the potential of tokenization-free,\nphoneme- and grapheme-based language models. We demonstrate that small models\nbased on the Llama architecture can achieve strong linguistic performance on\nstandard syntactic and novel lexical/phonetic benchmarks when trained with\ncharacter-level vocabularies. We further show that phoneme-based models without\nany graphemic biases almost match grapheme-based models in standard tasks and\nnovel evaluations. Our findings suggest a promising direction for creating more\nlinguistically plausible language models that are better suited for\ncomputational studies of language acquisition and processing.\n","authors":["Bastian Bunzeck","Daniel Duran","Leonie Schade","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2410.01487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01485v1","updated":"2024-10-02T12:35:53Z","published":"2024-10-02T12:35:53Z","title":"A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts","summary":"  Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.\n","authors":["Suyu Ge","Xihui Lin","Yunan Zhang","Jiawei Han","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18120v3","updated":"2024-10-02T12:34:25Z","published":"2024-02-28T07:18:39Z","title":"Exploring Multilingual Concepts of Human Value in Large Language Models:\n  Is Value Alignment Consistent, Transferable and Controllable across\n  Languages?","summary":"  Prior research has revealed that certain abstract concepts are linearly\nrepresented as directions in the representation space of LLMs, predominantly\ncentered around English. In this paper, we extend this investigation to a\nmultilingual context, with a specific focus on human values-related concepts\n(i.e., value concepts) due to their significance for AI safety. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality (e.g., monolingual, bilingual and\nmultilingual), we first empirically confirm the presence of value concepts\nwithin LLMs in a multilingual format. Further analysis on the cross-lingual\ncharacteristics of these concepts reveals 3 traits arising from language\nresource disparities: cross-lingual inconsistency, distorted linguistic\nrelationships, and unidirectional cross-lingual transfer between high- and\nlow-resource languages, all in terms of value concepts. Moreover, we validate\nthe feasibility of cross-lingual control over value alignment capabilities of\nLLMs, leveraging the dominant language as a source language. Ultimately,\nrecognizing the significant impact of LLMs' multilinguality on our results, we\nconsolidate our findings and provide prudent suggestions on the composition of\nmultilingual data for LLMs pre-training.\n","authors":["Shaoyang Xu","Weilong Dong","Zishan Guo","Xinwei Wu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.18120v3.pdf","comment":"EMNLP 2024 findings, code&dataset:\n  https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts"},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06670v2","updated":"2024-10-02T12:26:54Z","published":"2024-04-10T01:14:12Z","title":"What's Mine becomes Yours: Defining, Annotating and Detecting\n  Context-Dependent Paraphrases in News Interview Dialogs","summary":"  Best practices for high conflict conversations like counseling or customer\nsupport almost always include recommendations to paraphrase the previous\nspeaker. Although paraphrase classification has received widespread attention\nin NLP, paraphrases are usually considered independent from context, and common\nmodels and datasets are not applicable to dialog settings. In this work, we\ninvestigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\"\nbecomes Speaker 2: \"That book is yours.\"). We provide an operationalization of\ncontext-dependent paraphrases, and develop a training for crowd-workers to\nclassify paraphrases in dialog. We introduce a dataset with utterance pairs\nfrom NPR and CNN news interviews annotated for context-dependent paraphrases.\nTo enable analyses on label variation, the dataset contains 5,581 annotations\non 600 utterance pairs. We present promising results with in-context learning\nand with token classification models for automatic paraphrase detection in\ndialog.\n","authors":["Anna Wegmann","Tijs van den Broek","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.06670v2.pdf","comment":"Accepted as main conference paper to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01450v1","updated":"2024-10-02T12:01:32Z","published":"2024-10-02T12:01:32Z","title":"Agent-Driven Large Language Models for Mandarin Lyric Generation","summary":"  Generative Large Language Models have shown impressive in-context learning\nabilities, performing well across various tasks with just a prompt. Previous\nmelody-to-lyric research has been limited by scarce high-quality aligned data\nand unclear standard for creativeness. Most efforts focused on general themes\nor emotions, which are less valuable given current language model capabilities.\nIn tonal contour languages like Mandarin, pitch contours are influenced by both\nmelody and tone, leading to variations in lyric-melody fit. Our study,\nvalidated by the Mpop600 dataset, confirms that lyricists and melody writers\nconsider this fit during their composition process. In this research, we\ndeveloped a multi-agent system that decomposes the melody-to-lyric task into\nsub-tasks, with each agent controlling rhyme, syllable count, lyric-melody\nalignment, and consistency. Listening tests were conducted via a\ndiffusion-based singing voice synthesizer to evaluate the quality of lyrics\ngenerated by different agent groups.\n","authors":["Hong-Hsiang Liu","Yi-Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01450v1.pdf","comment":"6 pages, figures, Accepted at O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2404.03887v4","updated":"2024-10-02T11:56:35Z","published":"2024-04-05T04:25:47Z","title":"SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models","summary":"  This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.\n","authors":["Hyeonwoo Kim","Gyoungjin Gim","Yungi Kim","Jihoo Kim","Byungju Kim","Wonseok Lee","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.03887v4.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.13443v2","updated":"2024-10-02T11:46:10Z","published":"2024-06-19T11:08:56Z","title":"Dual-Phase Accelerated Prompt Optimization","summary":"  Gradient-free prompt optimization methods have made significant strides in\nenhancing the performance of closed-source Large Language Models (LLMs) across\na wide range of tasks. However, existing approaches make light of the\nimportance of high-quality prompt initialization and the identification of\neffective optimization directions, thus resulting in substantial optimization\nsteps to obtain satisfactory performance. In this light, we aim to accelerate\nprompt optimization process to tackle the challenge of low convergence rate. We\npropose a dual-phase approach which starts with generating high-quality initial\nprompts by adopting a well-designed meta-instruction to delve into\ntask-specific information, and iteratively optimize the prompts at the sentence\nlevel, leveraging previous tuning experience to expand prompt candidates and\naccept effective ones. Extensive experiments on eight datasets demonstrate the\neffectiveness of our proposed method, achieving a consistent accuracy gain over\nbaselines with less than five optimization steps.\n","authors":["Muchen Yang","Moxin Li","Yongle Li","Zijun Chen","Chongming Gao","Junqi Zhang","Yangyang Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.13443v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.09549v2","updated":"2024-10-02T11:44:26Z","published":"2024-06-13T19:30:32Z","title":"Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective","summary":"  Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10341v2","updated":"2024-10-02T11:38:51Z","published":"2024-09-16T14:56:59Z","title":"Detecting Sexism in German Online Newspaper Comments with Open-Source\n  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks\n  1 and 2, Closed Track)","summary":"  Sexism in online media comments is a pervasive challenge that often manifests\nsubtly, complicating moderation efforts as interpretations of what constitutes\nsexism can vary among individuals. We study monolingual and multilingual\nopen-source text embeddings to reliably detect sexism and misogyny in\nGerman-language online comments from an Austrian newspaper. We observed\nclassifiers trained on text embeddings to mimic closely the individual\njudgements of human annotators. Our method showed robust performance in the\nGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1\nscore of 0.597 (4th place, as reported on Codabench). It also accurately\npredicted the distribution of human annotations in GerMS-Detect Subtask 2, with\nan average Jensen-Shannon distance of 0.301 (2nd place). The computational\nefficiency of our approach suggests potential for scalable applications across\nvarious languages and linguistic contexts.\n","authors":["Florian Bremm","Patrick Gustav Blaneck","Tobias Bornheim","Niklas Grieger","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2409.10341v2.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.01434v1","updated":"2024-10-02T11:36:45Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions via\nsubnetworks that can be composed to perform more complex tasks. Recent\ndevelopments in mechanistic interpretability have made progress in identifying\nsubnetworks, often referred to as circuits, which represent the minimal\ncomputational subgraph responsible for a model's behavior on specific tasks.\nHowever, most studies focus on identifying circuits for individual tasks\nwithout investigating how functionally similar circuits relate to each other.\nTo address this gap, we examine the modularity of neural networks by analyzing\ncircuits for highly compositional subtasks within a transformer-based language\nmodel. Specifically, given a probabilistic context-free grammar, we identify\nand compare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through subnetwork set operations to\nrepresent more complex functional capabilities of the model.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v1.pdf","comment":"24 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02076v5","updated":"2024-10-02T11:29:18Z","published":"2024-09-03T17:25:54Z","title":"LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs","summary":"  In evaluating the long-context capabilities of large language models (LLMs),\nbenchmarks such as \"Needle-in-a-Haystack\" (NIAH), Ruler, and Needlebench are\ncommonly used. While these benchmarks measure how well models understand\nlong-context input sequences, they do not effectively gauge the quality of\nlong-form text generation--a critical aspect for applications such as design\nproposals and creative writing. To address this gap, we have introduced a new\nlong-form text evaluation benchmark, LongGenBench, which tests models' ability\nto identify specific events within generated long text sequences. In this\nbenchmark, we prompt long-context LMs to create long-form text that must\ninclude particular events or constraints and evaluate their ability to\nincorporate these elements. We evaluated ten long-context LMs across four\ndistinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenBench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.\n","authors":["Yuhao Wu","Ming Shan Hee","Zhiqing Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2409.02076v5.pdf","comment":"work in progress; Github: https://github.com/mozhu621/LongGenBench/"},{"id":"http://arxiv.org/abs/2410.01428v1","updated":"2024-10-02T11:26:02Z","published":"2024-10-02T11:26:02Z","title":"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with\n  Retrieval-Augmentation for Solving Challenging Tasks","summary":"  State-of-the-art large language models (LLMs) exhibit impressive\nproblem-solving capabilities but may struggle with complex reasoning and\nfactual correctness. Existing methods harness the strengths of chain-of-thought\nand retrieval-augmented generation (RAG) to decompose a complex problem into\nsimpler steps and apply retrieval to improve factual correctness. These methods\nwork well on straightforward reasoning tasks but often falter on challenging\ntasks such as competitive programming and mathematics, due to frequent\nreasoning errors and irrelevant knowledge retrieval. To address this, we\nintroduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a\nnovel framework that leverages fine-tuned critic models to guide both reasoning\nand retrieval processes through planning. CR-Planner solves a problem by\niteratively selecting and executing sub-goals. Initially, it identifies the\nmost promising sub-goal from reasoning, query generation, and retrieval, guided\nby rewards given by a critic model named sub-goal critic. It then executes this\nsub-goal through sampling and selecting the optimal output based on evaluations\nfrom another critic model named execution critic. This iterative process,\ninformed by retrieved information and critic models, enables CR-Planner to\neffectively navigate the solution space towards the final answer. We employ\nMonte Carlo Tree Search to collect the data for training the critic models,\nallowing for a systematic exploration of action sequences and their long-term\nimpacts. We validate CR-Planner on challenging domain-knowledge-intensive and\nreasoning-heavy tasks, including competitive programming, theorem-driven math\nreasoning, and complex domain retrieval problems. Our experiments demonstrate\nthat CR-Planner significantly outperforms baselines, highlighting its\neffectiveness in addressing challenging problems by improving both reasoning\nand retrieval.\n","authors":["Xingxuan Li","Weiwen Xu","Ruochen Zhao","Fangkai Jiao","Shafiq Joty","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.01428v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.18618v3","updated":"2024-10-02T11:08:29Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.11239v2","updated":"2024-10-02T11:07:14Z","published":"2024-09-17T14:40:02Z","title":"LLM-as-a-Judge & Reward Model: What They Can and Cannot Do","summary":"  LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\nin diverse contexts, such as non-English prompts, factual verification, or\nchallenging questions, remains unexplored. In this paper, we conduct a\ncomprehensive analysis of automated evaluators, reporting several key findings\non their behavior. First, we discover that English evaluation capabilities\nsignificantly influence language-specific evaluation capabilities, often more\nthan the language proficiency itself, enabling evaluators trained in English to\neasily transfer their skills to other languages. Second, we identify critical\nshortcomings, where LLMs fail to detect and penalize errors, such as factual\ninaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we find that state-of-the-art evaluators struggle with\nchallenging prompts, in either English or Korean, underscoring their\nlimitations in assessing or generating complex reasoning questions. We release\nthe dataset and codes used.\n","authors":["Guijin Son","Hyunwoo Ko","Hoyoung Lee","Yewon Kim","Seunghyeok Hong"],"pdf_url":"https://arxiv.org/pdf/2409.11239v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2406.14760v2","updated":"2024-10-02T11:03:16Z","published":"2024-06-20T22:10:52Z","title":"An LLM Feature-based Framework for Dialogue Constructiveness Assessment","summary":"  Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.\n","authors":["Lexin Zhou","Youmna Farag","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.14760v2.pdf","comment":"Paper accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01417v1","updated":"2024-10-02T10:58:54Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09177v2","updated":"2024-10-02T10:43:07Z","published":"2024-02-14T13:45:19Z","title":"Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks","summary":"  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.\n","authors":["Yixin Cheng","Markos Georgopoulos","Volkan Cevher","Grigorios G. Chrysos"],"pdf_url":"https://arxiv.org/pdf/2402.09177v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2409.17171v2","updated":"2024-10-02T10:28:02Z","published":"2024-09-19T21:45:13Z","title":"Cross-Domain Content Generation with Domain-Specific Small Language\n  Models","summary":"  Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.\n","authors":["Ankit Maloo","Abhinav Garg"],"pdf_url":"https://arxiv.org/pdf/2409.17171v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.01401v1","updated":"2024-10-02T10:27:07Z","published":"2024-10-02T10:27:07Z","title":"Question-guided Knowledge Graph Re-scoring and Injection for Knowledge\n  Graph Question Answering","summary":"  Knowledge graph question answering (KGQA) involves answering natural language\nquestions by leveraging structured information stored in a knowledge graph.\nTypically, KGQA initially retrieve a targeted subgraph from a large-scale\nknowledge graph, which serves as the basis for reasoning models to address\nqueries. However, the retrieved subgraph inevitably brings distraction\ninformation for knowledge utilization, impeding the model's ability to perform\naccurate reasoning. To address this issue, we propose a Question-guided\nKnowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the\ninput question, thereby focusing specifically on pertinent factual knowledge.\nMoreover, we introduce Knowformer, a parameter-efficient method for injecting\nthe re-scored knowledge graph into large language models to enhance their\nability to perform factual reasoning. Extensive experiments on multiple KGQA\nbenchmarks demonstrate the superiority of our method over existing systems.\n","authors":["Yu Zhang","Kehai Chen","Xuefeng Bai","zhao kang","Quanjiang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01401v1.pdf","comment":"findings of EMNLP2024"},{"id":"http://arxiv.org/abs/2410.01400v1","updated":"2024-10-02T10:24:51Z","published":"2024-10-02T10:24:51Z","title":"CrowdCounter: A benchmark type-specific multi-target counterspeech\n  dataset","summary":"  Counterspeech presents a viable alternative to banning or suspending users\nfor hate speech while upholding freedom of expression. However, writing\neffective counterspeech is challenging for moderators/users. Hence, developing\nsuggestion tools for writing counterspeech is the need of the hour. One\ncritical challenge in developing such a tool is the lack of quality and\ndiversity of the responses in the existing datasets. Hence, we introduce a new\ndataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs\nspanning six different counterspeech types (empathy, humor, questioning,\nwarning, shaming, contradiction), which is the first of its kind. The design of\nour annotation platform itself encourages annotators to write type-specific,\nnon-redundant and high-quality counterspeech. We evaluate two frameworks for\ngenerating counterspeech responses - vanilla and type-controlled prompts -\nacross four large language models. In terms of metrics, we evaluate the\nresponses using relevance, diversity and quality. We observe that Flan-T5 is\nthe best model in the vanilla framework across different models. Type-specific\nprompts enhance the relevance of the responses, although they might reduce the\nlanguage quality. DialoGPT proves to be the best at following the instructions\nand generating the type-specific counterspeech accurately.\n","authors":["Punyajoy Saha","Abhilash Datta","Abhik Jana","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.01400v1.pdf","comment":"19 pages, 1 figure, 14 tables, Code available\n  https://github.com/hate-alert/CrowdCounter"},{"id":"http://arxiv.org/abs/2410.01383v1","updated":"2024-10-02T09:51:42Z","published":"2024-10-02T09:51:42Z","title":"PairDistill: Pairwise Relevance Distillation for Dense Retrieval","summary":"  Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01383v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.01380v1","updated":"2024-10-02T09:49:45Z","published":"2024-10-02T09:49:45Z","title":"Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition","summary":"  In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.\n","authors":["Jiyeon Kim","Hyunji Lee","Hyowon Cho","Joel Jang","Hyeonbin Hwang","Seungpil Won","Youbin Ahn","Dohaeng Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.01380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18511v2","updated":"2024-10-02T09:42:17Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v2.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2410.01363v1","updated":"2024-10-02T09:23:07Z","published":"2024-10-02T09:23:07Z","title":"PCQPR: Proactive Conversational Question Planning with Reflection","summary":"  Conversational Question Generation (CQG) enhances the interactivity of\nconversational question-answering systems in fields such as education, customer\nservice, and entertainment. However, traditional CQG, focusing primarily on the\nimmediate context, lacks the conversational foresight necessary to guide\nconversations toward specified conclusions. This limitation significantly\nrestricts their ability to achieve conclusion-oriented conversational outcomes.\nIn this work, we redefine the CQG task as Conclusion-driven Conversational\nQuestion Generation (CCQG) by focusing on proactivity, not merely reacting to\nthe unfolding conversation but actively steering it towards a\nconclusion-oriented question-answer pair. To address this, we propose a novel\napproach, called Proactive Conversational Question Planning with self-Refining\n(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte\nCarlo Tree Search (MCTS) with the analytical capabilities of large language\nmodels (LLMs), PCQPR predicts future conversation turns and continuously\nrefines its questioning strategies. This iterative self-refining mechanism\nensures the generation of contextually relevant questions strategically devised\nto reach a specified outcome. Our extensive evaluations demonstrate that PCQPR\nsignificantly surpasses existing CQG methods, marking a paradigm shift towards\nconclusion-oriented conversational question-answering systems.\n","authors":["Shasha Guo","Lizi Liao","Jing Zhang","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01363v1.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2409.00997v2","updated":"2024-10-02T09:18:55Z","published":"2024-09-02T07:23:13Z","title":"DataSculpt: Crafting Data Landscapes for Long-Context LLMs through\n  Multi-Objective Partitioning","summary":"  In recent years, Large Language Models (LLMs) have demonstrated significant\nimprovements across a variety of tasks, one of which is the long-context\ncapability. The key to improving long-context performance lies in effective\ndata organization and management strategies that integrate data from multiple\ndomains and optimize the context window during training. Through extensive\nexperimental analysis, we identified three key challenges in designing\neffective data management strategies that enable the model to achieve\nlong-context capability without sacrificing performance in other tasks: (1) a\nshortage of long documents across multiple domains, (2) effective construction\nof context windows, and (3) efficient organization of large-scale datasets. To\naddress these challenges, we introduce DataSculpt, a novel data management\nframework designed for long-context training. We first formulate the\norganization of training data as a multi-objective combinatorial optimization\nproblem, focusing on attributes including relevance, homogeneity, integrity,\nand efficiency. Specifically, our approach utilizes a coarse-to-fine\nmethodology to optimize training data organization both efficiently and\neffectively. We begin by clustering the data based on semantic similarity\n(coarse), followed by a multi-objective greedy search within each cluster to\nscore and concatenate documents into various context windows (fine). Our\ncomprehensive evaluations demonstrate that DataSculpt significantly enhances\nlong-context training performance, resulting in improvements of 18.09% in\nretrieval augmentation, 21.23% in summarization, 21.27% in reading\ncomprehension, and a 3.81% increase in code completion, while also maintaining\noverall model proficiency with a 4.88% improvement.\n","authors":["Keer Lu","Xiaonan Nie","Zheng Liang","Da Pan","Shusen Zhang","Keshi Zhao","Weipeng Chen","Zenan Zhou","Guosheng Dong","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.00997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10770v4","updated":"2024-10-02T09:18:47Z","published":"2024-02-16T15:48:33Z","title":"How Reliable Are Automatic Evaluation Methods for Instruction-Tuned\n  LLMs?","summary":"  Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.\n","authors":["Ehsan Doostmohammadi","Oskar Holmström","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2402.10770v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01356v1","updated":"2024-10-02T09:14:39Z","published":"2024-10-02T09:14:39Z","title":"Assisted Data Annotation for Business Process Information Extraction\n  from Textual Documents","summary":"  Machine-learning based generation of process models from natural language\ntext process descriptions provides a solution for the time-intensive and\nexpensive process discovery phase. Many organizations have to carry out this\nphase, before they can utilize business process management and its benefits.\nYet, research towards this is severely restrained by an apparent lack of large\nand high-quality datasets. This lack of data can be attributed to, among other\nthings, an absence of proper tool assistance for dataset creation, resulting in\nhigh workloads and inferior data quality. We explore two assistance features to\nsupport dataset creation, a recommendation system for identifying process\ninformation in the text and visualization of the current state of already\nidentified process information as a graphical business process model. A\ncontrolled user study with 31 participants shows that assisting dataset\ncreators with recommendations lowers all aspects of workload, up to $-51.0\\%$,\nand significantly improves annotation quality, up to $+38.9\\%$. We make all\ndata and code available to encourage further research on additional novel\nassistance strategies.\n","authors":["Julian Neuberger","Han van der Aa","Lars Ackermann","Daniel Buschek","Jannic Herrmann","Stefan Jablonski"],"pdf_url":"https://arxiv.org/pdf/2410.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Défossez","Laurent Mazaré","Manu Orsini","Amélie Royer","Patrick Pérez","Hervé Jégou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11184v3","updated":"2024-10-02T09:07:15Z","published":"2024-04-17T09:01:02Z","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document","summary":"  Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems.\n","authors":["Joonho Yang","Seunghyun Yoon","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11184v3.pdf","comment":"Published as a main conference paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01335v1","updated":"2024-10-02T08:53:07Z","published":"2024-10-02T08:53:07Z","title":"Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models","summary":"  Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.\n","authors":["Lucas Bandarkar","Benjamin Muller","Pritish Yuvraj","Rui Hou","Nayan Singhal","Hongjiang Lv","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01335v1.pdf","comment":"11 main pages, 23 pages total, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.01334v1","updated":"2024-10-02T08:52:58Z","published":"2024-10-02T08:52:58Z","title":"Unveiling Language Skills under Circuits","summary":"  The exploration of language skills in language models (LMs) has always been\none of the central goals in mechanistic interpretability. However, existing\ncircuit analyses often fall short in representing the full functional scope of\nthese models, primarily due to the exclusion of Feed-Forward layers.\nAdditionally, isolating the effect of a single language skill from a text,\nwhich inherently involves multiple entangled skills, poses a significant\nchallenge. To address these gaps, we introduce a novel concept, Memory Circuit,\na minimum unit that fully and independently manipulates the memory-reading\nfunctionality of a language model, and disentangle the transformer model\nprecisely into a circuit graph which is an ensemble of paths connecting\ndifferent memory circuits. Based on this disentanglement, we identify salient\ncircuit paths, named as skill paths, responsible for three crucial language\nskills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning\n(ICL) Skill, leveraging causal effect estimation through interventions and\ncounterfactuals. Our experiments on various datasets confirm the correspondence\nbetween our identified skill paths and language skills, and validate three\nlongstanding hypotheses: 1) Language skills are identifiable through circuit\ndissection; 2) Simple language skills reside in shallow layers, whereas complex\nlanguage skills are found in deeper layers; 3) Complex language skills are\nformed on top of simpler language skills. Our codes are available at:\nhttps://github.com/Zodiark-ch/Language-Skill-of-LLMs.\n","authors":["Hang Chen","Jiaying Zhu","Xinyu Yang","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13979v3","updated":"2024-10-02T08:51:45Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03199v2","updated":"2024-10-02T08:45:32Z","published":"2024-05-24T13:33:11Z","title":"Bayesian WeakS-to-Strong from Text Classification to Generation","summary":"  Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.\n","authors":["Ziyun Cui","Ziyang Zhang","Wen Wu","Guangzhi Sun","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18164v2","updated":"2024-10-02T08:40:36Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.13550v2","updated":"2024-10-02T08:32:31Z","published":"2024-02-21T06:11:03Z","title":"Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues","summary":"  A successful negotiation requires a range of capabilities, including\ncomprehension of the conversation context, Theory-of-Mind (ToM) skills to infer\nthe partner's motives, strategic reasoning, and effective communication, making\nit challenging for automated systems. Despite the remarkable performance of\nLLMs in various NLP tasks, there is no systematic evaluation of their\ncapabilities in negotiation. Such an evaluation is critical for advancing AI\nnegotiation agents and negotiation research, ranging from designing dialogue\nsystems to providing pedagogical feedback and scaling up data collection\npractices. This work aims to systematically analyze the multifaceted\ncapabilities of LLMs across diverse dialogue scenarios throughout the stages of\na typical negotiation interaction. Our analysis highlights GPT-4's superior\nperformance in many tasks while identifying specific challenges, such as making\nsubjective assessments and generating contextually appropriate, strategically\nadvantageous responses.\n","authors":["Deuksin Kwon","Emily Weiss","Tara Kulshrestha","Kushal Chawla","Gale M. Lucas","Jonathan Gratch"],"pdf_url":"https://arxiv.org/pdf/2402.13550v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.20135v3","updated":"2024-10-02T08:32:02Z","published":"2024-09-30T09:34:31Z","title":"Federated Instruction Tuning of LLMs with Domain Coverage Augmentation","summary":"  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges.\n","authors":["Zezhou Wang","Yaxin Du","Zhuzhong Qian","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20135v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18256v2","updated":"2024-10-02T08:28:36Z","published":"2024-06-26T11:08:17Z","title":"Llamipa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.00727v2","updated":"2024-10-02T08:08:45Z","published":"2024-10-01T14:16:10Z","title":"Show Me What's Wrong!: Combining Charts and Text to Guide Data Analysis","summary":"  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome\nbut vital task across different domains. In the context of financial fraud\ndetection, analysts must quickly identify suspicious activity among\ntransactional data. This is an iterative process made of complex exploratory\ntasks such as recognizing patterns, grouping, and comparing. To mitigate the\ninformation overload inherent to these steps, we present a tool combining\nautomated information highlights, Large Language Model generated textual\ninsights, and visual analytics, facilitating exploration at different levels of\ndetail. We perform a segmentation of the data per analysis area and visually\nrepresent each one, making use of automated visual cues to signal which require\nmore attention. Upon user selection of an area, our system provides textual and\ngraphical summaries. The text, acting as a link between the high-level and\ndetailed views of the chosen segment, allows for a quick understanding of\nrelevant details. A thorough exploration of the data comprising the selection\ncan be done through graphical representations. The feedback gathered in a study\nperformed with seven domain experts suggests our tool effectively supports and\nguides exploratory analysis, easing the identification of suspicious\ninformation.\n","authors":["Beatriz Feliciano","Rita Costa","Jean Alves","Javier Liébana","Diogo Duarte","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2410.00727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01306v1","updated":"2024-10-02T08:01:05Z","published":"2024-10-02T08:01:05Z","title":"Emotion-Aware Response Generation Using Affect-Enriched Embeddings with\n  LLMs","summary":"  There is a need for empathetic and coherent responses in automated\nchatbot-facilitated psychotherapy sessions. This study addresses the challenge\nof enhancing the emotional and contextual understanding of large language\nmodels (LLMs) in psychiatric applications. We introduce a novel framework that\nintegrates multiple emotion lexicons, including NRC Emotion Lexicon, VADER,\nWordNet, and SentiWordNet, with state-of-the-art LLMs such as LLAMA 2, Flan-T5,\nChatGPT 3.0, and ChatGPT 4.0. The primary dataset comprises over 2,000 therapy\nsession transcripts from the Counseling and Psychotherapy database, covering\ndiscussions on anxiety, depression, trauma, and addiction. We segment the\ntranscripts into smaller chunks, enhancing them with lexical features and\ncomputing embeddings using BERT, GPT-3, and RoBERTa to capture semantic and\nemotional nuances. These embeddings are stored in a FAISS vector database,\nenabling efficient similarity search and clustering based on cosine similarity.\nUpon user query, the most relevant segments are retrieved and provided as\ncontext to the LLMs, significantly improving the models' ability to generate\nempathetic and contextually appropriate responses. Experimental evaluations\ndemonstrate that in-corporating emotion lexicons enhances empathy, coherence,\ninformativeness, and fluency scores. Our findings highlight the critical role\nof emotional embeddings in improving LLM performance for psychotherapy.\n","authors":["Abdur Rasool","Muhammad Irfan Shahzad","Hafsa Aslam","Vincent Chan"],"pdf_url":"https://arxiv.org/pdf/2410.01306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04927v3","updated":"2024-10-02T07:58:56Z","published":"2024-09-07T22:54:47Z","title":"Just ASR + LLM? A Study on Speech Large Language Models' Ability to\n  Identify and Understand Speaker in Spoken Dialogue","summary":"  In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. SpeechLLMs have demonstrated impressive spoken dialog\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ntranscript alone, i.e.\\ without speaker segmentation and identification. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered reliably with correct speaker identification. The results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that tasks focused on\nidentity-critical questions could offer a more accurate evaluation framework of\nSpeechLLMs in SQA.\n","authors":["Junkai Wu","Xulin Fan","Bo-Ru Lu","Xilin Jiang","Nima Mesgarani","Mark Hasegawa-Johnson","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2409.04927v3.pdf","comment":"Accepted to IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2410.01305v1","updated":"2024-10-02T07:57:33Z","published":"2024-10-02T07:57:33Z","title":"Revisiting Hierarchical Text Classification: Inference and Metrics","summary":"  Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}.\n","authors":["Roman Plaud","Matthieu Labeau","Antoine Saillenfest","Thomas Bonald"],"pdf_url":"https://arxiv.org/pdf/2410.01305v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2409.16911v2","updated":"2024-10-02T07:52:56Z","published":"2024-09-25T13:15:50Z","title":"Pruning Multilingual Large Language Models for Multilingual Inference","summary":"  Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.\n","authors":["Hwichan Kim","Jun Suzuki","Tosho Hirasawa","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2409.16911v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.04507v2","updated":"2024-10-02T07:52:37Z","published":"2024-09-06T16:32:46Z","title":"3D Data Long-Term Preservation in Cultural Heritage","summary":"  The report explores the challenges and strategies for preserving 3D digital\ndata in cultural heritage. It discusses the issue of technological\nobsolescence, emphasising the need for ustainable storage solutions and ongoing\ndata management strategies. Key topics include understanding technological\nobsolescence, the lifecycle of digital content, digital continuity, data\nmanagement plans (DMP), FAIR principles, and the use of public repositories.\nThe report also covers the importance of metadata in long-term digital\npreservation, including types of metadata and strategies for building valuable\nmetadata. It examines the evolving standards and interoperability in 3D format\npreservation and the importance of managing metadata and paradata. The document\nprovides a comprehensive overview of the challenges and solutions for\npreserving 3D cultural heritage data in the long term.\n","authors":["Nicola Amico","Achille Felicetti"],"pdf_url":"https://arxiv.org/pdf/2409.04507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16508v3","updated":"2024-10-02T07:51:47Z","published":"2024-02-26T11:42:29Z","title":"Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision","summary":"  Cross-lingual open domain question answering (CLQA) is a complex problem,\ncomprising cross-lingual retrieval from a multilingual knowledge base, followed\nby answer generation in the query language. Both steps are usually tackled by\nseparate models, requiring substantial annotated datasets, and typically\nauxiliary resources, like machine translation systems to bridge between\nlanguages. In this paper, we show that CLQA can be addressed using a single\nencoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural questions to supervise answer\ngeneration. Together, we show our approach, \\texttt{CLASS}, outperforms\ncomparable methods on both supervised and zero-shot language adaptation\nsettings, including those using machine translation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2402.16508v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01294v1","updated":"2024-10-02T07:40:56Z","published":"2024-10-02T07:40:56Z","title":"Endless Jailbreaks with Bijection Learning","summary":"  Despite extensive safety training, LLMs are vulnerable to adversarial inputs.\nIn this work, we introduce a simple but powerful attack paradigm, bijection\nlearning, that yields a practically endless set of jailbreak prompts. We\nexploit language models' advanced reasoning capabilities to teach them\ninvertible languages (bijections) in context, pass encoded queries to the model\nto bypass built-in safety mechanisms, and finally decode responses back into\nEnglish, yielding helpful replies to harmful requests. Our approach proves\neffective on a wide range of frontier language models and harm categories.\nBijection learning is an automated and universal attack that grows stronger\nwith scale: larger models with more advanced reasoning capabilities are more\nsusceptible to bijection learning jailbreaks despite stronger safety\nmechanisms.\n","authors":["Brian R. Y. Huang","Maximilian Li","Leonard Tang"],"pdf_url":"https://arxiv.org/pdf/2410.01294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15371v3","updated":"2024-10-02T07:38:02Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08700v3","updated":"2024-10-02T07:26:40Z","published":"2024-04-10T18:08:59Z","title":"DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs","summary":"  LLMs acquire knowledge from massive data snapshots collected at different\ntimestamps. Their knowledge is then commonly evaluated using static benchmarks.\nHowever, factual knowledge is generally subject to time-sensitive changes, and\nstatic benchmarks cannot address those cases. We present an approach to\ndynamically evaluate the knowledge in LLMs and their time-sensitiveness against\nWikidata, a publicly available up-to-date knowledge graph. We evaluate the\ntime-sensitive knowledge in twenty-four private and open-source LLMs, as well\nas the effectiveness of four editing methods in updating the outdated facts.\nOur results show that 1) outdatedness is a critical problem across\nstate-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with\nslight variations of the question prompt; and 3) the performance of the\nstate-of-the-art knowledge editing algorithms is very limited, as they can not\nreduce the cases of outdatedness and output inconsistency.\n","authors":["Seyed Mahed Mousavi","Simone Alghisi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2404.08700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01288v1","updated":"2024-10-02T07:18:16Z","published":"2024-10-02T07:18:16Z","title":"Mitigating Copy Bias in In-Context Learning through Neuron Pruning","summary":"  Large language models (LLMs) have demonstrated impressive few-shot in-context\nlearning (ICL) abilities. Still, we show that they are sometimes prone to a\n`copying bias', where they copy answers from provided examples instead of\nlearning the underlying patterns. In this work, we propose a novel and simple\nmethod to mitigate such copying bias. First, we create a synthetic task and use\nthe Integrated Gradients method to identify neurons that prioritize copying\nover generalization. We demonstrate that pruning these neurons consistently\nimproves performance across a diverse set of ICL tasks. We also show that our\nmethod is applicable across various LLM architectures, including Transformers\nand State-Space Models, without requiring modifications. In our analysis, we\nadopt a task-recognition perspective on ICL and examine task vectors (Hendel et\nal., 2023) induced by the model. We find that pruning enhances the quality of\nthese vectors, suggesting that the pruned neurons previously hindered effective\ntask recognition.\n","authors":["Ameen Ali","Lior Wolf","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2410.01288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01285v1","updated":"2024-10-02T07:14:26Z","published":"2024-10-02T07:14:26Z","title":"Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration","summary":"  The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.\n","authors":["Kangxi Wu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01285v1.pdf","comment":"Accepted to the EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2305.03977v3","updated":"2024-10-02T06:35:36Z","published":"2023-05-06T08:43:33Z","title":"Unlocking the Power of GANs in Non-Autoregressive Text Generation","summary":"  Generative Adversarial Networks (GANs) have been studied in text generation\nto tackle the exposure bias problem. Despite their remarkable development, they\nadopt autoregressive structures so suffering from high latency in both training\nand inference stages. Although GANs have potential to support efficient\ngeneration by adopting non-autoregressive (NAR) structures, their explorations\nin NAR models are extremely limited. In this work, we conduct pioneering study\nof building language GANs based on NAR structures. We identify two issues that\nconstrain the performance of GAN-based NAR models. Firstly, existing methods of\nincorporating latent variables provide highly similar representations which\ncannot describe the diversity of different words in sentences. We tackle this\nproblem by proposing Position-Aware Self-Modulation, providing more diverse and\neffective representations. Secondly, the attention mechanism in Transformer\ncannot accurately build word dependencies in the unstable training of GANs, and\nwe adopt Dependency Feed Forward Network to enhance the model capacity in\ndependency modeling. Armed with these two facilities, we propose a GAN-based\nNAR model, Adversarial Non-autoregressive Transformer (ANT). The experimental\nresults demonstrate that ANT can achieve comparable performance with mainstream\nmodels in a single forward pass and has great potential in various applications\nlike latent interpolation and semi-supervised learning.\n","authors":["Da Ren","Yi Cai","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2305.03977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02118v2","updated":"2024-10-02T06:32:10Z","published":"2024-07-02T10:06:41Z","title":"Breaking Language Barriers: Cross-Lingual Continual Pre-Training at\n  Scale","summary":"  In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.\n","authors":["Wenzhen Zheng","Wenbo Pan","Xu Xu","Libo Qin","Li Yue","Ming Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.02118v2.pdf","comment":"8 pages. Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01268v1","updated":"2024-10-02T06:24:51Z","published":"2024-10-02T06:24:51Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications","summary":"  This book serves as an introduction to deep learning and machine learning,\nfocusing on their applications in big data analytics. It covers essential\nconcepts, tools like ChatGPT and Claude, hardware recommendations, and\npractical guidance on setting up development environments using libraries like\nPyTorch and TensorFlow. Designed for beginners and advanced users alike, it\nprovides step-by-step instructions, hands-on projects, and insights into AI's\nfuture, including AutoML and edge computing.\n","authors":["Pohsun Feng","Ziqian Bi","Yizhu Wen","Xuanhe Pan","Benji Peng","Ming Liu","Jiawei Xu","Keyu Chen","Junyu Liu","Caitlyn Heqi Yin","Sen Zhang","Jinlang Wang","Qian Niu","Ming Li","Tianyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01268v1.pdf","comment":"This book contains 156 pages and 9 figures"},{"id":"http://arxiv.org/abs/2407.02834v3","updated":"2024-10-02T06:18:02Z","published":"2024-07-03T06:21:07Z","title":"Aspect-Based Sentiment Analysis Techniques: A Comparative Study","summary":"  Since the dawn of the digitalisation era, customer feedback and online\nreviews are unequivocally major sources of insights for businesses.\nConsequently, conducting comparative analyses of such sources has become the de\nfacto modus operandi of any business that wishes to give itself a competitive\nedge over its peers and improve customer loyalty. Sentiment analysis is one\nsuch method instrumental in gauging public interest, exposing market trends,\nand analysing competitors. While traditional sentiment analysis focuses on\noverall sentiment, as the needs advance with time, it has become important to\nexplore public opinions and sentiments on various specific subjects, products\nand services mentioned in the reviews on a finer-granular level. To this end,\nAspect-based Sentiment Analysis (ABSA), supported by advances in Artificial\nIntelligence (AI) techniques which have contributed to a paradigm shift from\nsimple word-level analysis to tone and context-aware analyses, focuses on\nidentifying specific aspects within the text and determining the sentiment\nassociated with each aspect. In this study, we compare several deep-NN methods\nfor ABSA on two benchmark datasets (Restaurant14 and Laptop-14) and found that\nFAST LSA obtains the best overall results of 87.6% and 82.6% accuracy but does\nnot pass LSA+DeBERTa which reports 90.33% and 86.21% accuracy respectively.\n","authors":["Dineth Jayakody","Koshila Isuranda","A V A Malkith","Nisansa de Silva","Sachintha Rajith Ponnamperuma","G G N Sandamali","K L K Sudheera"],"pdf_url":"https://arxiv.org/pdf/2407.02834v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13621v2","updated":"2024-10-02T06:14:17Z","published":"2024-09-20T16:32:54Z","title":"Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network","summary":"  Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.\n","authors":["Haoran Li","Qiang Gao","Hongmei Wu","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.13621v2.pdf","comment":"EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI"},{"id":"http://arxiv.org/abs/2410.01257v1","updated":"2024-10-02T06:05:52Z","published":"2024-10-02T06:05:52Z","title":"HelpSteer2-Preference: Complementing Ratings with Preferences","summary":"  Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We\nalso demonstrate the effectiveness of this reward model at aligning models to\nfollow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the\ntrained Reward Model at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward\n","authors":["Zhilin Wang","Alexander Bukharin","Olivier Delalleau","Daniel Egert","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2410.01257v1.pdf","comment":"26 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.03868v2","updated":"2024-10-02T05:51:53Z","published":"2024-04-05T02:53:51Z","title":"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge\n  Graph Construction","summary":"  In this work, we are interested in automated methods for knowledge graph\ncreation (KGC) from input text. Progress on large language models (LLMs) has\nprompted a series of recent works applying them to KGC, e.g., via zero/few-shot\nprompting. Despite successes on small domain-specific datasets, these models\nface difficulties scaling up to text common in many real-world applications. A\nprincipal issue is that, in prior methods, the KG schema has to be included in\nthe LLM prompt to generate valid triplets; larger and more complex schemas\neasily exceed the LLMs' context window length. Furthermore, there are scenarios\nwhere a fixed pre-defined schema is not available and we would like the method\nto construct a high-quality KG with a succinct self-generated schema. To\naddress these problems, we propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by\nschema definition and post-hoc canonicalization. EDC is flexible in that it can\nbe applied to settings where a pre-defined target schema is available and when\nit is not; in the latter case, it constructs a schema automatically and applies\nself-canonicalization. To further improve performance, we introduce a trained\ncomponent that retrieves schema elements relevant to the input text; this\nimproves the LLMs' extraction performance in a retrieval-augmented\ngeneration-like manner. We demonstrate on three KGC benchmarks that EDC is able\nto extract high-quality triplets without any parameter tuning and with\nsignificantly larger schemas compared to prior works. Code for EDC is available\nat https://github.com/clear-nus/edc.\n","authors":["Bowen Zhang","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2404.03868v2.pdf","comment":"18 pages, 3 figures, Proceedings of the 2024 Conference on Empirical\n  Methods in Natural Language Processing"},{"id":"http://arxiv.org/abs/2410.01246v1","updated":"2024-10-02T05:22:07Z","published":"2024-10-02T05:22:07Z","title":"AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended\n  Responses","summary":"  Question answering (QA) tasks have been extensively studied in the field of\nnatural language processing (NLP). Answers to open-ended questions are highly\ndiverse and difficult to quantify, and cannot be simply evaluated as correct or\nincorrect, unlike close-ended questions with definitive answers. While large\nlanguage models (LLMs) have demonstrated strong capabilities across various\ntasks, they exhibit relatively weaker performance in evaluating answers to\nopen-ended questions. In this study, we propose a method that leverages LLMs\nand the analytic hierarchy process (AHP) to assess answers to open-ended\nquestions. We utilized LLMs to generate multiple evaluation criteria for a\nquestion. Subsequently, answers were subjected to pairwise comparisons under\neach criterion with LLMs, and scores for each answer were calculated in the\nAHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and\nGPT-4. Our results indicate that our approach more closely aligns with human\njudgment compared to the four baselines. Additionally, we explored the impact\nof the number of criteria, variations in models, and differences in datasets on\nthe results.\n","authors":["Xiaotian Lu","Jiyi Li","Koh Takeuchi","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2410.01246v1.pdf","comment":"Accepted for EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.01242v1","updated":"2024-10-02T05:07:02Z","published":"2024-10-02T05:07:02Z","title":"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance","summary":"  Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.\n","authors":["Haolin Jin","Zechao Sun","Yiheng Yang","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01240v1","updated":"2024-10-02T05:04:06Z","published":"2024-10-02T05:04:06Z","title":"Automatic deductive coding in discourse analysis: an application of\n  large language models in learning analytics","summary":"  Deductive coding is a common discourse analysis method widely used by\nlearning science and learning analytics researchers for understanding teaching\nand learning interactions. It often requires researchers to manually label all\ndiscourses to be analyzed according to a theoretically guided coding scheme,\nwhich is time-consuming and labor-intensive. The emergence of large language\nmodels such as GPT has opened a new avenue for automatic deductive coding to\novercome the limitations of traditional deductive coding. To evaluate the\nusefulness of large language models in automatic deductive coding, we employed\nthree different classification methods driven by different artificial\nintelligence technologies, including the traditional text classification method\nwith text feature engineering, BERT-like pretrained language model and GPT-like\npretrained large language model (LLM). We applied these methods to two\ndifferent datasets and explored the potential of GPT and prompt engineering in\nautomatic deductive coding. By analyzing and comparing the accuracy and Kappa\nvalues of these three classification methods, we found that GPT with prompt\nengineering outperformed the other two methods on both datasets with limited\nnumber of training samples. By providing detailed prompt structures, the\nreported work demonstrated how large language models can be used in the\nimplementation of automatic deductive coding.\n","authors":["Lishan Zhang","Han Wu","Xiaoshan Huang","Tengfei Duan","Hanxiang Du"],"pdf_url":"https://arxiv.org/pdf/2410.01240v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.05152v2","updated":"2024-10-02T05:02:02Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v2.pdf","comment":"EMNLP 2024 Findings; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.04081v3","updated":"2024-10-02T05:00:57Z","published":"2024-09-06T07:44:44Z","title":"UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity","summary":"  Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.\n","authors":["Yicheng Fu","Raviteja Anantha","Prabal Vashisht","Jianpeng Cheng","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2409.04081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01035v2","updated":"2024-10-02T04:20:31Z","published":"2024-09-02T08:10:51Z","title":"Unleashing the Power of Task-Specific Directions in Parameter Efficient\n  Fine-tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet requiring extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs)-critical for transitioning large\nmodels from pretrained states to task-specific enhancements in PEFT. We propose\na framework to clearly define these directions and explore their properties,\nand practical utilization challenges. We then introduce a novel approach,\nLoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning\nprocess, thereby enhancing model performance on targeted tasks. Extensive\nexperiments have conclusively demonstrated the effectiveness of LoRA-Dash, and\nin-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The\ncode is available at https://github.com/Chongjie-Si/Subspace-Tuning.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v2.pdf","comment":"Revisions ongoing. Codes in\n  https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2401.05967v3","updated":"2024-10-02T04:17:36Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v3.pdf","comment":"EMNLP2024 findings (Long)"},{"id":"http://arxiv.org/abs/2409.19541v3","updated":"2024-10-02T04:15:11Z","published":"2024-09-29T03:56:50Z","title":"Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance\n  Regularization","summary":"  Language models frequently inherit societal biases from their training data.\nNumerous techniques have been proposed to mitigate these biases during both the\npre-training and fine-tuning stages. However, fine-tuning a pre-trained\ndebiased language model on a downstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing methods for downstream tasks either (i)\nrequire labels of protected attributes (e.g., age, race, or political views)\nthat are often not available or (ii) rely on indicators of bias, which\nrestricts their applicability to gender debiasing since they rely on\ngender-specific words. To address this, we introduce a novel debiasing\nregularization technique based on the class-wise variance of embeddings.\nCrucially, our method does not require attribute labels and targets any\nattribute, thus addressing the shortcomings of existing debiasing methods. Our\nexperiments on encoder language models and three datasets demonstrate that our\nmethod outperforms existing strong debiasing baselines that rely on target\nattribute labels while maintaining performance on the target task.\n","authors":["Shahed Masoudian","Markus Frohmann","Navid Rekabsaz","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2409.19541v3.pdf","comment":"Accepted to EMNLP 2024"}]},"2024-10-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.02763v1","updated":"2024-10-03T17:59:58Z","published":"2024-10-03T17:59:58Z","title":"Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos","summary":"  There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02763v1.pdf","comment":"Project Page: https://vinoground.github.io"},{"id":"http://arxiv.org/abs/2404.10917v2","updated":"2024-10-03T17:59:55Z","published":"2024-04-16T21:33:05Z","title":"Which questions should I answer? Salience Prediction of Inquisitive\n  Questions","summary":"  Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.\n","authors":["Yating Wu","Ritika Mangla","Alexandros G. Dimakis","Greg Durrett","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2404.10917v2.pdf","comment":"Camera Ready for EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02760v1","updated":"2024-10-03T17:59:30Z","published":"2024-10-03T17:59:30Z","title":"Erasing Conceptual Knowledge from Language Models","summary":"  Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info\n","authors":["Rohit Gandikota","Sheridan Feucht","Samuel Marks","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.02760v1.pdf","comment":"Project Page: https://elm.baulab.info"},{"id":"http://arxiv.org/abs/2410.02756v1","updated":"2024-10-03T17:58:55Z","published":"2024-10-03T17:58:55Z","title":"CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text","summary":"  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on\nMultilingual Coreference Resolution. In this third iteration of the shared\ntask, a novel objective is to also predict empty nodes needed for zero\ncoreference mentions (while the empty nodes were given on input in previous\nyears). This way, coreference resolution can be performed on raw text. We\nevaluate two model variants: a~two-stage approach (where the empty nodes are\npredicted first using a pretrained encoder model and then processed together\nwith sentence words by another pretrained model) and a single-stage approach\n(where a single pretrained encoder model generates empty nodes, coreference\nmentions, and coreference links jointly). In both settings, CorPipe surpasses\nother participants by a large margin of 3.9 and 2.8 percent points,\nrespectively. The source code and the trained model are available at\nhttps://github.com/ufal/crac2024-corpipe .\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2410.02756v1.pdf","comment":"Accepted to CRAC 2024"},{"id":"http://arxiv.org/abs/2410.02755v1","updated":"2024-10-03T17:58:29Z","published":"2024-10-03T17:58:29Z","title":"SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost","summary":"  Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.\n","authors":["Jifan Zhang","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v1","updated":"2024-10-03T17:57:22Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18957v2","updated":"2024-10-03T17:57:07Z","published":"2024-09-27T17:58:50Z","title":"LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction","summary":"  Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2409.18957v2.pdf","comment":"Updated title, abstract, and images"},{"id":"http://arxiv.org/abs/2410.02748v1","updated":"2024-10-03T17:57:01Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11687v2","updated":"2024-10-03T17:56:34Z","published":"2024-06-17T16:05:32Z","title":"Tokenization Falling Short: The Curse of Tokenization","summary":"  Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.\n","authors":["Yekun Chai","Yewei Fang","Qiwei Peng","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2406.11687v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2404.07840v3","updated":"2024-10-03T17:56:12Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We make our code and data publicly available at\nhttps://github.com/ernie-research/gptfluence.\n","authors":["Yekun Chai","Qingyi Liu","Shuohuan Wang","Yu Sun","Qiwei Peng","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2307.10432v3","updated":"2024-10-03T17:55:29Z","published":"2023-07-19T19:40:34Z","title":"PharmacyGPT: The AI Pharmacist","summary":"  In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.\n","authors":["Zhengliang Liu","Zihao Wu","Mengxuan Hu","Bokai Zhao","Lin Zhao","Tianyi Zhang","Haixing Dai","Xianyan Chen","Ye Shen","Sheng Li","Quanzheng Li","Xiang Li","Brian Murray","Tianming Liu","Andrea Sikora"],"pdf_url":"https://arxiv.org/pdf/2307.10432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02744v1","updated":"2024-10-03T17:55:17Z","published":"2024-10-03T17:55:17Z","title":"Neutral residues: revisiting adapters for model extension","summary":"  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.\n","authors":["Franck Signe Talla","Herve Jegou","Edouard Grave"],"pdf_url":"https://arxiv.org/pdf/2410.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02743v1","updated":"2024-10-03T17:55:13Z","published":"2024-10-03T17:55:13Z","title":"MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions","summary":"  Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .\n","authors":["Yekun Chai","Haoran Sun","Huang Fang","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2410.02743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02742v1","updated":"2024-10-03T17:55:09Z","published":"2024-10-03T17:55:09Z","title":"Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models","summary":"  Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.\n","authors":["Haolan Liu","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.02742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02741v1","updated":"2024-10-03T17:54:56Z","published":"2024-10-03T17:54:56Z","title":"Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Lei Xu","Mohammed Asad Karim","Saket Dingliwal","Aparna Elangovan"],"pdf_url":"https://arxiv.org/pdf/2410.02741v1.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.02736v1","updated":"2024-10-03T17:53:30Z","published":"2024-10-03T17:53:30Z","title":"Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge","summary":"  LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.\n","authors":["Jiayi Ye","Yanbo Wang","Yue Huang","Dongping Chen","Qihui Zhang","Nuno Moniz","Tian Gao","Werner Geyer","Chao Huang","Pin-Yu Chen","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02730v1","updated":"2024-10-03T17:49:28Z","published":"2024-10-03T17:49:28Z","title":"DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes\n  and Objects","summary":"  Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent.\n","authors":["Zhaowei Wang","Hongming Zhang","Tianqing Fang","Ye Tian","Yue Yang","Kaixin Ma","Xiaoman Pan","Yangqiu Song","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02730v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.02729v1","updated":"2024-10-03T17:49:09Z","published":"2024-10-03T17:49:09Z","title":"Unified Multi-Modal Interleaved Document Representation for Information\n  Retrieval","summary":"  Information Retrieval (IR) methods aim to identify relevant documents in\nresponse to a given query, which have gained remarkable attention due to their\nsuccessful application in various natural language tasks. However, existing\napproaches typically consider only the textual information within the\ndocuments, which overlooks the fact that documents can contain multiple\nmodalities, including texts, images, and tables. Further, they often segment\neach long document into multiple discrete passages for embedding, preventing\nthem from capturing the overall document context and interactions between\nparagraphs. We argue that these two limitations lead to suboptimal document\nrepresentations for retrieval. In this work, to address them, we aim to produce\nmore comprehensive and nuanced document representations by holistically\nembedding documents interleaved with different modalities. Specifically, we\nachieve this by leveraging the capability of recent vision-language models that\nenable the processing and integration of text, images, and tables into a\nunified format and representation. Moreover, to mitigate the information loss\nfrom segmenting documents into passages, instead of representing and retrieving\npassages individually, we further merge the representations of segmented\npassages into one single document representation, while we additionally\nintroduce a reranking strategy to decouple and identify the relevant passage\nwithin the document if necessary. Then, through extensive experiments on\ndiverse information retrieval scenarios considering both the textual and\nmultimodal queries, we show that our approach substantially outperforms\nrelevant baselines, thanks to the consideration of the multimodal information\ninterleaved within the documents in a unified way.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02725v1","updated":"2024-10-03T17:47:29Z","published":"2024-10-03T17:47:29Z","title":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation","summary":"  Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.\n","authors":["Rohin Manvi","Anikait Singh","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.02725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10710v3","updated":"2024-10-03T17:46:40Z","published":"2024-04-16T16:36:50Z","title":"Autoregressive Pre-Training on Pixels and Texts","summary":"  The integration of visual and textual information represents a promising\ndirection in the advancement of language models. In this paper, we explore the\ndual modality of language--both visual and textual--within an autoregressive\nframework, pre-trained on both document images and texts. Our method employs a\nmultimodal training strategy, utilizing visual data through next patch\nprediction with a regression head and/or textual data through next token\nprediction with a classification head. We focus on understanding the\ninteraction between these two modalities and their combined impact on model\nperformance. Our extensive evaluation across a wide range of benchmarks shows\nthat incorporating both visual and textual data significantly improves the\nperformance of pixel-based language models. Remarkably, we find that a\nunidirectional pixel-based model trained solely on visual data can achieve\ncomparable results to state-of-the-art bidirectional models on several language\nunderstanding tasks. This work uncovers the untapped potential of integrating\nvisual and textual modalities for more effective language modeling. We release\nour code, data, and model checkpoints at\n\\url{https://github.com/ernie-research/pixelgpt}.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02724v1","updated":"2024-10-03T17:45:31Z","published":"2024-10-03T17:45:31Z","title":"Large Language Models as Markov Chains","summary":"  Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.\n","authors":["Oussama Zekri","Ambroise Odonnat","Abdelhakim Benechehab","Linus Bleistein","Nicolas Boullé","Ievgen Redko"],"pdf_url":"https://arxiv.org/pdf/2410.02724v1.pdf","comment":"49 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.02721v1","updated":"2024-10-03T17:40:55Z","published":"2024-10-03T17:40:55Z","title":"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization","summary":"  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.\n","authors":["Ryan C. Barron","Ves Grantcharov","Selma Wanna","Maksim E. Eren","Manish Bhattarai","Nicholas Solovyev","George Tompkins","Charles Nicholas","Kim Ø. Rasmussen","Cynthia Matuszek","Boian S. Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2410.02721v1.pdf","comment":"9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024"},{"id":"http://arxiv.org/abs/2410.02719v1","updated":"2024-10-03T17:39:38Z","published":"2024-10-03T17:39:38Z","title":"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling\n  for Retrieval-Augmented Generation","summary":"  We present UncertaintyRAG, a novel approach for long-context\nRetrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio\n(SNR)-based span uncertainty to estimate similarity between text chunks. This\nspan uncertainty enhances model calibration, improving robustness and\nmitigating semantic inconsistencies introduced by random chunking. Leveraging\nthis insight, we propose an efficient unsupervised learning technique to train\nthe retrieval model, alongside an effective data sampling and scaling strategy.\nUncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving\nstate-of-the-art results while using only 4% of the training data compared to\nother advanced open-source retrieval models under distribution shift settings.\nOur method demonstrates strong calibration through span uncertainty, leading to\nimproved generalization and robustness in long-context RAG tasks. Additionally,\nUncertaintyRAG provides a lightweight retrieval model that can be integrated\ninto any large language model with varying context window lengths, without the\nneed for fine-tuning, showcasing the flexibility of our approach.\n","authors":["Zixuan Li","Jing Xiong","Fanghua Ye","Chuanyang Zheng","Xun Wu","Jianqiao Lu","Zhongwei Wan","Xiaodan Liang","Chengming Li","Zhenan Sun","Lingpeng Kong","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.02719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02713v1","updated":"2024-10-03T17:36:49Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v1.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02712v1","updated":"2024-10-03T17:36:33Z","published":"2024-10-03T17:36:33Z","title":"LLaVA-Critic: Learning to Evaluate Multimodal Models","summary":"  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.\n","authors":["Tianyi Xiong","Xiyao Wang","Dong Guo","Qinghao Ye","Haoqi Fan","Quanquan Gu","Heng Huang","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02712v1.pdf","comment":"Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic"},{"id":"http://arxiv.org/abs/2410.02707v1","updated":"2024-10-03T17:31:31Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02703v1","updated":"2024-10-03T17:27:30Z","published":"2024-10-03T17:27:30Z","title":"Selective Attention Improves Transformer","summary":"  Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.\n","authors":["Yaniv Leviathan","Matan Kalman","Yossi Matias"],"pdf_url":"https://arxiv.org/pdf/2410.02703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12683v2","updated":"2024-10-03T17:27:28Z","published":"2023-12-20T00:49:52Z","title":"Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?","summary":"  The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.\n","authors":["Tannon Kew","Florian Schottmann","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2312.12683v2.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.07071v2","updated":"2024-10-03T17:26:48Z","published":"2024-07-09T17:44:34Z","title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps","summary":"  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n","authors":["Yung-Sung Chuang","Linlu Qiu","Cheng-Yu Hsieh","Ranjay Krishna","Yoon Kim","James Glass"],"pdf_url":"https://arxiv.org/pdf/2407.07071v2.pdf","comment":"EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens"},{"id":"http://arxiv.org/abs/2311.00237v3","updated":"2024-10-03T17:25:02Z","published":"2023-11-01T02:40:42Z","title":"The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis","summary":"  Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey.\n","authors":["Yuxiang Zhou","Jiazheng Li","Yanzheng Xiang","Hanqi Yan","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2311.00237v3.pdf","comment":"Accepted to the main conference of EMNLP 2024. Resources are\n  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources"},{"id":"http://arxiv.org/abs/2410.02694v1","updated":"2024-10-03T17:20:11Z","published":"2024-10-03T17:20:11Z","title":"HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly","summary":"  There have been many benchmarks for evaluating long-context language models\n(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack\n(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate\nto the diverse downstream applications of LCLMs, and the inconsistency further\ncomplicates model comparison. We investigate the underlying reasons behind\ncurrent practices and find that existing benchmarks often provide noisy signals\ndue to low coverage of applications, insufficient lengths, unreliable metrics,\nand incompatibility with base models. In this work, we present HELMET (How to\nEvaluate Long-context Models Effectively and Thoroughly), a comprehensive\nbenchmark encompassing seven diverse, application-centric categories. We also\naddress many issues in previous benchmarks by adding controllable lengths up to\n128k tokens, model-based evaluation for reliable metrics, and few-shot\nprompting for robustly evaluating base models. Consequently, we demonstrate\nthat HELMET offers more reliable and consistent rankings of frontier LCLMs.\nThrough a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks\nlike NIAH are not good predictors of downstream performance; (2) the diverse\ncategories in HELMET exhibit distinct trends and low correlation with each\nother; and (3) while most LCLMs achieve perfect NIAH scores, open-source models\nsignificantly lag behind closed ones when the task requires full-context\nreasoning or following complex instructions -- the gap widens with increased\nlengths. Finally, we recommend using our RAG tasks for fast model development,\nas they are easy to run and more predictive of other downstream performance;\nultimately, we advocate for a holistic evaluation across diverse tasks.\n","authors":["Howard Yen","Tianyu Gao","Minmin Hou","Ke Ding","Daniel Fleischer","Peter Izasak","Moshe Wasserblat","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02694v1.pdf","comment":"Code and data are available here:\n  https://github.com/princeton-nlp/HELMET"},{"id":"http://arxiv.org/abs/2410.02691v1","updated":"2024-10-03T17:18:03Z","published":"2024-10-03T17:18:03Z","title":"On the Proper Treatment of Tokenization in Psycholinguistics","summary":"  Language models are widely used in computational psycholinguistics to test\ntheories that relate the negative log probability (the surprisal) of a region\nof interest (a substring of characters) under a language model to its cognitive\ncost experienced by readers, as operationalized, for example, by gaze duration\non the region. However, the application of modern language models to\npsycholinguistic studies is complicated by the practice of using tokenization\nas an intermediate step in training a model. Doing so results in a language\nmodel over token strings rather than one over character strings. Vexingly,\nregions of interest are generally misaligned with these token strings. The\npaper argues that token-level language models should be (approximately)\nmarginalized into character-level language models before they are used in\npsycholinguistic studies to compute the surprisal of a region of interest;\nthen, the marginalized character-level language model can be used to compute\nthe surprisal of an arbitrary character substring, which we term a focal area,\nthat the experimenter may wish to use as a predictor. Our proposal of\nmarginalizing a token-level model into a character-level one solves this\nmisalignment issue independently of the tokenization scheme. Empirically, we\ndiscover various focal areas whose surprisal is a better psychometric predictor\nthan the surprisal of the region of interest itself.\n","authors":["Mario Giulianelli","Luca Malagutti","Juan Luis Gastaldi","Brian DuSell","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.02691v1.pdf","comment":"Main conference long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.03741v2","updated":"2024-10-03T17:15:24Z","published":"2024-01-08T09:01:29Z","title":"Enhanced Automated Code Vulnerability Repair using Large Language Models","summary":"  This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.\n","authors":["David de-Fitero-Dominguez","Eva Garcia-Lopez","Antonio Garcia-Cabot","Jose-Javier Martinez-Herraiz"],"pdf_url":"https://arxiv.org/pdf/2401.03741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03650v2","updated":"2024-10-03T17:13:04Z","published":"2024-09-05T16:08:19Z","title":"On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.\n","authors":["Yong Lin","Skyler Seto","Maartje ter Hoeve","Katherine Metcalf","Barry-John Theobald","Xuan Wang","Yizhe Zhang","Chen Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03650v2.pdf","comment":"12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.02684v1","updated":"2024-10-03T17:10:41Z","published":"2024-10-03T17:10:41Z","title":"HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router","summary":"  As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Ruibin Yuan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.02684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18725v2","updated":"2024-10-03T17:10:09Z","published":"2024-06-26T19:48:48Z","title":"Jailbreaking LLMs with Arabic Transliteration and Arabizi","summary":"  This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.\n","authors":["Mansour Al Ghanim","Saleh Almohaimeed","Mengxin Zheng","Yan Solihin","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2406.18725v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02683v1","updated":"2024-10-03T17:08:52Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2311.09756v2","updated":"2024-10-03T17:04:50Z","published":"2023-11-16T10:30:26Z","title":"StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning","summary":"  Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v2.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02678v1","updated":"2024-10-03T17:04:48Z","published":"2024-10-03T17:04:48Z","title":"Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data","summary":"  Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.\n","authors":["William Held","Ella Li","Michael Ryan","Weiyan Shi","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02677v1","updated":"2024-10-03T17:04:31Z","published":"2024-10-03T17:04:31Z","title":"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs","summary":"  To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.\n","authors":["Yu Ying Chiu","Liwei Jiang","Bill Yuchen Lin","Chan Young Park","Shuyue Stella Li","Sahithya Ravi","Mehar Bhatia","Maria Antoniak","Yulia Tsvetkov","Vered Shwartz","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02677v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.02675v1","updated":"2024-10-03T17:02:21Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable success achieved by neural networks, particularly\nthose represented by MLP and Transformer, we reveal that they exhibit potential\nflaws in the modeling and reasoning of periodicity, i.e., they tend to memorize\nthe periodic data rather than genuinely understanding the underlying principles\nof periodicity. However, periodicity is a crucial trait in various forms of\nreasoning and generalization, underpinning predictability across natural and\nengineered systems through recurring patterns in observations. In this paper,\nwe propose FAN, a novel network architecture based on Fourier Analysis, which\nempowers the ability to efficiently model and reason about periodic phenomena.\nBy introducing Fourier Series, the periodicity is naturally integrated into the\nstructure and computational processes of the neural network, thus achieving a\nmore accurate expression and prediction of periodic patterns. As a promising\nsubstitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in\nvarious models with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the effectiveness of FAN in modeling and reasoning about\nperiodic functions, and the superiority and generalizability of FAN across a\nrange of real-world tasks, including symbolic formula representation, time\nseries forecasting, and language modeling.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02674v1","updated":"2024-10-03T16:58:21Z","published":"2024-10-03T16:58:21Z","title":"Examining Language Modeling Assumptions Using an Annotated Literary\n  Dialect Corpus","summary":"  We present a dataset of 19th century American literary orthovariant tokens\nwith a novel layer of human-annotated dialect group tags designed to serve as\nthe basis for computational experiments exploring literarily meaningful\northographic variation. We perform an initial broad set of experiments over\nthis dataset using both token (BERT) and character (CANINE)-level contextual\nlanguage models. We find indications that the \"dialect effect\" produced by\nintentional orthographic variation employs multiple linguistic channels, and\nthat these channels are able to be surfaced to varied degrees given particular\nlanguage modelling assumptions. Specifically, we find evidence showing that\nchoice of tokenization scheme meaningfully impact the type of orthographic\ninformation a model is able to surface.\n","authors":["Craig Messner","Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2410.02674v1.pdf","comment":"Accepted to NLP4DH@EMNLP2024"},{"id":"http://arxiv.org/abs/2407.07950v2","updated":"2024-10-03T16:54:59Z","published":"2024-07-10T18:00:05Z","title":"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance","summary":"  The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.\n","authors":["Kaitlyn Zhou","Jena D. Hwang","Xiang Ren","Nouha Dziri","Dan Jurafsky","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2407.07950v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.07565v3","updated":"2024-10-03T16:48:55Z","published":"2024-07-10T11:50:20Z","title":"On Leakage of Code Generation Evaluation Datasets","summary":"  In this paper, we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nTo address this, we release Less Basic Python Problems (LBPP): an\nuncontaminated new benchmark of 161 prompts with their associated Python\nsolutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .\n","authors":["Alexandre Matton","Tom Sherborne","Dennis Aumiller","Elena Tommasone","Milad Alizadeh","Jingyi He","Raymond Ma","Maxime Voisin","Ellen Gilsenan-McMahon","Matthias Gallé"],"pdf_url":"https://arxiv.org/pdf/2407.07565v3.pdf","comment":"EMNLP 2024 Findings. 5 main pages, 9 in total"},{"id":"http://arxiv.org/abs/2410.02660v1","updated":"2024-10-03T16:46:52Z","published":"2024-10-03T16:46:52Z","title":"How to Train Long-Context Language Models (Effectively)","summary":"  We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.\n","authors":["Tianyu Gao","Alexander Wettig","Howard Yen","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02660v1.pdf","comment":"Our code, data, and models are available at\n  https://github.com/princeton-nlp/ProLong"},{"id":"http://arxiv.org/abs/2407.11969v3","updated":"2024-10-03T16:46:09Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v3.pdf","comment":"Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2410.02657v1","updated":"2024-10-03T16:43:17Z","published":"2024-10-03T16:43:17Z","title":"Hate Personified: Investigating the role of LLMs in content moderation","summary":"  For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.\n","authors":["Sarah Masud","Sahajpreet Singh","Viktor Hangya","Alexander Fraser","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2410.02657v1.pdf","comment":"17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains"},{"id":"http://arxiv.org/abs/2402.16382v2","updated":"2024-10-03T16:39:32Z","published":"2024-02-26T08:08:03Z","title":"Immunization against harmful fine-tuning attacks","summary":"  Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","Łukasz Bartoszcze","Jan Batzner","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2402.16382v2.pdf","comment":"Published in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02653v1","updated":"2024-10-03T16:36:35Z","published":"2024-10-03T16:36:35Z","title":"Measuring and Improving Persuasiveness of Generative Models","summary":"  LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.\n","authors":["Somesh Singh","Yaman K Singla","Harini SI","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2410.02653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02650v1","updated":"2024-10-03T16:34:46Z","published":"2024-10-03T16:34:46Z","title":"Undesirable Memorization in Large Language Models: A Survey","summary":"  While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.\n","authors":["Ali Satvaty","Suzan Verberne","Fatih Turkmen"],"pdf_url":"https://arxiv.org/pdf/2410.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02647v1","updated":"2024-10-03T16:33:35Z","published":"2024-10-03T16:33:35Z","title":"Immunogenicity Prediction with Dual Attention Enables Vaccine Target\n  Selection","summary":"  Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce ProVaccine, a novel\ndeep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 9,500 antigen sequences, structures, and immunogenicity\nlabels from bacteria, viruses, and tumors. Extensive experiments demonstrate\nthat ProVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research.\n","authors":["Song Li","Yang Tan","Song Ke","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02647v1.pdf","comment":"18 pages, 11 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.02026v2","updated":"2024-10-03T16:31:59Z","published":"2024-09-03T16:20:22Z","title":"Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization","summary":"  In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.\n","authors":["Sean I. Young"],"pdf_url":"https://arxiv.org/pdf/2409.02026v2.pdf","comment":"Preprint. 17 pages, 4 figures, 5 appendices"},{"id":"http://arxiv.org/abs/2409.11295v2","updated":"2024-10-03T16:30:43Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.02642v1","updated":"2024-10-03T16:25:37Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jiménez Gutiérrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02631v1","updated":"2024-10-03T16:15:04Z","published":"2024-10-03T16:15:04Z","title":"Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning","summary":"  Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.\n","authors":["Tianxiang Hu","Pei Zhang","Baosong Yang","Jun Xie","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.02631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08702v4","updated":"2024-10-03T16:11:43Z","published":"2024-02-13T16:38:01Z","title":"PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling","summary":"  Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.\n","authors":["Yongchao Chen","Jacob Arkin","Yilun Hao","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2402.08702v4.pdf","comment":"62 pages, 14 figures, Published in EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2403.13681v2","updated":"2024-10-03T16:01:01Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?","summary":"  In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01744v2","updated":"2024-10-03T15:57:05Z","published":"2024-10-02T16:55:01Z","title":"Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v2.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2409.05197v2","updated":"2024-10-03T15:55:40Z","published":"2024-09-08T19:22:58Z","title":"Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?","summary":"  State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.\n","authors":["Neeladri Bhuiya","Viktor Schlegel","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2409.05197v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.12191v2","updated":"2024-10-03T15:54:49Z","published":"2024-09-18T17:59:32Z","title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution","summary":"  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .\n","authors":["Peng Wang","Shuai Bai","Sinan Tan","Shijie Wang","Zhihao Fan","Jinze Bai","Keqin Chen","Xuejing Liu","Jialin Wang","Wenbin Ge","Yang Fan","Kai Dang","Mengfei Du","Xuancheng Ren","Rui Men","Dayiheng Liu","Chang Zhou","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.12191v2.pdf","comment":"Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors"},{"id":"http://arxiv.org/abs/2410.02613v1","updated":"2024-10-03T15:51:36Z","published":"2024-10-03T15:51:36Z","title":"NL-Eye: Abductive NLI for Images","summary":"  Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.\n","authors":["Mor Ventura","Michael Toker","Nitay Calderon","Zorik Gekhman","Yonatan Bitton","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2410.02613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02611v1","updated":"2024-10-03T15:50:08Z","published":"2024-10-03T15:50:08Z","title":"IndicSentEval: How Effectively do Multilingual Transformer Models encode\n  Linguistic Properties for Indic Languages?","summary":"  Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].\n","authors":["Akhilesh Aravapalli","Mounika Marreddy","Subba Reddy Oota","Radhika Mamidi","Manish Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.02611v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.02609v1","updated":"2024-10-03T15:49:35Z","published":"2024-10-03T15:49:35Z","title":"Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in\n  Under-Resourced Languages Using Explainable AI","summary":"  The proliferation of fake news has emerged as a significant threat to the\nintegrity of information dissemination, particularly on social media platforms.\nMisinformation can spread quickly due to the ease of creating and disseminating\ncontent, affecting public opinion and sociopolitical events. Identifying false\ninformation is therefore essential to reducing its negative consequences and\nmaintaining the reliability of online news sources. Traditional approaches to\nfake news detection often rely solely on content-based features, overlooking\nthe crucial role of social context in shaping the perception and propagation of\nnews articles. In this paper, we propose a comprehensive approach that\nintegrates social context-based features with news content features to enhance\nthe accuracy of fake news detection in under-resourced languages. We perform\nseveral experiments utilizing a variety of methodologies, including traditional\nmachine learning, neural networks, ensemble learning, and transfer learning.\nAssessment of the outcomes of the experiments shows that the ensemble learning\napproach has the highest accuracy, achieving a 0.99 F1 score. Additionally,\nwhen compared with monolingual models, the fine-tuned model with the target\nlanguage outperformed others, achieving a 0.94 F1 score. We analyze the\nfunctioning of the models, considering the important features that contribute\nto model performance, using explainable AI techniques.\n","authors":["Mesay Gemeda Yigezu","Melkamu Abay Mersha","Girma Yohannis Bade","Jugal Kalita","Olga Kolesnikova","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2410.02609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10960v3","updated":"2024-10-03T15:48:45Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v3.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.18256v3","updated":"2024-10-03T15:48:31Z","published":"2024-06-26T11:08:17Z","title":"Llamipa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.18164v3","updated":"2024-10-03T15:46:16Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2304.08460v3","updated":"2024-10-03T15:46:13Z","published":"2023-04-17T17:36:35Z","title":"LongForm: Effective Instruction Tuning with Reverse Instructions","summary":"  Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.\n","authors":["Abdullatif Köksal","Timo Schick","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.08460v3.pdf","comment":"EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks"},{"id":"http://arxiv.org/abs/2407.12402v2","updated":"2024-10-03T15:45:52Z","published":"2024-07-17T08:28:55Z","title":"TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish","summary":"  Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.\n","authors":["Arda Yüksel","Abdullatif Köksal","Lütfi Kerem Şenel","Anna Korhonen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2407.12402v2.pdf","comment":"EMNLP 2024 - Findings"},{"id":"http://arxiv.org/abs/2404.14741v2","updated":"2024-10-03T15:44:59Z","published":"2024-04-23T04:47:22Z","title":"Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering","summary":"  To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.\n","authors":["Yao Xu","Shizhu He","Jiabei Chen","Zihao Wang","Yangqiu Song","Hanghang Tong","Guang Liu","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.14741v2.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.02603v1","updated":"2024-10-03T15:44:42Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01769v2","updated":"2024-10-03T15:30:12Z","published":"2024-10-02T17:25:37Z","title":"Quantifying Generalization Complexity for Large Language Models","summary":"  While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.\n","authors":["Zhenting Qi","Hongyin Luo","Xuliang Huang","Zhuokai Zhao","Yibo Jiang","Xiangjun Fan","Himabindu Lakkaraju","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.01769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02584v1","updated":"2024-10-03T15:28:05Z","published":"2024-10-03T15:28:05Z","title":"Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions","summary":"  As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.\n","authors":["Angana Borah","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2410.02584v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2310.04484v3","updated":"2024-10-03T15:20:17Z","published":"2023-10-06T13:28:04Z","title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning","summary":"  Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions.\n","authors":["Wanyun Cui","Qianle Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11194v2","updated":"2024-10-03T15:13:58Z","published":"2024-06-17T04:00:04Z","title":"In-Context Editing: Learning Knowledge from Self-Induced Distributions","summary":"  In scenarios where language models must incorporate new information\nefficiently without extensive retraining, traditional fine-tuning methods are\nprone to overfitting, degraded generalization, and unnatural language\ngeneration. To address these limitations, we introduce Consistent In-Context\nEditing (ICE), a novel approach leveraging the model's in-context learning\ncapability to optimize toward a contextual distribution rather than a one-hot\ntarget. ICE introduces a simple yet effective optimization framework for the\nmodel to internalize new knowledge by aligning its output distributions with\nand without additional context. This method enhances the robustness and\neffectiveness of gradient-based tuning methods, preventing overfitting and\npreserving the model's integrity. We analyze ICE across four critical aspects\nof knowledge editing: accuracy, locality, generalization, and linguistic\nquality, demonstrating its advantages. Experimental results confirm the\neffectiveness of ICE and demonstrate its potential for continual editing,\nensuring that the integrity of the model is preserved while updating\ninformation.\n","authors":["Siyuan Qi","Bangcheng Yang","Kailin Jiang","Xiaobo Wang","Jiaqi Li","Yifan Zhong","Yaodong Yang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.11194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02560v1","updated":"2024-10-03T15:04:27Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Yakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v1.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.02558v1","updated":"2024-10-03T15:04:00Z","published":"2024-10-03T15:04:00Z","title":"Improving Unsupervised Constituency Parsing via Maximizing Semantic\n  Information","summary":"  Unsupervised constituency parsers organize phrases within a sentence into a\ntree-shaped syntactic constituent structure that reflects the organization of\nsentence semantics. However, the traditional objective of maximizing sentence\nlog-likelihood (LL) does not explicitly account for the close relationship\nbetween the constituent structure and the semantics, resulting in a weak\ncorrelation between LL values and parsing accuracy. In this paper, we introduce\na novel objective for training unsupervised parsers: maximizing the information\nbetween constituent structures and sentence semantics (SemInfo). We introduce a\nbag-of-substrings model to represent the semantics and apply the\nprobability-weighted information metric to estimate the SemInfo. Additionally,\nwe develop a Tree Conditional Random Field (TreeCRF)-based model to apply the\nSemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)\ninduction, the state-of-the-art method for unsupervised constituency parsing.\nExperiments demonstrate that SemInfo correlates more strongly with parsing\naccuracy than LL. Our algorithm significantly enhances parsing accuracy by an\naverage of 7.85 points across five PCFG variants and in four languages,\nachieving new state-of-the-art results in three of the four languages.\n","authors":["Junjie Chen","Xiangheng He","Yusuke Miyao","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2410.02558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12471v2","updated":"2024-10-03T14:56:29Z","published":"2024-06-18T10:20:36Z","title":"Fighting Randomness with Randomness: Mitigating Optimisation Instability\n  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation","summary":"  While fine-tuning of pre-trained language models generally helps to overcome\nthe lack of labelled training samples, it also displays model performance\ninstability. This instability mainly originates from randomness in\ninitialisation or data shuffling. To address this, researchers either modify\nthe training process or augment the available samples, which typically results\nin increased computational costs. We propose a new mitigation strategy, called\nDelayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths\nof ensembling, noise regularisation and model interpolation, while retaining\ncomputational efficiency. We compare DENI with 9 representative mitigation\nstrategies across 3 models, 4 tuning strategies and 7 text classification\ndatasets. We show that: 1) DENI outperforms the best performing mitigation\nstrategy (Ensemble), while using only a fraction of its cost; 2) the mitigation\nstrategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,\noutperforming full fine-tuning in specific cases; and 3) combining DENI with\ndata augmentation often leads to even more effective instability mitigation.\n","authors":["Branislav Pecher","Jan Cegin","Robert Belanec","Jakub Simko","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2406.12471v2.pdf","comment":"Accepted to the Findings of the EMNLP'24 Conference"},{"id":"http://arxiv.org/abs/2402.12817v2","updated":"2024-10-03T14:56:24Z","published":"2024-02-20T08:38:19Z","title":"On Sensitivity of Learning with Limited Labelled Data to the Effects of\n  Randomness: Impact of Interactions and Systematic Choices","summary":"  While learning with limited labelled data can improve performance when the\nlabels are lacking, it is also sensitive to the effects of uncontrolled\nrandomness introduced by so-called randomness factors (e.g., varying order of\ndata). We propose a method to systematically investigate the effects of\nrandomness factors while taking the interactions between them into\nconsideration. To measure the true effects of an individual randomness factor,\nour method mitigates the effects of other factors and observes how the\nperformance varies across multiple runs. Applying our method to multiple\nrandomness factors across in-context learning and fine-tuning approaches on 7\nrepresentative text classification tasks and meta-learning on 3 tasks, we show\nthat: 1) disregarding interactions between randomness factors in existing works\ncaused inconsistent findings due to incorrect attribution of the effects of\nrandomness factors, such as disproving the consistent sensitivity of in-context\nlearning to sample order even with random sample selection; and 2) besides\nmutual interactions, the effects of randomness factors, especially sample\norder, are also dependent on more systematic choices unexplored in existing\nworks, such as number of classes, samples per class or choice of prompt format.\n","authors":["Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2402.12817v2.pdf","comment":"Accepted to the EMNLP'24 Main Conference"},{"id":"http://arxiv.org/abs/2409.19700v2","updated":"2024-10-03T14:56:02Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02551v1","updated":"2024-10-03T14:55:22Z","published":"2024-10-03T14:55:22Z","title":"ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration","summary":"  We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.\n","authors":["Zixiang Wang","Yinghao Zhu","Huiya Zhao","Xiaochen Zheng","Tianlong Wang","Wen Tang","Yasha Wang","Chengwei Pan","Ewen M. Harrison","Junyi Gao","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02458v1","updated":"2024-10-03T14:50:33Z","published":"2024-10-03T14:50:33Z","title":"MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation","summary":"  Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n","authors":["Gurucharan Marthi Krishna Kumar","Aman Chadha","Janine Mendola","Amir Shmuel"],"pdf_url":"https://arxiv.org/pdf/2410.02458v1.pdf","comment":"Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2409.15977v3","updated":"2024-10-03T14:45:55Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://tcsinger.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.18045v3","updated":"2024-10-03T14:44:44Z","published":"2024-02-28T04:43:46Z","title":"Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore","summary":"  Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.\n","authors":["Sheikh Shafayat","Eunsu Kim","Juhyun Oh","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.18045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02538v1","updated":"2024-10-03T14:43:43Z","published":"2024-10-03T14:43:43Z","title":"Algorithms For Automatic Accentuation And Transcription Of Russian Texts\n  In Speech Recognition Systems","summary":"  This paper presents an overview of rule-based system for automatic\naccentuation and phonemic transcription of Russian texts for speech connected\ntasks, such as Automatic Speech Recognition (ASR). Two parts of the developed\nsystem, accentuation and transcription, use different approaches to achieve\ncorrect phonemic representations of input phrases. Accentuation is based on\n\"Grammatical dictionary of the Russian language\" of A.A. Zaliznyak and\nwiktionary corpus. To distinguish homographs, the accentuation system also\nutilises morphological information of the sentences based on Recurrent Neural\nNetworks (RNN). Transcription algorithms apply the rules presented in the\nmonograph of B.M. Lobanov and L.I. Tsirulnik \"Computer Synthesis and Voice\nCloning\". The rules described in the present paper are implemented in an\nopen-source module, which can be of use to any scientific study connected to\nASR or Speech To Text (STT) tasks. Automatically marked up text annotations of\nthe Russian Voxforge database were used as training data for an acoustic model\nin CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,\nmean Word Accuracy being 71.2%. The developed toolkit is written in the Python\nlanguage and is accessible on GitHub for any researcher interested.\n","authors":["Olga Iakovenko","Ivan Bondarenko","Mariya Borovikova","Daniil Vodolazsky"],"pdf_url":"https://arxiv.org/pdf/2410.02538v1.pdf","comment":"Speech and Computer 20th International Conference, SPECOM 2018,\n  Leipzig, Germany, Proceedings 20"},{"id":"http://arxiv.org/abs/2402.17512v3","updated":"2024-10-03T14:41:43Z","published":"2024-02-27T13:54:48Z","title":"Latte: Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.\n","authors":["Rares Dolga","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02525v1","updated":"2024-10-03T14:33:34Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17954v3","updated":"2024-10-03T14:29:11Z","published":"2024-02-28T00:24:29Z","title":"Twists, Humps, and Pebbles: Multilingual Speech Recognition Models\n  Exhibit Gender Performance Gaps","summary":"  Current automatic speech recognition (ASR) models are designed to be used\nacross many languages and tasks without substantial changes. However, this\nbroad language coverage hides performance gaps within languages, for example,\nacross genders. Our study systematically evaluates the performance of two\nwidely used multilingual ASR models on three datasets, encompassing 19\nlanguages from eight language families and two speaking conditions. Our\nfindings reveal clear gender disparities, with the advantaged group varying\nacross languages and models. Surprisingly, those gaps are not explained by\nacoustic or lexical properties. However, probing internal model states reveals\na correlation with gendered performance gap. That is, the easier it is to\ndistinguish speaker gender in a language using probes, the more the gap\nreduces, favoring female speakers. Our results show that gender disparities\npersist even in state-of-the-art models. Our findings have implications for the\nimprovement of multilingual ASR systems, underscoring the importance of\naccessibility to training data and nuanced evaluation to predict and mitigate\ngender gaps. We release all code and artifacts at\nhttps://github.com/g8a9/multilingual-asr-gender-gap.\n","authors":["Giuseppe Attanasio","Beatrice Savoldi","Dennis Fucci","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2402.17954v3.pdf","comment":"Accepted at EMNLP 2024. Code and artifacts at\n  https://github.com/g8a9/multilingual-asr-gender-gap"},{"id":"http://arxiv.org/abs/2410.02521v1","updated":"2024-10-03T14:28:40Z","published":"2024-10-03T14:28:40Z","title":"Methods for Automatic Matrix Language Determination of Code-Switched\n  Speech","summary":"  Code-switching (CS) is the process of speakers interchanging between two or\nmore languages which in the modern world becomes increasingly common. In order\nto better describe CS speech the Matrix Language Frame (MLF) theory introduces\nthe concept of a Matrix Language, which is the language that provides the\ngrammatical structure for a CS utterance. In this work the MLF theory was used\nto develop systems for Matrix Language Identity (MLID) determination. The MLID\nof English/Mandarin and English/Spanish CS text and speech was compared to\nacoustic language identity (LID), which is a typical way to identify a language\nin monolingual utterances. MLID predictors from audio show higher correlation\nwith the textual principles than LID in all cases while also outperforming LID\nin an MLID recognition task based on F1 macro (60\\%) and correlation score\n(0.38). This novel approach has identified that non-English languages (Mandarin\nand Spanish) are preferred over the English language as the ML contrary to the\nmonolingual choice of LID.\n","authors":["Olga Iakovenko","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2410.02521v1.pdf","comment":"Accepted at EMNLP"},{"id":"http://arxiv.org/abs/2309.15656v2","updated":"2024-10-03T14:27:14Z","published":"2023-09-27T13:45:38Z","title":"Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis","summary":"  Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.\n","authors":["Ildikó Pilán","Laurent Prévot","Hendrik Buschmeier","Pierre Lison"],"pdf_url":"https://arxiv.org/pdf/2309.15656v2.pdf","comment":"Updated version for SIGdial 2024"},{"id":"http://arxiv.org/abs/2408.03350v2","updated":"2024-10-03T14:20:40Z","published":"2024-08-05T20:19:18Z","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","summary":"  Real-world formal theorem proving often depends on a wealth of context,\nincluding definitions, lemmas, comments, file structure, and other information.\nWe introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new context that is not seen during\ntraining. miniCTX contains theorems sourced from real Lean projects and\ntextbooks, each associated with a context that can span tens of thousands of\ntokens. Models are tasked with proving a theorem given access to code from the\ntheorem's repository, which contains context that is needed for the proof. As a\nbaseline for miniCTX, we tested fine-tuning and prompting methods that\ncondition theorem proving on preceding context. Both approaches substantially\noutperform traditional methods that rely solely on state information. We found\nthat this ability to use context is not captured by previous benchmarks such as\nminiF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting\nand annotating theorem proving data, making it easy to add new projects into\nminiCTX to ensure that contexts are not seen during training. miniCTX offers a\nchallenging and realistic evaluation of neural theorem provers.\n","authors":["Jiewen Hu","Thomas Zhu","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2408.03350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02507v1","updated":"2024-10-03T14:15:00Z","published":"2024-10-03T14:15:00Z","title":"Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration","summary":"  Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.\n","authors":["Weikang Yuan","Junjie Cao","Zhuoren Jiang","Yangyang Kang","Jun Lin","Kaisong Song","tianqianjin lin","Pengwei Yan","Changlong Sun","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18028v2","updated":"2024-10-03T14:11:23Z","published":"2024-09-26T16:34:35Z","title":"Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective","summary":"  A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.\n","authors":["Yotam Wolf","Binyamin Rothberg","Dorin Shteyman","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2409.18028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02503v1","updated":"2024-10-03T14:06:43Z","published":"2024-10-03T14:06:43Z","title":"Mixed-Session Conversation with Egocentric Memory","summary":"  Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.\n","authors":["Jihyoung Jang","Taeyoung Kim","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02503v1.pdf","comment":"EMNLP Findings 2024 (30 pages); Project website:\n  https://mixed-session.github.io/"},{"id":"http://arxiv.org/abs/2407.03277v2","updated":"2024-10-03T14:05:14Z","published":"2024-07-03T17:04:17Z","title":"Evaluating Automatic Metrics with Incremental Machine Translation\n  Systems","summary":"  We introduce a dataset comprising commercial machine translations, gathered\nweekly over six years across 12 translation directions. Since human A/B testing\nis commonly used, we assume commercial systems improve over time, which enables\nus to evaluate machine translation (MT) metrics based on their preference for\nmore recent translations. Our study not only confirms several prior findings,\nsuch as the advantage of neural metrics over non-neural ones, but also explores\nthe debated issue of how MT quality affects metric reliability--an\ninvestigation that smaller datasets in previous research could not sufficiently\nexplore. Overall, our research demonstrates the dataset's value as a testbed\nfor metric evaluation. We release our code at https://github.com/gjwubyron/Evo\n","authors":["Guojun Wu","Shay B. Cohen","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2407.03277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02499v1","updated":"2024-10-03T14:01:01Z","published":"2024-10-03T14:01:01Z","title":"Defining Knowledge: Bridging Epistemology and Large Language Models","summary":"  Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.\n","authors":["Constanza Fierro","Ruchira Dhar","Filippos Stamatiou","Nicolas Garneau","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2410.02499v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02498v1","updated":"2024-10-03T14:00:44Z","published":"2024-10-03T14:00:44Z","title":"Dynamic Gradient Alignment for Online Data Mixing","summary":"  The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.\n","authors":["Simin Fan","David Grangier","Pierre Ablin"],"pdf_url":"https://arxiv.org/pdf/2410.02498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02492v1","updated":"2024-10-03T13:57:07Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2404.07103v3","updated":"2024-10-03T13:55:08Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v3.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2405.13448v2","updated":"2024-10-03T13:53:59Z","published":"2024-05-22T08:38:26Z","title":"Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning","summary":"  Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines.\n","authors":["Yuanhao Yue","Chengyu Wang","Jun Huang","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13448v2.pdf","comment":"emnlp 2024 findings"},{"id":"http://arxiv.org/abs/2407.04069v2","updated":"2024-10-03T13:51:53Z","published":"2024-07-04T17:15:37Z","title":"A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations","summary":"  Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.\n","authors":["Md Tahmid Rahman Laskar","Sawsan Alqahtani","M Saiful Bari","Mizanur Rahman","Mohammad Abdullah Matin Khan","Haidar Khan","Israt Jahan","Amran Bhuiyan","Chee Wei Tan","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2407.04069v2.pdf","comment":"Accepted at EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2312.02783v3","updated":"2024-10-03T13:47:02Z","published":"2023-12-05T14:14:27Z","title":"Large Language Models on Graphs: A Comprehensive Survey","summary":"  Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.\n","authors":["Bowen Jin","Gang Liu","Chi Han","Meng Jiang","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2312.02783v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2401.16332v4","updated":"2024-10-03T13:40:39Z","published":"2024-01-29T17:38:14Z","title":"Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering","summary":"  Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.\n","authors":["Yotam Wolf","Noam Wies","Dorin Shteyman","Binyamin Rothberg","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2401.16332v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17041v4","updated":"2024-10-03T13:31:39Z","published":"2023-11-28T18:53:06Z","title":"Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties","summary":"  A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.\n","authors":["Keunwoo Peter Yu","Zheyuan Zhang","Fengyuan Hu","Shane Storks","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2311.17041v4.pdf","comment":"16 pages, LaTeX; Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2404.15206v3","updated":"2024-10-03T13:23:59Z","published":"2024-04-23T16:39:03Z","title":"Does Instruction Tuning Make LLMs More Consistent?","summary":"  The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall.\n","authors":["Constanza Fierro","Jiaang Li","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2404.15206v3.pdf","comment":"We need to run extra experiments to ensure some of the claims in the\n  paper are fully correct"},{"id":"http://arxiv.org/abs/2410.02465v1","updated":"2024-10-03T13:15:19Z","published":"2024-10-03T13:15:19Z","title":"Response Tuning: Aligning Large Language Models without Instruction","summary":"  Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.\n","authors":["Seokhyun An","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02465v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2410.01242v2","updated":"2024-10-03T13:12:24Z","published":"2024-10-02T05:07:02Z","title":"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance","summary":"  Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.\n","authors":["Haolin Jin","Zechao Sun","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07431v2","updated":"2024-10-03T13:07:25Z","published":"2024-09-11T17:21:59Z","title":"Synthetic continued pretraining","summary":"  Pretraining on large-scale, unstructured internet text enables language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient--to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining with EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If, instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.\n","authors":["Zitong Yang","Neil Band","Shuangping Li","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2409.07431v2.pdf","comment":"Updated organization of experimental results and methods\n  introduction. Released the dataset and model weights artifact"},{"id":"http://arxiv.org/abs/2406.19999v2","updated":"2024-10-03T13:02:11Z","published":"2024-06-28T15:34:26Z","title":"The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models","summary":"  Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models.\n","authors":["Xinyi Chen","Baohao Liao","Jirui Qi","Panagiotis Eustratiadis","Christof Monz","Arianna Bisazza","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.19999v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02441v1","updated":"2024-10-03T12:39:14Z","published":"2024-10-03T12:39:14Z","title":"Embedded Topic Models Enhanced by Wikification","summary":"  Topic modeling analyzes a collection of documents to learn meaningful\npatterns of words. However, previous topic models consider only the spelling of\nwords and do not take into consideration the homography of words. In this\nstudy, we incorporate the Wikipedia knowledge into a neural topic model to make\nit aware of named entities. We evaluate our method on two datasets, 1) news\narticles of \\textit{New York Times} and 2) the AIDA-CoNLL dataset. Our\nexperiments show that our method improves the performance of neural topic\nmodels in generalizability. Moreover, we analyze frequent terms in each topic\nand the temporal dependencies between topics to demonstrate that our\nentity-aware topic models can capture the time-series development of topics\nwell.\n","authors":["Takashi Shibuya","Takehito Utsuro"],"pdf_url":"https://arxiv.org/pdf/2410.02441v1.pdf","comment":"Accepted at EMNLP 2024 Workshop NLP for Wikipedia"},{"id":"http://arxiv.org/abs/2410.02433v1","updated":"2024-10-03T12:28:13Z","published":"2024-10-03T12:28:13Z","title":"Better Call SAUL: Fluent and Consistent Language Model Editing with\n  Generation Regularization","summary":"  To ensure large language models contain up-to-date knowledge, they need to be\nupdated regularly. However, model editing is challenging as it might also\naffect knowledge that is unrelated to the new data. State-of-the-art methods\nidentify parameters associated with specific knowledge and then modify them via\ndirect weight updates. However, these locate-and-edit methods suffer from heavy\ncomputational overhead and lack theoretical validation. In contrast, directly\nfine-tuning the model on requested edits affects the model's behavior on\nunrelated knowledge, and significantly damages the model's generation fluency\nand consistency. To address these challenges, we propose SAUL, a streamlined\nmodel editing method that uses sentence concatenation with augmented random\nfacts for generation regularization. Evaluations on three model editing\nbenchmarks show that SAUL is a practical and reliable solution for model\nediting outperforming state-of-the-art methods while maintaining generation\nquality and reducing computational overhead.\n","authors":["Mingyang Wang","Lukas Lange","Heike Adel","Jannik Strötgen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2410.02433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02429v1","updated":"2024-10-03T12:24:18Z","published":"2024-10-03T12:24:18Z","title":"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.\n","authors":["Tuo An","Yunjiao Zhou","Han Zou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02429v1.pdf","comment":"21 pages, 10 figures, submitted to ICLR 2025 Conference"},{"id":"http://arxiv.org/abs/2410.02428v1","updated":"2024-10-03T12:21:17Z","published":"2024-10-03T12:21:17Z","title":"Collective Critics for Creative Story Generation","summary":"  Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.\n","authors":["Minwook Bae","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02428v1.pdf","comment":"EMNLP 2024 (36 pages)"},{"id":"http://arxiv.org/abs/2406.13092v2","updated":"2024-10-03T12:20:10Z","published":"2024-06-18T22:44:50Z","title":"Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language\n  Story Understanding","summary":"  Story video-text alignment, a core task in computational story understanding,\naims to align video clips with corresponding sentences in their descriptions.\nHowever, progress on the task has been held back by the scarcity of manually\nannotated video-text correspondence and the heavy concentration on English\nnarrations of Hollywood movies. To address these issues, in this paper, we\nconstruct a large-scale multilingual video story dataset named Multilingual\nSynopses of Movie Narratives (M-SYMON), containing 13,166 movie summary videos\nfrom 7 languages, as well as manual annotation of fine-grained video-text\ncorrespondences for 101.5 hours of video. Training on the human annotated data\nfrom SyMoN outperforms the SOTA methods by 15.7 and 16.2 percentage points on\nClip Accuracy and Sentence IoU scores, respectively, demonstrating the\neffectiveness of the annotations. As benchmarks for future research, we create\n6 baseline approaches with different multilingual training strategies, compare\ntheir performance in both intra-lingual and cross-lingual setups, exemplifying\nthe challenges of multilingual video-text alignment. The dataset is released\nat: https://github.com/insundaycathy/M-SyMoN\n","authors":["Yidan Sun","Jianfei Yu","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2406.13092v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.02426v1","updated":"2024-10-03T12:19:49Z","published":"2024-10-03T12:19:49Z","title":"Learning the Latent Rules of a Game from Data: A Chess Story","summary":"  We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.\n","authors":["Ben Fauber"],"pdf_url":"https://arxiv.org/pdf/2410.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02425v1","updated":"2024-10-03T12:19:06Z","published":"2024-10-03T12:19:06Z","title":"LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services","summary":"  As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.\n","authors":["Małgorzata Łazuka","Andreea Anghel","Thomas Parnell"],"pdf_url":"https://arxiv.org/pdf/2410.02425v1.pdf","comment":"Accepted to the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC '24)"},{"id":"http://arxiv.org/abs/2410.02417v1","updated":"2024-10-03T12:07:34Z","published":"2024-10-03T12:07:34Z","title":"MenakBERT -- Hebrew Diacriticizer","summary":"  Diacritical marks in the Hebrew language give words their vocalized form. The\ntask of adding diacritical marks to plain Hebrew text is still dominated by a\nsystem that relies heavily on human-curated resources. Recent models trained on\ndiacritized Hebrew texts still present a gap in performance. We use a recently\ndeveloped char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a\ncharacter level transformer pretrained on Hebrew text and fine-tuned to produce\ndiacritical marks for Hebrew sentences. We continue to show how finetuning a\nmodel for diacritizing transfers to a task such as part of speech tagging.\n","authors":["Ido Cohen","Jacob Gidron","Idan Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.02417v1.pdf","comment":"Published at ISCOL2022 as a poster"},{"id":"http://arxiv.org/abs/2406.11096v3","updated":"2024-10-03T11:57:00Z","published":"2024-06-16T22:59:18Z","title":"The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models","summary":"  Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs.\n","authors":["Bolei Ma","Xinpeng Wang","Tiancheng Hu","Anna-Carolina Haensch","Michael A. Hedderich","Barbara Plank","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2406.11096v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2212.00596v2","updated":"2024-10-03T11:42:43Z","published":"2022-12-01T15:48:51Z","title":"Language models and brains align due to more than next-word prediction\n  and word-level information","summary":"  Pretrained language models have been shown to significantly predict brain\nrecordings of people comprehending language. Recent work suggests that the\nprediction of the next word is a key mechanism that contributes to this\nalignment. What is not yet understood is whether prediction of the next word is\nnecessary for this observed alignment or simply sufficient, and whether there\nare other shared mechanisms or information that are similarly important. In\nthis work, we take a step towards understanding the reasons for brain alignment\nvia two simple perturbations in popular pretrained language models. These\nperturbations help us design contrasts that can control for different types of\ninformation. By contrasting the brain alignment of these differently perturbed\nmodels, we show that improvements in alignment with brain recordings are due to\nmore than improvements in next-word prediction and word-level information.\n","authors":["Gabriele Merlin","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2212.00596v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02396v1","updated":"2024-10-03T11:17:58Z","published":"2024-10-03T11:17:58Z","title":"Parameter Competition Balancing for Model Merging","summary":"  While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.\n","authors":["Guodong Du","Junlin Lee","Jing Li","Runhua Jiang","Yifei Guo","Shuyang Yu","Hanting Liu","Sim Kuan Goh","Ho-Kin Tang","Daojing He","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02396v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.13560v2","updated":"2024-10-03T11:17:43Z","published":"2024-06-19T13:48:19Z","title":"Lexically Grounded Subword Segmentation","summary":"  We present three innovations in tokenization and subword segmentation. First,\nwe propose to use unsupervised morphological analysis with Morfessor as\npre-tokenization. Second, we present an algebraic method for obtaining subword\nembeddings grounded in a word embedding space. Based on that, we design a novel\nsubword segmentation algorithm that uses the embeddings, ensuring that the\nprocedure considers lexical meaning. Third, we introduce an efficient\nsegmentation algorithm based on a subword bigram model that can be initialized\nwith the lexically aware segmentation method to avoid using Morfessor and large\nembedding tables at inference time. We evaluate the proposed approaches using\ntwo intrinsic metrics and measure their performance on two downstream tasks:\npart-of-speech tagging and machine translation. Our experiments show\nsignificant improvements in the morphological plausibility of the segmentation\nwhen evaluated using segmentation precision on morpheme boundaries and improved\nR\\'enyi efficiency in 8 languages. Although the proposed tokenization methods\ndo not have a large impact on automatic translation quality, we observe\nconsistent performance gains in the arguably more morphological task of\npart-of-speech tagging.\n","authors":["Jindřich Libovický","Jindřich Helcl"],"pdf_url":"https://arxiv.org/pdf/2406.13560v2.pdf","comment":"Camera-ready, EMNLP Main conf"},{"id":"http://arxiv.org/abs/2406.13663v3","updated":"2024-10-03T11:03:22Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v3.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2410.02381v1","updated":"2024-10-03T11:01:25Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.02889v2","updated":"2024-10-03T11:01:14Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v2.pdf","comment":"20 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.02378v1","updated":"2024-10-03T10:51:02Z","published":"2024-10-03T10:51:02Z","title":"Towards Comprehensive Detection of Chinese Harmful Memes","summary":"  This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes\nhave proliferated on the Chinese Internet, while research on detecting Chinese\nharmful memes significantly lags behind due to the absence of reliable datasets\nand effective detectors. To this end, we focus on the comprehensive detection\nof Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful\nmeme dataset, which consists of 12,000 samples with fine-grained annotations\nfor various meme types. Additionally, we propose a baseline detector,\nMultimodal Knowledge Enhancement (MKE), incorporating contextual information of\nmeme content generated by the LLM to enhance the understanding of Chinese\nmemes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of\nMKE. The resources for this paper are available at\nhttps://github.com/DUT-lujunyu/ToxiCN_MM.\n","authors":["Junyu Lu","Bo Xu","Xiaokun Zhang","Hongbo Wang","Haohao Zhu","Dongyu Zhang","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2410.02378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02365v1","updated":"2024-10-03T10:24:24Z","published":"2024-10-03T10:24:24Z","title":"From Concrete to Abstract: A Multimodal Generative Approach to Abstract\n  Concept Learning","summary":"  Understanding and manipulating concrete and abstract concepts is fundamental\nto human intelligence. Yet, they remain challenging for artificial agents. This\npaper introduces a multimodal generative approach to high order abstract\nconcept learning, which integrates visual and categorical linguistic\ninformation from concrete ones. Our model initially grounds subordinate level\nconcrete concepts, combines them to form basic level concepts, and finally\nabstracts to superordinate level concepts via the grounding of basic-level\nconcepts. We evaluate the model language learning ability through\nlanguage-to-visual and visual-to-language tests with high order abstract\nconcepts. Experimental results demonstrate the proficiency of the model in both\nlanguage understanding and language naming tasks.\n","authors":["Haodong Xie","Rahul Singh Maharjan","Federico Tavella","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2410.02365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v1","updated":"2024-10-03T10:06:27Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02343v1","updated":"2024-10-03T09:53:48Z","published":"2024-10-03T09:53:48Z","title":"Listening to the Wise Few: Select-and-Copy Attention Heads for\n  Multiple-Choice QA","summary":"  A standard way to evaluate the abilities of LLM involves presenting a\nmultiple-choice question and selecting the option with the highest logit as the\nmodel's predicted answer. However, such a format for evaluating LLMs has\nlimitations, since even if the model knows the correct answer, it may struggle\nto select the corresponding letter simply due to difficulties in following this\nrigid format. To address this, we introduce new scores that better capture and\nreveal model's underlying knowledge: the Query-Key Score (QK-score), derived\nfrom the interaction between query and key representations in attention heads,\nand the Attention Score, based on attention weights. These scores are extracted\nfrom specific \\textit{select-and-copy} heads, which show consistent performance\nacross popular Multi-Choice Question Answering (MCQA) datasets. Based on these\nscores, our method improves knowledge extraction, yielding up to 16\\% gain for\nLLaMA2-7B and up to 10\\% for larger models on popular MCQA benchmarks. At the\nsame time, the accuracy on a simple synthetic dataset, where the model\nexplicitly knows the right answer, increases by almost 60\\%, achieving nearly\nperfect accuracy, therefore demonstrating the method's efficiency in mitigating\nMCQA format limitations. To support our claims, we conduct experiments on\nmodels ranging from 7 billion to 70 billion parameters in both zero- and\nfew-shot setups.\n","authors":["Eduard Tulchinskii","Laida Kushnareva","Kristian Kuznetsov","Anastasia Voznyuk","Andrei Andriiainen","Irina Piontkovskaya","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2410.02343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02338v1","updated":"2024-10-03T09:48:09Z","published":"2024-10-03T09:48:09Z","title":"How Much Can RAG Help the Reasoning of LLM?","summary":"  Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.\n","authors":["Jingyu Liu","Jiaen Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18369v2","updated":"2024-10-03T09:45:47Z","published":"2024-05-28T17:08:31Z","title":"PromptWizard: Task-Aware Prompt Optimization Framework","summary":"  Large language models (LLMs) have transformed AI across diverse domains, with\nprompting being central to their success in guiding model outputs. However,\nmanual prompt engineering is both labor-intensive and domain-specific,\nnecessitating the need for automated solutions. We introduce PromptWizard, a\nnovel, fully automated framework for discrete prompt optimization, utilizing a\nself-evolving, self-adapting mechanism. Through a feedback-driven critique and\nsynthesis process, PromptWizard achieves an effective balance between\nexploration and exploitation, iteratively refining both prompt instructions and\nin-context examples to generate human-readable, task-specific prompts. This\nguided approach systematically improves prompt quality, resulting in superior\nperformance across 45 tasks. PromptWizard excels even with limited training\ndata, smaller LLMs, and various LLM architectures. Additionally, our cost\nanalysis reveals a substantial reduction in API calls, token usage, and overall\ncost, demonstrating PromptWizard's efficiency, scalability, and advantages over\nexisting prompt optimization strategies.\n","authors":["Eshaan Agarwal","Joykirat Singh","Vivek Dani","Raghav Magazine","Tanuja Ganu","Akshay Nambi"],"pdf_url":"https://arxiv.org/pdf/2405.18369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12319v2","updated":"2024-10-03T09:38:48Z","published":"2024-06-18T06:43:04Z","title":"On the Adversarial Vulnerability of Pairwise Evaluation Using Large\n  Language Models","summary":"  Pairwise evaluation using large language models (LLMs) is widely adopted for\nevaluating generated outputs. However, the reliability of LLM evaluators is\noften compromised by their biased preferences, such as favoring verbosity and\nan authoritative tone. In this work, we find that the evaluation setup itself\ncan significantly amplify these biases, where pairwise evaluators exhibit more\nundesirable tendencies than pointwise evaluators. Our analysis further reveals\nthat even when pairwise evaluators make incorrect judgments, they can still\naccurately identify shortcomings in low-quality outputs. As a simple remedy, we\nalso propose incorporating pointwise reasoning into pairwise evaluation.\nExperimental results show that our method improves the performance of pairwise\nevaluators on adversarial samples across various models. We hope our findings\nencourage further exploration into the reliability of LLM evaluators.\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04459v3","updated":"2024-10-03T09:32:31Z","published":"2024-07-05T12:09:40Z","title":"Generalists vs. Specialists: Evaluating Large Language Models for Urdu","summary":"  In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b,\nwith special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that\nhave been fine-tuned on specific tasks. We focus on seven classification and\nseven generation tasks to evaluate the performance of these models on Urdu\nlanguage. Urdu has 70 million native speakers, yet it remains underrepresented\nin Natural Language Processing (NLP). Despite the frequent advancements in\nLarge Language Models (LLMs), their performance in low-resource languages,\nincluding Urdu, still needs to be explored. We also conduct a human evaluation\nfor the generation tasks and compare the results with the evaluations performed\nby GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose\nmodels consistently outperform general-purpose models across various tasks. We\nalso find that the evaluation done by GPT-4-Turbo for generation tasks aligns\nmore closely with human evaluation compared to the evaluation the evaluation\ndone by Llama-3-8b. This paper contributes to the NLP community by providing\ninsights into the effectiveness of general and specific-purpose LLMs for\nlow-resource languages.\n","authors":["Samee Arif","Abdul Hameed Azeemi","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2407.04459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02330v1","updated":"2024-10-03T09:28:59Z","published":"2024-10-03T09:28:59Z","title":"Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection","summary":"  As a manner to augment pre-trained large language models (LLM), knowledge\ninjection is critical to develop vertical domain large models and has been\nwidely studied. Although most current approaches, including parameter-efficient\nfine-tuning (PEFT) and block expansion methods, uniformly apply knowledge\nacross all LLM layers, it raises the question: are all layers equally crucial\nfor knowledge injection? We begin by evaluating the importance of each layer in\nfinding the optimal layer range for knowledge injection. Intuitively, the more\nimportant layers should play a more critical role in knowledge injection and\ndeserve a denser injection. We observe performance dips in question-answering\nbenchmarks after the removal or expansion of the shallow layers, and the\ndegradation shrinks as the layer gets deeper, indicating that the shallow\nlayers hold the key to knowledge injection. This insight leads us to propose\nthe S strategy, a post-pretraining strategy of selectively enhancing shallow\nlayers while pruning the less effective deep ones. Based on this strategy, we\nintroduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the\ncorpus of code $\\&$ math and demonstrated the effectiveness of our strategy.\nFurther experiments across different LLM, Mistral-7B, and a legal corpus\nconfirmed the general applicability of the approach, underscoring its\nwide-ranging efficacy. Our code is available at:\n\\https://github.com/txchen-USTC/Llama-Slayer\n","authors":["Tianxiang Chen","Zhentao Tan","Tao Gong","Yue Wu","Qi Chu","Bin Liu","Jieping Ye","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16050v2","updated":"2024-10-03T09:24:56Z","published":"2024-02-25T10:27:46Z","title":"Efficient Temporal Extrapolation of Multimodal Large Language Models\n  with Temporal Grounding Bridge","summary":"  Despite progress in multimodal large language models (MLLMs), the challenge\nof interpreting long-form videos in response to linguistic queries persists,\nlargely due to the inefficiency in temporal grounding and limited pre-trained\ncontext window size. In this work, we introduce Temporal Grounding Bridge\n(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding\ncapabilities and broadens their contextual scope. Our framework significantly\nenhances the temporal capabilities of current MLLMs through three key\ninnovations: an efficient multi-span temporal grounding algorithm applied to\nlow-dimension temporal features projected from flow; a multimodal length\nextrapolation training paradigm that utilizes low-dimension temporal features\nto extend the training context window size; and a bootstrapping framework that\nbridges our model with pluggable MLLMs without requiring annotation. We\nvalidate TGB across seven video benchmarks and demonstrate substantial\nperformance improvements compared with prior MLLMs. Notably, our model,\ninitially trained on sequences of four frames, effectively handles sequences up\nto 16 longer without sacrificing performance, highlighting its scalability and\neffectiveness in real-world applications. Our code is publicly available at\nhttps://github.com/bigai-nlco/VideoTGB\n","authors":["Yuxuan Wang","Yueqian Wang","Pengfei Wu","Jianxin Liang","Dongyan Zhao","Yang Liu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.16050v2.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.12924v2","updated":"2024-10-03T09:21:57Z","published":"2024-09-04T03:17:19Z","title":"WaveletGPT: Wavelets Meet Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02320v1","updated":"2024-10-03T08:56:29Z","published":"2024-10-03T08:56:29Z","title":"Post-edits Are Preferences Too","summary":"  Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors \\emph{create} $s_1$ and know that it should be better\nthan $s_2$. We attempt to use these implicit preferences for PO and show that\nit helps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.\n","authors":["Nathaniel Berger","Stefan Riezler","Miriam Exel","Matthias Huck"],"pdf_url":"https://arxiv.org/pdf/2410.02320v1.pdf","comment":"To appear at the Ninth Conference on Machine Translation (WMT24)"},{"id":"http://arxiv.org/abs/2406.02069v3","updated":"2024-10-03T08:46:42Z","published":"2024-06-04T07:51:30Z","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling","summary":"  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.\n","authors":["Zefan Cai","Yichi Zhang","Bofei Gao","Yuliang Liu","Tianyu Liu","Keming Lu","Wayne Xiong","Yue Dong","Baobao Chang","Junjie Hu","Wen Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.02069v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02308v1","updated":"2024-10-03T08:44:17Z","published":"2024-10-03T08:44:17Z","title":"Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large\n  Language Models","summary":"  Phrases are fundamental linguistic units through which humans convey\nsemantics. This study critically examines the capacity of API-based large\nlanguage models (LLMs) to comprehend phrase semantics, utilizing three\nhuman-annotated datasets. We assess the performance of LLMs in executing phrase\nsemantic reasoning tasks guided by natural language instructions and explore\nthe impact of common prompting techniques, including few-shot demonstrations\nand Chain-of-Thought reasoning. Our findings reveal that LLMs greatly\noutperform traditional embedding methods across the datasets; however, they do\nnot show a significant advantage over fine-tuned methods. The effectiveness of\nadvanced prompting strategies shows variability. We conduct detailed error\nanalyses to interpret the limitations faced by LLMs in comprehending phrase\nsemantics. Code and data can be found at\nhttps://github.com/memray/llm_phrase_semantics.\n","authors":["Rui Meng","Ye Liu","Lifu Tu","Daqing He","Yingbo Zhou","Semih Yavuz"],"pdf_url":"https://arxiv.org/pdf/2410.02308v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17233v2","updated":"2024-10-03T08:43:25Z","published":"2024-06-25T02:37:53Z","title":"Self-Constructed Context Decompilation with Fined-grained Alignment\n  Enhancement","summary":"  Decompilation transforms compiled code back into a high-level programming\nlanguage for analysis when source code is unavailable. Previous work has\nprimarily focused on enhancing decompilation performance by increasing the\nscale of model parameters or training data for pre-training. Based on the\ncharacteristics of the decompilation task, we propose two methods: (1) Without\nfine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method\nrecompiles the LLM's decompilation results to construct pairs for in-context\nlearning, helping the model improve decompilation performance. (2) Fine-grained\nAlignment Enhancement (FAE), which meticulously aligns assembly code with\nsource code at the statement level by leveraging debugging information, is\nemployed during the fine-tuning phase to achieve further improvements in\ndecompilation. By integrating these two methods, we achieved a Re-Executability\nperformance improvement of approximately 3.90% on the Decompile-Eval benchmark,\nestablishing a new state-of-the-art performance of 52.41%. The code, data, and\nmodels are available at https://github.com/AlongWY/sccdec.\n","authors":["Yunlong Feng","Dechuan Teng","Yang Xu","Honglin Mu","Xiao Xu","Libo Qin","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2406.17233v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02298v1","updated":"2024-10-03T08:34:17Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02297v1","updated":"2024-10-03T08:27:59Z","published":"2024-10-03T08:27:59Z","title":"Make Compound Sentences Simple to Analyze: Learning to Split Sentences\n  for Aspect-based Sentiment Analysis","summary":"  In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods\nhave shown promising results and achieved substantial advancements. However,\ndespite these advancements, the tasks of extracting sentiment quadruplets,\nwhich capture the nuanced sentiment expressions within a sentence, remain\nsignificant challenges. In particular, compound sentences can potentially\ncontain multiple quadruplets, making the extraction task increasingly difficult\nas sentence complexity grows. To address this issue, we are focusing on\nsimplifying sentence structures to facilitate the easier recognition of these\nelements and crafting a model that integrates seamlessly with various ABSA\ntasks. In this paper, we propose Aspect Term Oriented Sentence Splitter\n(ATOSS), which simplifies compound sentence into simpler and clearer forms,\nthereby clarifying their structure and intent. As a plug-and-play module, this\napproach retains the parameters of the ABSA model while making it easier to\nidentify essential intent within input sentences. Extensive experimental\nresults show that utilizing ATOSS outperforms existing methods in both ASQP and\nACOS tasks, which are the primary tasks for extracting sentiment quadruplets.\n","authors":["Yongsik Seo","Sungwon Song","Ryang Heo","Jieyong Kim","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02297v1.pdf","comment":"Accepted at EMNLP 2024 (Findings, long paper)"},{"id":"http://arxiv.org/abs/2410.02296v1","updated":"2024-10-03T08:27:54Z","published":"2024-10-03T08:27:54Z","title":"Language Models are Graph Learners","summary":"  Language Models (LMs) are increasingly challenging the dominance of\ndomain-specific models, including Graph Neural Networks (GNNs) and Graph\nTransformers (GTs), in graph learning tasks. Following this trend, we propose a\nnovel approach that empowers off-the-shelf LMs to achieve performance\ncomparable to state-of-the-art GNNs on node classification tasks, without\nrequiring any architectural modification. By preserving the LM's original\narchitecture, our approach retains a key benefit of LM instruction tuning: the\nability to jointly train on diverse datasets, fostering greater flexibility and\nefficiency. To achieve this, we introduce two key augmentation strategies: (1)\nEnriching LMs' input using topological and semantic retrieval methods, which\nprovide richer contextual information, and (2) guiding the LMs' classification\nprocess through a lightweight GNN classifier that effectively prunes class\ncandidates. Our experiments on real-world datasets show that backbone Flan-T5\nmodels equipped with these augmentation strategies outperform state-of-the-art\ntext-output node classifiers and are comparable to top-performing vector-output\nnode classifiers. By bridging the gap between specialized task-specific node\nclassifiers and general LMs, this work paves the way for more versatile and\nwidely applicable graph learning models. We will open-source the code upon\npublication.\n","authors":["Zhe Xu","Kaveh Hassani","Si Zhang","Hanqing Zeng","Michihiro Yasunaga","Limei Wang","Dongqi Fu","Ning Yao","Bo Long","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2410.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18430v2","updated":"2024-10-03T08:24:40Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2410.02293v1","updated":"2024-10-03T08:23:06Z","published":"2024-10-03T08:23:06Z","title":"Efficient Second-Order Neural Network Optimization via Adaptive Trust\n  Region Methods","summary":"  Second-order optimization methods offer notable advantages in training deep\nneural networks by utilizing curvature information to achieve faster\nconvergence. However, traditional second-order techniques are computationally\nprohibitive, primarily due to the large matrix inversions and high memory\ndemands they require. While adaptive trust-region methods have been developed\nto mitigate these issues, their performance is often hindered by conservative\nestimates of key parameters, such as the Lipschitz constant of the Hessian,\nresulting in suboptimal outcomes. In this paper, we introduce\nSecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to\novercome these limitations. SOAA approximates the Fisher information matrix\nusing a diagonal representation, reducing computational complexity from\n\\(O(n^{2})\\) to \\(O(n)\\), thereby making it suitable for large-scale deep\nlearning models, including large language models (LLMs). Additionally, the\nalgorithm integrates an adaptive trust-region mechanism that dynamically\nadjusts the trust region size based on observed loss reduction, ensuring both\nrobust convergence and computational efficiency. We empirically demonstrate\nthat SOAA achieves faster and more stable convergence compared to first-order\noptimizers, such as Adam, under similar computational constraints. However, the\ndiagonal approximation of the Fisher information matrix may be less effective\nin capturing higher-order interactions between gradients, suggesting potential\nareas for further refinement and future research.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.02293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02284v1","updated":"2024-10-03T08:07:55Z","published":"2024-10-03T08:07:55Z","title":"Correlation and Navigation in the Vocabulary Key Representation Space of\n  Language Models","summary":"  Language model (LM) decoding is based on the next-token prediction (NTP)\nprobability distribution. For neural LMs (e.g., Transformer-based), NTP\ndistribution is essentially a softmax-regularized dot product between an\nencoded input context (query) and fixed vocabulary representations (keys). In\nthis paper, we study the effect of the key distribution on the NTP\ndistribution, with a focus on whether the similarity between keys will trigger\nspurious correlations in NTP. Through knowledge-probing tasks, we show that in\nthe NTP distribution, the few top-ranked tokens are typically accurate.\nHowever, the middle-ranked prediction is highly biased towards the tokens that\nare distributionally (not necessarily semantically) similar to these top ones.\nFor instance, if \"P\" is predicted as the top-1 token, \"A\"-\"Z\" will all be\nranked high in NTP, no matter whether they can lead to correct decoding\nresults. This hurts the sampling diversity and makes the sampling of correct,\nlong-tail results hopeless and noisy. We attempt to alleviate this issue via a\nnovel in-context method that iteratively pushes the query representation away\nfrom explored regions. Specifically, we include the explored decoding results\nin the context and prompt the LM to generate something else, which encourages\nthe LM to produce a query representation that has small dot products with\nexplored keys. Experiments on knowledge-probing tasks show that our method\nleads to efficient navigation away from explored keys to correct new keys. We\nfurther extend our method to open-ended and chain-of-thought (for reasoning)\ngeneration. Experiment results show that ICN contributes to better generation\ndiversity and improved self-consistency voting performance. Finally, we discuss\npotential training issues caused by the fixed key space together with the\nchallenges and possible ways to address them in future research.\n","authors":["Letian Peng","Chenyang An","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2410.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02283v1","updated":"2024-10-03T08:07:14Z","published":"2024-10-03T08:07:14Z","title":"Morphological evaluation of subwords vocabulary used by BETO language\n  model","summary":"  Subword tokenization algorithms used by Large Language Models are\nsignificantly more efficient and can independently build the necessary\nvocabulary of words and subwords without human intervention. However, those\nsubwords do not always align with real morphemes, potentially impacting the\nmodels' performance, though it remains uncertain when this might occur. In\nprevious research, we proposed a method to assess the morphological quality of\nvocabularies, focusing on the overlap between these vocabularies and the\nmorphemes of a given language. Our evaluation method was built on three quality\nmeasures, relevance, cohesion, and morphological accuracy, and a procedure for\ntheir assessment. By applying this method to vocabularies created by three\nsubword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that\nthese vocabularies generally exhibit very low morphological quality. In this\narticle, we apply this evaluation to the tokenizer of BETO, a BERT language\nmodel trained on large Spanish corpora. This evaluation, along with our\nprevious results, helped us conclude that its vocabulary has a low\nmorphological quality, and we also found that training the tokenizer in a\nlarger corpus does not improve the morphological quality of the generated\nvocabulary. Additionally, this evaluation helps clarify the algorithm used by\nthe tokenizer, that is, Wordpiece, given the inconsistencies between the\nauthors' claims and the model's configuration.\n","authors":["Óscar García-Sierra","Ana Fernández-Pampillón Cesteros","Miguel Ortega-Martín"],"pdf_url":"https://arxiv.org/pdf/2410.02283v1.pdf","comment":"in Spanish language"},{"id":"http://arxiv.org/abs/2406.11341v2","updated":"2024-10-03T08:07:01Z","published":"2024-06-17T08:59:04Z","title":"A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences","summary":"  The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.\n","authors":["Leonardo Bertolazzi","Albert Gatt","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.11341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11016v2","updated":"2024-10-03T08:05:14Z","published":"2024-06-16T17:19:23Z","title":"Optimized Speculative Sampling for GPU Hardware Accelerators","summary":"  In this work, we optimize speculative sampling for parallel hardware\naccelerators to improve sampling speed. We notice that substantial portions of\nthe intermediate matrices necessary for speculative sampling can be computed\nconcurrently. This allows us to distribute the workload across multiple GPU\nthreads, enabling simultaneous operations on matrix segments within thread\nblocks. This results in profiling time improvements ranging from 6% to 13%\nrelative to the baseline implementation, without compromising accuracy. To\nfurther accelerate speculative sampling, probability distributions\nparameterized by softmax are approximated by sigmoid. This approximation\napproach results in significantly greater relative improvements in profiling\ntime, ranging from 37% to 94%, with a minor decline in accuracy. We conduct\nextensive experiments on both automatic speech recognition and summarization\ntasks to validate the effectiveness of our optimization methods.\n","authors":["Dominik Wagner","Seanie Lee","Ilja Baumann","Philipp Seeberger","Korbinian Riedhammer","Tobias Bocklet"],"pdf_url":"https://arxiv.org/pdf/2406.11016v2.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02281v1","updated":"2024-10-03T08:03:40Z","published":"2024-10-03T08:03:40Z","title":"Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition","summary":"  The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.\n","authors":["Arthur Amalvy","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2410.02281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19335v2","updated":"2024-10-03T07:59:36Z","published":"2024-04-30T08:01:49Z","title":"StablePT: Towards Stable Prompting for Few-shot Learning via Input\n  Separation","summary":"  Large language models have shown their ability to become effective few-shot\nlearners with prompting, revolutionizing the paradigm of learning with data\nscarcity. However, this approach largely depends on the quality of prompt\ninitialization, and always exhibits large variability among different runs.\nSuch property makes prompt tuning highly unreliable and vulnerable to poorly\nconstructed prompts, which limits its extension to more real-world\napplications. To tackle this issue, we propose to treat the hard prompt and\nsoft prompt as separate inputs to mitigate noise brought by the prompt\ninitialization. Furthermore, we optimize soft prompts with contrastive learning\nfor utilizing class-aware information in the training process to maintain model\nperformance. Experimental results demonstrate that \\sysname outperforms\nstate-of-the-art methods by 6.97% in accuracy and reduces the standard\ndeviation by 1.92 on average. Furthermore, extensive experiments underscore its\nrobustness and stability across 8 datasets covering various tasks. Codes are\navailable at https://github.com/lccc0528/Stable/tree/main.\n","authors":["Xiaoming Liu","Chen Liu","Zhaohan Zhang","Chengzhengxu Li","Longtian Wang","Yu Lan","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2404.19335v2.pdf","comment":"EMNLP 2024 Findings"}]},"2024-10-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.05269v1","updated":"2024-10-07T17:59:58Z","published":"2024-10-07T17:59:58Z","title":"Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models","summary":"  Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.\n","authors":["Fei Wang","Ninareh Mehrabi","Palash Goyal","Rahul Gupta","Kai-Wei Chang","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2410.05269v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/"},{"id":"http://arxiv.org/abs/2410.05267v1","updated":"2024-10-07T17:59:48Z","published":"2024-10-07T17:59:48Z","title":"Grounding Partially-Defined Events in Multimodal Data","summary":"  How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.\n","authors":["Kate Sanders","Reno Kriz","David Etter","Hannah Recknor","Alexander Martin","Cameron Carpenter","Jingyang Lin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.05267v1.pdf","comment":"Preprint; 9 pages; 2024 EMNLP Findings"},{"id":"http://arxiv.org/abs/2406.11839v2","updated":"2024-10-07T17:59:42Z","published":"2024-06-17T17:59:58Z","title":"mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models","summary":"  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n","authors":["Fei Wang","Wenxuan Zhou","James Y. Huang","Nan Xu","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11839v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO"},{"id":"http://arxiv.org/abs/2410.05265v1","updated":"2024-10-07T17:59:35Z","published":"2024-10-07T17:59:35Z","title":"PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs","summary":"  Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.\n","authors":["Mengzhao Chen","Yi Liu","Jiahao Wang","Yi Bin","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05265v1.pdf","comment":"A PTQ method to significantly boost the performance of static\n  activation quantization"},{"id":"http://arxiv.org/abs/2410.05262v1","updated":"2024-10-07T17:58:47Z","published":"2024-10-07T17:58:47Z","title":"TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles","summary":"  As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"\n","authors":["Qingchen Yu","Shichao Song","Ke Fang","Yunfeng Shi","Zifan Zheng","Hanyu Wang","Simin Niu","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.05262v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2410.05258v1","updated":"2024-10-07T17:57:38Z","published":"2024-10-07T17:57:38Z","title":"Differential Transformer","summary":"  Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.\n","authors":["Tianzhu Ye","Li Dong","Yuqing Xia","Yutao Sun","Yi Zhu","Gao Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.05258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05254v1","updated":"2024-10-07T17:55:35Z","published":"2024-10-07T17:55:35Z","title":"GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments","summary":"  Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.\n","authors":["Eilam Shapira","Omer Madmon","Itamar Reinman","Samuel Joseph Amouyal","Roi Reichart","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2410.05254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05252v1","updated":"2024-10-07T17:55:10Z","published":"2024-10-07T17:55:10Z","title":"Causal Micro-Narratives","summary":"  We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.\n","authors":["Mourad Heddaya","Qingcheng Zeng","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2410.05252v1.pdf","comment":"Accepted to EMNLP 2024 Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2410.05248v1","updated":"2024-10-07T17:52:21Z","published":"2024-10-07T17:52:21Z","title":"SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe","summary":"  To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.\n","authors":["Yuxin Xiao","Shujian Zhang","Wenxuan Zhou","Marzyeh Ghassemi","Sanqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.05248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17975v2","updated":"2024-10-07T17:49:13Z","published":"2024-06-25T23:12:07Z","title":"SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)","summary":"  Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.\n","authors":["Matthieu Meeus","Igor Shilov","Shubham Jain","Manuel Faysse","Marek Rei","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2406.17975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05243v1","updated":"2024-10-07T17:47:50Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly take\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01574v5","updated":"2024-10-07T17:46:08Z","published":"2024-06-03T17:53:00Z","title":"MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark (Published at NeurIPS 2024 Track Datasets and\n  Benchmarks)","summary":"  In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.\n","authors":["Yubo Wang","Xueguang Ma","Ge Zhang","Yuansheng Ni","Abhranil Chandra","Shiguang Guo","Weiming Ren","Aaran Arulraj","Xuan He","Ziyan Jiang","Tianle Li","Max Ku","Kai Wang","Alex Zhuang","Rongqi Fan","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01574v5.pdf","comment":"This version has been accepted and published at NeurIPS 2024 Track\n  Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2410.05239v1","updated":"2024-10-07T17:42:53Z","published":"2024-10-07T17:42:53Z","title":"TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models","summary":"  Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.\n","authors":["Rabin Adhikari","Safal Thapaliya","Manish Dhakal","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2410.05239v1.pdf","comment":"Accepted at ACCV 2024 (oral presentation)"},{"id":"http://arxiv.org/abs/2410.05235v1","updated":"2024-10-07T17:41:45Z","published":"2024-10-07T17:41:45Z","title":"CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with\n  Explanatory Argumentative Structures","summary":"  Explaining Artificial Intelligence (AI) decisions is a major challenge\nnowadays in AI, in particular when applied to sensitive scenarios like medicine\nand law. However, the need to explain the rationale behind decisions is a main\nissue also for human-based deliberation as it is important to justify\n\\textit{why} a certain decision has been taken. Resident medical doctors for\ninstance are required not only to provide a (possibly correct) diagnosis, but\nalso to explain how they reached a certain conclusion. Developing new tools to\naid residents to train their explanation skills is therefore a central\nobjective of AI in education. In this paper, we follow this direction, and we\npresent, to the best of our knowledge, the first multilingual dataset for\nMedical Question Answering where correct and incorrect diagnoses for a clinical\ncase are enriched with a natural language explanation written by doctors. These\nexplanations have been manually annotated with argument components (i.e.,\npremise, claim) and argument relations (i.e., attack, support), resulting in\nthe Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases\nin four languages (English, Spanish, French, Italian) with explanations, where\nwe annotated 5021 claims, 2313 premises, 2431 support relations, and 1106\nattack relations. We conclude by showing how competitive baselines perform over\nthis challenging dataset for the argument mining task.\n","authors":["katerina Sviridova","Anar Yeginbergen","Ainara Estarrona","Elena Cabrio","Serena Villata","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.05235v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.05224v1","updated":"2024-10-07T17:29:40Z","published":"2024-10-07T17:29:40Z","title":"Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates","summary":"  Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.\n","authors":["Avanika Narayan","Mayee F. Chen","Kush Bhatia","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2410.05224v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2410.05222v1","updated":"2024-10-07T17:26:31Z","published":"2024-10-07T17:26:31Z","title":"Precise Model Benchmarking with Only a Few Observations","summary":"  How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.\n","authors":["Riccardo Fogliato","Pratik Patil","Nil-Jana Akpinar","Mathew Monfort"],"pdf_url":"https://arxiv.org/pdf/2410.05222v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.15877v3","updated":"2024-10-07T17:23:30Z","published":"2024-06-22T15:52:04Z","title":"BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions","summary":"  Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.\n","authors":["Terry Yue Zhuo","Minh Chien Vu","Jenny Chim","Han Hu","Wenhao Yu","Ratnadira Widyasari","Imam Nur Bani Yusuf","Haolan Zhan","Junda He","Indraneil Paul","Simon Brunner","Chen Gong","Thong Hoang","Armel Randy Zebaze","Xiaoheng Hong","Wen-Ding Li","Jean Kaddour","Ming Xu","Zhihan Zhang","Prateek Yadav","Naman Jain","Alex Gu","Zhoujun Cheng","Jiawei Liu","Qian Liu","Zijian Wang","David Lo","Binyuan Hui","Niklas Muennighoff","Daniel Fried","Xiaoning Du","Harm de Vries","Leandro Von Werra"],"pdf_url":"https://arxiv.org/pdf/2406.15877v3.pdf","comment":"44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)"},{"id":"http://arxiv.org/abs/2410.05218v1","updated":"2024-10-07T17:22:56Z","published":"2024-10-07T17:22:56Z","title":"Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories","summary":"  Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.\n","authors":["Toni J. B. Liu","Nicolas Boullé","Raphaël Sarfati","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.05218v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2309.02233v3","updated":"2024-10-07T17:21:45Z","published":"2023-09-05T13:39:38Z","title":"Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)","summary":"  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.\n","authors":["Yubo Wang","Xueguang Ma","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.02233v3.pdf","comment":"This version has been accepted and published at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.05210v1","updated":"2024-10-07T17:16:20Z","published":"2024-10-07T17:16:20Z","title":"Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality","summary":"  In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.\n","authors":["Youngtaek Oh","Jae Won Cho","Dong-Jin Kim","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.05210v1.pdf","comment":"EMNLP 2024 (Long, Main). Project page:\n  https://ytaek-oh.github.io/fsc-clip"},{"id":"http://arxiv.org/abs/2406.06369v4","updated":"2024-10-07T17:13:45Z","published":"2024-06-10T15:30:13Z","title":"Annotation alignment: Comparing LLM and human annotations of\n  conversational safety","summary":"  Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.\n","authors":["Rajiv Movva","Pang Wei Koh","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2406.06369v4.pdf","comment":"EMNLP 2024 (Main). Main text contains 6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.05206v1","updated":"2024-10-07T17:09:03Z","published":"2024-10-07T17:09:03Z","title":"Studying and Mitigating Biases in Sign Language Understanding Models","summary":"  Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.\n","authors":["Katherine Atwell","Danielle Bragg","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.05206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05193v1","updated":"2024-10-07T16:50:47Z","published":"2024-10-07T16:50:47Z","title":"RevisEval: Improving LLM-as-a-Judge via Response-Adapted References","summary":"  With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.\n","authors":["Qiyuan Zhang","Yufei Wang","Tiezheng YU","Yuxin Jiang","Chuhan Wu","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Fuyuan Lyu","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05192v1","updated":"2024-10-07T16:49:39Z","published":"2024-10-07T16:49:39Z","title":"Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss\n  Landscape Perspective","summary":"  Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\nat any time with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.\n","authors":["Kaiyue Wen","Zhiyuan Li","Jason Wang","David Hall","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05192v1.pdf","comment":"45 pages,13 figures"},{"id":"http://arxiv.org/abs/2410.02525v2","updated":"2024-10-07T16:46:05Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00099v4","updated":"2024-10-07T16:45:42Z","published":"2024-04-30T18:00:02Z","title":"Creative Beam Search: LLM-as-a-Judge For Improving Response Generation","summary":"  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2405.00099v4.pdf","comment":"Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)"},{"id":"http://arxiv.org/abs/2410.05183v1","updated":"2024-10-07T16:42:10Z","published":"2024-10-07T16:42:10Z","title":"Beyond Correlation: Interpretable Evaluation of Machine Translation\n  Metrics","summary":"  Machine Translation (MT) evaluation metrics assess translation quality\nautomatically. Recently, researchers have employed MT metrics for various new\nuse cases, such as data filtering and translation re-ranking. However, most MT\nmetrics return assessments as scalar scores that are difficult to interpret,\nposing a challenge to making informed design choices. Moreover, MT metrics'\ncapabilities have historically been evaluated using correlation with human\njudgment, which, despite its efficacy, falls short of providing intuitive\ninsights into metric performance, especially in terms of new metric use cases.\nTo address these issues, we introduce an interpretable evaluation framework for\nMT metrics. Within this framework, we evaluate metrics in two scenarios that\nserve as proxies for the data filtering and translation re-ranking use cases.\nFurthermore, by measuring the performance of MT metrics using Precision,\nRecall, and F-score, we offer clearer insights into their capabilities than\ncorrelation with human judgments. Finally, we raise concerns regarding the\nreliability of manually curated data following the Direct Assessments+Scalar\nQuality Metrics (DA+SQM) guidelines, reporting a notably low agreement with\nMultidimensional Quality Metrics (MQM) annotations.\n","authors":["Stefano Perrella","Lorenzo Proietti","Pere-Lluís Huguet Cabot","Edoardo Barba","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05183v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference. 26 pages"},{"id":"http://arxiv.org/abs/2410.05180v1","updated":"2024-10-07T16:40:21Z","published":"2024-10-07T16:40:21Z","title":"Enhancing Equity in Large Language Models for Medical Applications","summary":"  Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups.\n","authors":["Yuelyu Ji","Wenhe Ma","Sonish Sivarajkumar","Hang Zhang","Eugene Mathew Sadhu","Zhuochun Li","Xizhi Wu","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02381v2","updated":"2024-10-07T16:39:24Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.14901v2","updated":"2024-10-07T16:38:35Z","published":"2024-02-22T18:09:33Z","title":"A Usage-centric Take on Intent Understanding in E-Commerce","summary":"  Identifying and understanding user intents is a pivotal task for E-Commerce.\nDespite its essential role in product recommendation and business user\nprofiling analysis, intent understanding has not been consistently defined or\naccurately benchmarked. In this paper, we focus on predicative user intents as\n\"how a customer uses a product\", and pose intent understanding as a natural\nlanguage reasoning task, independent of product ontologies. We identify two\nweaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:\ncategory-rigidity and property-ambiguity. They limit its ability to strongly\nalign user intents with products having the most desirable property, and to\nrecommend useful products across diverse categories. Following these\nobservations, we introduce a Product Recovery Benchmark featuring a novel\nevaluation framework and an example dataset. We further validate the above\nFolkScope weaknesses on this benchmark. Our code and dataset are available at\nhttps://github.com/stayones/Usgae-Centric-Intent-Understanding.\n","authors":["Wendi Zhou","Tianyi Li","Pavlos Vougiouklis","Mark Steedman","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2402.14901v2.pdf","comment":"Acepted by EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2310.09675v2","updated":"2024-10-07T16:28:52Z","published":"2023-10-14T22:24:26Z","title":"Efficient Model-Agnostic Multi-Group Equivariant Networks","summary":"  Constructing model-agnostic group equivariant networks, such as equitune\n(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be\ncomputationally expensive for large product groups. We address this problem by\nproviding efficient model-agnostic equivariant designs for two related\nproblems: one where the network has multiple inputs each with potentially\ndifferent groups acting on them, and another where there is a single input but\nthe group acting on it is a large product group. For the first design, we\ninitially consider a linear model and characterize the entire equivariant space\nthat satisfies this constraint. This characterization gives rise to a novel\nfusion layer between different channels that satisfies an invariance-symmetry\n(IS) constraint, which we call an IS layer. We then extend this design beyond\nlinear models, similar to equitune, consisting of equivariant and IS layers. We\nalso show that the IS layer is a universal approximator of invariant-symmetric\nfunctions. Inspired by the first design, we use the notion of the IS property\nto design a second efficient model-agnostic equivariant design for large\nproduct groups acting on a single input. For the first design, we provide\nexperiments on multi-image classification where each view is transformed\nindependently with transformations such as rotations. We find equivariant\nmodels are robust to such transformations and perform competitively otherwise.\nFor the second design, we consider three applications: language\ncompositionality on the SCAN dataset to product groups; fairness in natural\nlanguage generation from GPT-2 to address intersectionality; and robust\nzero-shot image classification with CLIP. Overall, our methods are simple and\ngeneral, competitive with equitune and its variants, while also being\ncomputationally more efficient.\n","authors":["Razan Baltaji","Sourya Basu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.09675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v2","updated":"2024-10-07T16:26:00Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05168v1","updated":"2024-10-07T16:25:39Z","published":"2024-10-07T16:25:39Z","title":"ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation","summary":"  Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.05168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02902v2","updated":"2024-10-07T16:25:04Z","published":"2024-10-03T18:48:38Z","title":"Better Instruction-Following Through Minimum Bayes Risk","summary":"  General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.\n","authors":["Ian Wu","Patrick Fernandes","Amanda Bertsch","Seungone Kim","Sina Pakazad","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.02902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05165v1","updated":"2024-10-07T16:23:36Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05162v1","updated":"2024-10-07T16:14:47Z","published":"2024-10-07T16:14:47Z","title":"Deciphering the Interplay of Parametric and Non-parametric Memory in\n  Retrieval-augmented Language Models","summary":"  Generative language models often struggle with specialized or less-discussed\nknowledge. A potential solution is found in Retrieval-Augmented Generation\n(RAG) models which act like retrieving information before generating responses.\nIn this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides\nbetween what it already knows (parametric) and what it retrieves\n(non-parametric). We use causal mediation analysis and controlled experiments\nto examine how internal representations influence information processing. Our\nfindings disentangle the effects of parametric knowledge and the retrieved\ncontext. They indicate that in cases where the model can choose between both\ntypes of information (parametric and non-parametric), it relies more on the\ncontext than the parametric knowledge. Furthermore, the analysis investigates\nthe computations involved in \\emph{how} the model uses the information from the\ncontext. We find that multiple mechanisms are active within the model and can\nbe detected with mediation analysis: first, the decision of \\emph{whether the\ncontext is relevant}, and second, how the encoder computes output\nrepresentations to support copying when relevant.\n","authors":["Mehrdad Farahani","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2410.05162v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05160v1","updated":"2024-10-07T16:14:05Z","published":"2024-10-07T16:14:05Z","title":"VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks","summary":"  Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.\n","authors":["Ziyan Jiang","Rui Meng","Xinyi Yang","Semih Yavuz","Yingbo Zhou","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05160v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2405.14577v2","updated":"2024-10-07T16:01:49Z","published":"2024-05-23T13:51:55Z","title":"Representation noising effectively prevents harmful fine-tuning on LLMs","summary":"  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","Łukasz Bartoszcze","David Atanasov","Robie Gonzales","Subhabrata Majumdar","Carsten Maple","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2405.14577v2.pdf","comment":"Published in NeurIPs 2024"},{"id":"http://arxiv.org/abs/2311.09090v4","updated":"2024-10-07T16:01:06Z","published":"2023-11-15T16:35:59Z","title":"Social Bias Probing: Fairness Benchmarking for Language Models","summary":"  While the impact of social biases in language models has been recognized,\nprior methods for bias evaluation have been limited to binary association tests\non small datasets, limiting our understanding of bias complexities. This paper\nproposes a novel framework for probing language models for social biases by\nassessing disparate treatment, which involves treating individuals differently\naccording to their affiliation with a sensitive demographic group. We curate\nSoFa, a large-scale benchmark designed to address the limitations of existing\nfairness collections. SoFa expands the analysis beyond the binary comparison of\nstereotypical versus anti-stereotypical identities to include a diverse range\nof identities and stereotypes. Comparing our methodology with existing\nbenchmarks, we reveal that biases within language models are more nuanced than\nacknowledged, indicating a broader scope of encoded biases than previously\nrecognized. Benchmarking LMs on SoFa, we expose how identities expressing\ndifferent religions lead to the most pronounced disparate treatments across all\nmodels. Finally, our findings indicate that real-life adversities faced by\nvarious groups such as women and people with disabilities are mirrored in the\nbehavior of these models.\n","authors":["Marta Marchiori Manerba","Karolina Stańczak","Riccardo Guidotti","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2311.09090v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05146v1","updated":"2024-10-07T15:58:03Z","published":"2024-10-07T15:58:03Z","title":"CTC-GMM: CTC guided modality matching for fast and accurate streaming\n  speech translation","summary":"  Models for streaming speech translation (ST) can achieve high accuracy and\nlow latency if they're developed with vast amounts of paired audio in the\nsource language and written text in the target language. Yet, these text labels\nfor the target language are often pseudo labels due to the prohibitive cost of\nmanual ST data labeling. In this paper, we introduce a methodology named\nConnectionist Temporal Classification guided modality matching (CTC-GMM) that\nenhances the streaming ST model by leveraging extensive machine translation\n(MT) text data. This technique employs CTC to compress the speech sequence into\na compact embedding sequence that matches the corresponding text sequence,\nallowing us to utilize matched {source-target} language text pairs from the MT\ncorpora to refine the streaming ST model further. Our evaluations with FLEURS\nand CoVoST2 show that the CTC-GMM approach can increase translation accuracy\nrelatively by 13.9% and 6.4% respectively, while also boosting decoding speed\nby 59.7% on GPU.\n","authors":["Rui Zhao","Jinyu Li","Ruchao Fan","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2410.05146v1.pdf","comment":"Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)"},{"id":"http://arxiv.org/abs/2407.10930v2","updated":"2024-10-07T15:52:48Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.00126v2","updated":"2024-10-07T15:44:45Z","published":"2024-02-29T21:05:37Z","title":"FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition","summary":"  Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.\n","authors":["Xiaoqiang Wang","Lingfei Wu","Tengfei Ma","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00126v2.pdf","comment":"Accepted at EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2404.12132v2","updated":"2024-10-07T15:28:18Z","published":"2024-04-18T12:33:57Z","title":"Non-Invasive Suicide Risk Prediction Through Speech Analysis","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17385v2","updated":"2024-10-07T15:15:18Z","published":"2024-06-25T09:04:21Z","title":"Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance","summary":"  Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.\n","authors":["Manon Reusens","Philipp Borchert","Jochen De Weerdt","Bart Baesens"],"pdf_url":"https://arxiv.org/pdf/2406.17385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05257v2","updated":"2024-10-07T15:14:26Z","published":"2024-09-09T00:40:47Z","title":"UPCS: Unbiased Persona Construction for Dialogue Generation","summary":"  Narrative systems, such as dialogue and storytelling systems, often utilize\npersona profiles to enhance personalized interactions. Existing persona\nprofiles frequently exhibit biases, posing risks to system integrity and\nfairness. To address this, we introduce the UPCS framework, which categorizes\ncharacter descriptions into eight dimensions, including bias mitigation\nstrategies. Experimental results demonstrate UPCS's superiority in accuracy,\ndiversity, bias elimination, and user satisfaction, marking a significant\nadvancement in persona construction for reliable narrative systems.\n","authors":["Kuiyun Chen","Yanbin Wei"],"pdf_url":"https://arxiv.org/pdf/2409.05257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11472v3","updated":"2024-10-07T15:11:12Z","published":"2022-05-23T17:14:32Z","title":"Diversity Over Size: On the Effect of Sample and Topic Sizes for\n  Topic-Dependent Argument Mining Datasets","summary":"  The task of Argument Mining, that is extracting and classifying argument\ncomponents for a specific topic from large document sources, is an inherently\ndifficult task for machine learning models and humans alike, as large Argument\nMining datasets are rare and recognition of argument components requires expert\nknowledge. The task becomes even more difficult if it also involves stance\ndetection of retrieved arguments. In this work, we investigate the effect of\nArgument Mining dataset composition in few- and zero-shot settings. Our\nfindings show that, while fine-tuning is mandatory to achieve acceptable model\nperformance, using carefully composed training samples and reducing the\ntraining sample size by up to almost 90% can still yield 95% of the maximum\nperformance. This gain is consistent across three Argument Mining tasks on\nthree different datasets. We also publish a new dataset for future\nbenchmarking.\n","authors":["Benjamin Schiller","Johannes Daxenberger","Andreas Waldis","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2205.11472v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00161v2","updated":"2024-10-07T15:07:09Z","published":"2024-09-30T19:09:13Z","title":"KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head","summary":"  Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.\n","authors":["Isaac Rehg"],"pdf_url":"https://arxiv.org/pdf/2410.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15929v2","updated":"2024-10-07T15:01:48Z","published":"2024-02-24T23:16:57Z","title":"Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs","summary":"  Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.\n","authors":["Isha Chaudhary","Vedaant V. Jain","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2402.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05102v1","updated":"2024-10-07T15:01:29Z","published":"2024-10-07T15:01:29Z","title":"SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks","summary":"  Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.\n","authors":["Fenia Christopoulou","Ronald Cardenas","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05102v1.pdf","comment":"20 papges, 9 figures, 5 tables. Under Review"},{"id":"http://arxiv.org/abs/2406.16078v2","updated":"2024-10-07T15:01:26Z","published":"2024-06-23T11:11:46Z","title":"First Heuristic Then Rational: Dynamic Use of Heuristics in Language\n  Model Reasoning","summary":"  Multi-step reasoning instruction, such as chain-of-thought prompting, is\nwidely adopted to explore better language models (LMs) performance. We report\non the systematic strategy that LMs employ in such a multi-step reasoning\nprocess. Our controlled experiments reveal that LMs rely more heavily on\nheuristics, such as lexical overlap, in the earlier stages of reasoning, where\nmore reasoning steps remain to reach a goal. Conversely, their reliance on\nheuristics decreases as LMs progress closer to the final answer through\nmultiple reasoning steps. This suggests that LMs can backtrack only a limited\nnumber of future steps and dynamically combine heuristic strategies with\nrationale ones in tasks involving multi-step reasoning.\n","authors":["Yoichi Aoki","Keito Kudo","Tatsuki Kuribayashi","Shusaku Sone","Masaya Taniguchi","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2406.16078v2.pdf","comment":"This paper is accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05099v1","updated":"2024-10-07T14:55:20Z","published":"2024-10-07T14:55:20Z","title":"Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances","summary":"  Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.\n","authors":["Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2410.05099v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2410.02707v2","updated":"2024-10-07T14:46:11Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16671v7","updated":"2024-10-07T14:44:44Z","published":"2024-02-26T15:47:01Z","title":"StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding","summary":"  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.\n","authors":["Alex Zhuang","Ge Zhang","Tianyu Zheng","Xinrun Du","Junjie Wang","Weiming Ren","Stephen W. Huang","Jie Fu","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16671v7.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05085v1","updated":"2024-10-07T14:39:45Z","published":"2024-10-07T14:39:45Z","title":"Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification","summary":"  Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.\n","authors":["Jeremie Bogaert","Marie-Catherine de Marneffe","Antonin Descampe","Louis Escouflaire","Cedrick Fairon","Francois-Xavier Standaert"],"pdf_url":"https://arxiv.org/pdf/2410.05085v1.pdf","comment":"This paper is a faithful translation of a paper which was\n  peer-reviewed and published in the French journal Traitement Automatique des\n  Langues, n. 64"},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05080v1","updated":"2024-10-07T14:33:50Z","published":"2024-10-07T14:33:50Z","title":"ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery","summary":"  The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.\n","authors":["Ziru Chen","Shijie Chen","Yuting Ning","Qianheng Zhang","Boshi Wang","Botao Yu","Yifei Li","Zeyi Liao","Chen Wei","Zitong Lu","Vishal Dey","Mingyi Xue","Frazier N. Baker","Benjamin Burns","Daniel Adu-Ampratwum","Xuhui Huang","Xia Ning","Song Gao","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05080v1.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2410.05077v1","updated":"2024-10-07T14:31:43Z","published":"2024-10-07T14:31:43Z","title":"ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering","summary":"  Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.\n","authors":["Francesco Maria Molfese","Simone Conia","Riccardo Orlando","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05077v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.05076v1","updated":"2024-10-07T14:30:27Z","published":"2024-10-07T14:30:27Z","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention","summary":"  Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.\n","authors":["Lijie Yang","Zhihao Zhang","Zhuofu Chen","Zikun Li","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2410.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12034v2","updated":"2024-10-07T14:27:56Z","published":"2024-06-17T19:06:54Z","title":"Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts","summary":"  We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.\n","authors":["Junmo Kang","Leonid Karlinsky","Hongyin Luo","Zhen Wang","Jacob Hansen","James Glass","David Cox","Rameswar Panda","Rogerio Feris","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2406.12034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.02325v2","updated":"2024-10-07T14:26:18Z","published":"2021-12-04T13:18:12Z","title":"A Russian Jeopardy! Data Set for Question-Answering Systems","summary":"  Question answering (QA) is one of the most common NLP tasks that relates to\nnamed entity recognition, fact extraction, semantic search and some other\nfields. In industry, it is much appreciated in chatbots and corporate\ninformation systems. It is also a challenging task that attracted the attention\nof a very general audience at the quiz show Jeopardy! In this article we\ndescribe a Jeopardy!-like Russian QA data set collected from the official\nRussian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like\nquestions with 29,375 from the Russian analogue of Jeopardy! - \"Own Game\". We\nobserve its linguistic features and the related QA-task. We conclude about\nperspectives of a QA competition based on the data set collected from this\ndatabase.\n","authors":["Elena Mikhalkova"],"pdf_url":"https://arxiv.org/pdf/2112.02325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19745v2","updated":"2024-10-07T14:17:44Z","published":"2024-09-29T15:40:54Z","title":"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead","summary":"  Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability.\n","authors":["Tao Tan","Yining Qian","Ang Lv","Hongzhan Lin","Songhao Wu","Yongbo Wang","Feng Wang","Jingtong Wu","Xin Lu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19745v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.05052v1","updated":"2024-10-07T14:09:58Z","published":"2024-10-07T14:09:58Z","title":"Initialization of Large Language Models via Reparameterization to\n  Mitigate Loss Spikes","summary":"  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a\nfundamental issue in the pre-training of large language models. This paper\nsupposes that the non-uniformity of the norm of the parameters is one of the\ncauses of loss spikes. Here, in training of neural networks, the scale of the\ngradients is required to be kept constant throughout the layers to avoid the\nvanishing and exploding gradients problem. However, to meet these requirements\nin the Transformer model, the norm of the model parameters must be non-uniform,\nand thus, parameters whose norm is smaller are more sensitive to the parameter\nupdate. To address this issue, we propose a novel technique, weight scaling as\nreparameterization (WeSaR). WeSaR introduces a gate parameter per parameter\nmatrix and adjusts it to the value satisfying the requirements. Because of the\ngate parameter, WeSaR sets the norm of the original parameters uniformly, which\nresults in stable training. Experimental results with the Transformer decoders\nconsisting of 130 million, 1.3 billion, and 13 billion parameters showed that\nWeSaR stabilizes and accelerates training and that it outperformed compared\nmethods including popular initialization methods.\n","authors":["Kosuke Nishida","Kyosuke Nishida","Kuniko Saito"],"pdf_url":"https://arxiv.org/pdf/2410.05052v1.pdf","comment":"EMNLP2024 accepted"},{"id":"http://arxiv.org/abs/2406.12058v4","updated":"2024-10-07T14:08:13Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v4.pdf","comment":"Accepted in BlackboxNLP @ EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05047v1","updated":"2024-10-07T14:01:20Z","published":"2024-10-07T14:01:20Z","title":"A test suite of prompt injection attacks for LLM-based machine\n  translation","summary":"  LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.\n","authors":["Antonio Valerio Miceli-Barone","Zhifan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05046v1","updated":"2024-10-07T14:00:18Z","published":"2024-10-07T14:00:18Z","title":"Named Clinical Entity Recognition Benchmark","summary":"  This technical report introduces a Named Clinical Entity Recognition\nBenchmark for evaluating language models in healthcare, addressing the crucial\nnatural language processing (NLP) task of extracting structured information\nfrom clinical narratives to support applications like automated coding,\nclinical trial cohort identification, and clinical decision support.\n  The leaderboard provides a standardized platform for assessing diverse\nlanguage models, including encoder and decoder architectures, on their ability\nto identify and classify clinical entities across multiple medical domains. A\ncurated collection of openly available clinical datasets is utilized,\nencompassing entities such as diseases, symptoms, medications, procedures, and\nlaboratory measurements. Importantly, these entities are standardized according\nto the Observational Medical Outcomes Partnership (OMOP) Common Data Model,\nensuring consistency and interoperability across different healthcare systems\nand datasets, and a comprehensive evaluation of model performance. Performance\nof models is primarily assessed using the F1-score, and it is complemented by\nvarious assessment modes to provide comprehensive insights into model\nperformance. The report also includes a brief analysis of models evaluated to\ndate, highlighting observed trends and limitations.\n  By establishing this benchmarking framework, the leaderboard aims to promote\ntransparency, facilitate comparative analyses, and drive innovation in clinical\nentity recognition tasks, addressing the need for robust evaluation methods in\nhealthcare NLP.\n","authors":["Wadood M Abdul","Marco AF Pimentel","Muhammad Umar Salman","Tathagata Raha","Clément Christophe","Praveen K Kanithi","Nasir Hayat","Ronnie Rajan","Shadab Khan"],"pdf_url":"https://arxiv.org/pdf/2410.05046v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05045v1","updated":"2024-10-07T14:00:08Z","published":"2024-10-07T14:00:08Z","title":"Can LLMs plan paths with extra hints from solvers?","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.\n","authors":["Erik Wu","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2410.05045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05021v1","updated":"2024-10-07T13:24:24Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15160v2","updated":"2024-10-07T13:19:53Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","authors":["Gilad Yehudai","Haim Kaplan","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2407.15160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05018v1","updated":"2024-10-07T13:19:08Z","published":"2024-10-07T13:19:08Z","title":"On the Biased Assessment of Expert Finding Systems","summary":"  In large organisations, identifying experts on a given topic is crucial in\nleveraging the internal knowledge spread across teams and departments.\nSo-called enterprise expert retrieval systems automatically discover and\nstructure employees' expertise based on the vast amount of heterogeneous data\navailable about them and the work they perform. Evaluating these systems\nrequires comprehensive ground truth expert annotations, which are hard to\nobtain. Therefore, the annotation process typically relies on automated\nrecommendations of knowledge areas to validate. This case study provides an\nanalysis of how these recommendations can impact the evaluation of expert\nfinding systems. We demonstrate on a popular benchmark that system-validated\nannotations lead to overestimated performance of traditional term-based\nretrieval models and even invalidate comparisons with more recent neural\nmethods. We also augment knowledge areas with synonyms to uncover a strong bias\ntowards literal mentions of their constituent words. Finally, we propose\nconstraints to the annotation process to prevent these biased evaluations, and\nshow that this still allows annotation suggestions of high utility. These\nfindings should inform benchmark creation or selection for expert finding, to\nguarantee meaningful comparison of methods.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Chris Develder","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2410.05018v1.pdf","comment":"Accepted to the 4th Workshop on Recommender Systems for Human\n  Resources (RecSys in HR 2024) as part of RecSys 2024"},{"id":"http://arxiv.org/abs/2402.18376v2","updated":"2024-10-07T13:17:03Z","published":"2024-02-28T14:52:15Z","title":"Tokenization Is More Than Compression","summary":"  Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n","authors":["Craig W. Schmidt","Varshini Reddy","Haoran Zhang","Alec Alameddine","Omri Uzan","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2402.18376v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05006v1","updated":"2024-10-07T13:05:26Z","published":"2024-10-07T13:05:26Z","title":"SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness","summary":"  Accurately modeling the relationships between skills is a crucial part of\nhuman resources processes such as recruitment and employee development. Yet, no\nbenchmarks exist to evaluate such methods directly. We construct and release\nSkillMatch, a benchmark for the task of skill relatedness, based on expert\nknowledge mining from millions of job ads. Additionally, we propose a scalable\nself-supervised learning technique to adapt a Sentence-BERT model based on\nskill co-occurrence in job ads. This new method greatly surpasses traditional\nmodels for skill relatedness as measured on SkillMatch. By releasing SkillMatch\npublicly, we aim to contribute a foundation for research towards increased\naccuracy and transparency of skill-based recommendation systems.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Thomas Demeester","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2410.05006v1.pdf","comment":"Accepted to the International workshop on AI for Human Resources and\n  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2406.04866v2","updated":"2024-10-07T12:32:24Z","published":"2024-06-07T12:01:59Z","title":"ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question\n  Answering","summary":"  We introduce ComplexTempQA, a large-scale dataset consisting of over 100\nmillion question-answer pairs designed to tackle the challenges in temporal\nquestion answering. ComplexTempQA significantly surpasses existing benchmarks\nlike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from\nWikipedia and Wikidata, the dataset covers questions spanning over two decades\nand offers an unmatched breadth of topics. We introduce a unique taxonomy that\ncategorizes questions as attributes, comparisons, and counting questions, each\nrevolving around events, entities, and time periods. One standout feature of\nComplexTempQA is the high complexity of its questions, which demand effective\ncapabilities for answering such as across-time comparison, temporal\naggregation, and multi-hop reasoning involving temporal event ordering and\nentity recognition. Additionally, each question is accompanied by detailed\nmetadata, including specific time scopes, allowing for comprehensive evaluation\nand enhancement of the temporal reasoning abilities of large language models.\nComplexTempQA serves both as a testing ground for developing sophisticated AI\nmodels and as a foundation for advancing research in question answering,\ninformation retrieval, and language understanding.\n","authors":["Raphael Gruber","Abdelrahman Abdallah","Michael Färber","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2406.04866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04981v1","updated":"2024-10-07T12:22:06Z","published":"2024-10-07T12:22:06Z","title":"On the Rigour of Scientific Writing: Criteria, Analysis, and Insights","summary":"  Rigour is crucial for scientific research as it ensures the reproducibility\nand validity of results and findings. Despite its importance, little work\nexists on modelling rigour computationally, and there is a lack of analysis on\nwhether these criteria can effectively signal or measure the rigour of\nscientific papers in practice. In this paper, we introduce a bottom-up,\ndata-driven framework to automatically identify and define rigour criteria and\nassess their relevance in scientific writing. Our framework includes rigour\nkeyword extraction, detailed rigour definition generation, and salient criteria\nidentification. Furthermore, our framework is domain-agnostic and can be\ntailored to the evaluation of scientific rigour for different areas,\naccommodating the distinct salient criteria across fields. We conducted\ncomprehensive experiments based on datasets collected from two high impact\nvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the\neffectiveness of our framework in modelling rigour. In addition, we analyse\nlinguistic patterns of rigour, revealing that framing certainty is crucial for\nenhancing the perception of scientific rigour, while suggestion certainty and\nprobability uncertainty diminish it.\n","authors":["Joseph James","Chenghao Xiao","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04981v1.pdf","comment":"Accepted Findings at EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.02987v2","updated":"2024-10-07T12:11:58Z","published":"2024-02-05T13:18:42Z","title":"Reconstruct Your Previous Conversations! Comprehensively Investigating\n  Privacy Leakage Risks in Conversations with GPT Models","summary":"  Significant advancements have recently been made in large language models\nrepresented by GPT models. Users frequently have multi-round private\nconversations with cloud-hosted GPT models for task optimization. Yet, this\noperational paradigm introduces additional attack surfaces, particularly in\ncustom GPTs and hijacked chat sessions. In this paper, we introduce a\nstraightforward yet potent Conversation Reconstruction Attack. This attack\ntargets the contents of previous conversations between GPT models and benign\nusers, i.e., the benign users' input contents during their interaction with GPT\nmodels. The adversary could induce GPT models to leak such contents by querying\nthem with designed malicious prompts. Our comprehensive examination of privacy\nrisks during the interactions with GPT models under this attack reveals GPT-4's\nconsiderable resilience. We present two advanced attacks targeting improved\nreconstruction of past conversations, demonstrating significant privacy leakage\nacross all models under these advanced techniques. Evaluating various defense\nmechanisms, we find them ineffective against these attacks. Our findings\nhighlight the ease with which privacy can be compromised in interactions with\nGPT models, urging the community to safeguard against potential abuses of these\nmodels' capabilities.\n","authors":["Junjie Chu","Zeyang Sha","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02987v2.pdf","comment":"Accepted in EMNLP 2024. 14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.19339v2","updated":"2024-10-07T12:05:55Z","published":"2024-09-28T12:49:16Z","title":"Visual Question Decomposition on Multimodal Large Language Models","summary":"  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n","authors":["Haowei Zhang","Jianzhe Liu","Zhen Han","Shuo Chen","Bailan He","Volker Tresp","Zhiqiang Xu","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2409.19339v2.pdf","comment":"Accepted to EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.04962v1","updated":"2024-10-07T12:01:32Z","published":"2024-10-07T12:01:32Z","title":"Activation Scaling for Steering and Interpreting Language Models","summary":"  Given the prompt \"Rome is in\", can we steer a language model to flip its\nprediction of an incorrect token \"France\" to a correct token \"Italy\" by only\nmultiplying a few relevant activation vectors with scalars? We argue that\nsuccessfully intervening on a model is a prerequisite for interpreting its\ninternal workings. Concretely, we establish a three-term objective: a\nsuccessful intervention should flip the correct with the wrong token and vice\nversa (effectiveness), and leave other tokens unaffected (faithfulness), all\nwhile being sparse (minimality). Using gradient-based optimization, this\nobjective lets us learn (and later evaluate) a specific kind of efficient and\ninterpretable intervention: activation scaling only modifies the signed\nmagnitude of activation vectors to strengthen, weaken, or reverse the steering\ndirections already encoded in the model. On synthetic tasks, this intervention\nperforms comparably with steering vectors in terms of effectiveness and\nfaithfulness, but is much more minimal allowing us to pinpoint interpretable\nmodel components. We evaluate activation scaling from different angles, compare\nperformance on different datasets, and make activation scalars a learnable\nfunction of the activation vectors themselves to generalize to varying-length\nprompts.\n","authors":["Niklas Stoehr","Kevin Du","Vésteinn Snæbjarnarson","Robert West","Ryan Cotterell","Aaron Schein"],"pdf_url":"https://arxiv.org/pdf/2410.04962v1.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.17023v2","updated":"2024-10-07T11:59:37Z","published":"2024-07-24T06:06:07Z","title":"DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models","summary":"  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.\n","authors":["Sara Vera Marjanović","Haeun Yu","Pepa Atanasova","Maria Maistro","Christina Lioma","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2407.17023v2.pdf","comment":"15 pages, 6 figures, Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.04185v2","updated":"2024-10-07T11:54:11Z","published":"2024-09-06T11:01:55Z","title":"Residual Stream Analysis with Multi-Layer SAEs","summary":"  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, SAEs are\nusually trained separately on each transformer layer, making it difficult to\nuse them to study how information flows across layers. To solve this problem,\nwe introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer. Given that the residual\nstream is understood to preserve information across layers, we expected MLSAE\nlatents to `switch on' at a token position and remain active at later layers.\nInterestingly, we find that individual latents are often active at a single\nlayer for a given token or prompt, but this layer may differ for different\ntokens or prompts. We quantify these phenomena by defining a distribution over\nlayers and considering its variance. We find that the variance of the\ndistributions of latent activations over layers is about two orders of\nmagnitude greater when aggregating over tokens compared with a single token.\nFor larger underlying models, the degree to which latents are active at\nmultiple layers increases, which is consistent with the fact that the residual\nstream activation vectors at adjacent layers become more similar. Finally, we\nrelax the assumption that the residual stream basis is the same at every layer\nby applying pre-trained tuned-lens transformations, but our findings remain\nqualitatively similar. Our results represent a new approach to understanding\nhow representations change as they flow through transformers. We release our\ncode to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.\n","authors":["Tim Lawson","Lucy Farnik","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2409.04185v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2407.10805v4","updated":"2024-10-07T11:52:50Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Cehao Yang","Jiaxin Mao","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14883v2","updated":"2024-10-07T11:37:44Z","published":"2024-04-23T10:09:46Z","title":"Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans","summary":"  Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.\n","authors":["Vittoria Dentella","Fritz Guenther","Evelina Leivada"],"pdf_url":"https://arxiv.org/pdf/2404.14883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04925v1","updated":"2024-10-07T11:17:05Z","published":"2024-10-07T11:17:05Z","title":"Intent Classification for Bank Chatbots through LLM Fine-Tuning","summary":"  This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.\n","authors":["Bibiána Lajčinová","Patrik Valábek","Michal Spišiak"],"pdf_url":"https://arxiv.org/pdf/2410.04925v1.pdf","comment":"7 pages, no figures"},{"id":"http://arxiv.org/abs/2401.05930v4","updated":"2024-10-07T09:58:48Z","published":"2024-01-11T14:09:09Z","title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully","summary":"  Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks.\n","authors":["Jushi Kai","Tianhang Zhang","Hai Hu","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2401.05930v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.04878v1","updated":"2024-10-07T09:57:59Z","published":"2024-10-07T09:57:59Z","title":"Leveraging Grammar Induction for Language Understanding and Generation","summary":"  Grammar induction has made significant progress in recent years. However, it\nis not clear how the application of induced grammar could enhance practical\nperformance in downstream tasks. In this work, we introduce an unsupervised\ngrammar induction method for language understanding and generation. We\nconstruct a grammar parser to induce constituency structures and dependency\nrelations, which is simultaneously trained on downstream tasks without\nadditional syntax annotations. The induced grammar features are subsequently\nincorporated into Transformer as a syntactic mask to guide self-attention. We\nevaluate and apply our method to multiple machine translation tasks and natural\nlanguage understanding tasks. Our method demonstrates superior performance\ncompared to the original Transformer and other models enhanced with external\nparsers. Experimental results indicate that our method is effective in both\nfrom-scratch and pre-trained scenarios. Additionally, our research highlights\nthe contribution of explicitly modeling the grammatical structure of texts to\nneural network models.\n","authors":["Jushi Kai","Shengyuan Hou","Yusheng Huang","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04878v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.15625v2","updated":"2024-10-07T09:49:08Z","published":"2024-08-28T08:25:22Z","title":"CBF-LLM: Safe Control for LLM Alignment","summary":"  This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.\n","authors":["Yuya Miyaoka","Masaki Inoue"],"pdf_url":"https://arxiv.org/pdf/2408.15625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15489v2","updated":"2024-10-07T08:55:15Z","published":"2024-07-22T09:16:30Z","title":"A Comparison of Language Modeling and Translation as Multilingual\n  Pretraining Objectives","summary":"  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing best practices in\npretraining has, therefore, become a major focus of NLP research, especially\nsince insights gained from monolingual English models may not necessarily apply\nto more complex multilingual models. One significant caveat of the current\nstate of the art is that different works are rarely comparable: they often\ndiscuss different parameter counts, training data, and evaluation methodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pretraining objective\nunder the right conditions. We make our code, data, and model weights available\nat \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n","authors":["Zihao Li","Shaoxiong Ji","Timothee Mickus","Vincent Segonne","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2407.15489v2.pdf","comment":"Proceedings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.04838v1","updated":"2024-10-07T08:53:00Z","published":"2024-10-07T08:53:00Z","title":"Rationale-Aware Answer Verification by Pairwise Self-Evaluation","summary":"  Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.\n","authors":["Akira Kawabata","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2410.04838v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.00545v2","updated":"2024-10-07T08:52:39Z","published":"2024-10-01T09:38:34Z","title":"What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine\n  Translation with a Human-centered Study","summary":"  Gender bias in machine translation (MT) is recognized as an issue that can\nharm people and society. And yet, advancements in the field rarely involve\npeople, the final MT users, or inform how they might be impacted by biased\ntechnologies. Current evaluations are often restricted to automatic methods,\nwhich offer an opaque estimate of what the downstream impact of gender\ndisparities might be. We conduct an extensive human-centered study to examine\nif and to what extent bias in MT brings harms with tangible costs, such as\nquality of service gaps across women and men. To this aim, we collect\nbehavioral data from 90 participants, who post-edited MT outputs to ensure\ncorrect gender translation. Across multiple datasets, languages, and types of\nusers, our study shows that feminine post-editing demands significantly more\ntechnical and temporal effort, also corresponding to higher financial costs.\nExisting bias measurements, however, fail to reflect the found disparities. Our\nfindings advocate for human-centered approaches that can inform the societal\nimpact of bias.\n","authors":["Beatrice Savoldi","Sara Papi","Matteo Negri","Ana Guerberof","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2410.00545v2.pdf","comment":"Accepted ad EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.06551v2","updated":"2024-10-07T08:49:49Z","published":"2024-07-09T05:16:22Z","title":"OffsetBias: Leveraging Debiased Data for Tuning Evaluators","summary":"  Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public.\n","authors":["Junsoo Park","Seungyeon Jwa","Meiying Ren","Daeyoung Kim","Sanghyuk Choi"],"pdf_url":"https://arxiv.org/pdf/2407.06551v2.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2408.08313v2","updated":"2024-10-07T08:44:35Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v2.pdf","comment":"Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)"},{"id":"http://arxiv.org/abs/2410.04834v1","updated":"2024-10-07T08:44:04Z","published":"2024-10-07T08:44:04Z","title":"As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss","summary":"  Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.\n","authors":["Xin Mao","Feng-Lin Li","Huimin Xu","Wei Zhang","Wang Chen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2410.04834v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.02298v2","updated":"2024-10-07T08:40:35Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.04819v1","updated":"2024-10-07T08:13:16Z","published":"2024-10-07T08:13:16Z","title":"MINER: Mining the Underlying Pattern of Modality-Specific Neurons in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have significantly\nadvanced, integrating more modalities into diverse applications. However, the\nlack of explainability remains a major barrier to their use in scenarios\nrequiring decision transparency. Current neuron-level explanation paradigms\nmainly focus on knowledge localization or language- and domain-specific\nanalyses, leaving the exploration of multimodality largely unaddressed. To\ntackle these challenges, we propose MINER, a transferable framework for mining\nmodality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)\nmodality separation, (2) importance score calculation, (3) importance score\naggregation, (4) modality-specific neuron selection. Extensive experiments\nacross six benchmarks and two representative MLLMs show that (I) deactivating\nONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for\nQwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly\nconverge in the lower layers, (III) MSNs influence how key information from\nvarious modalities converges to the last token, (IV) two intriguing phenomena\nworth further investigation, i.e., semantic probing and semantic telomeres. The\nsource code is available at this URL.\n","authors":["Kaichen Huang","Jiahao Huo","Yibo Yan","Kun Wang","Yutao Yue","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00016v3","updated":"2024-10-07T07:54:37Z","published":"2024-07-28T15:45:08Z","title":"Towards a Universal Method for Meaningful Signal Detection","summary":"  It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.\n","authors":["Louis Mahon"],"pdf_url":"https://arxiv.org/pdf/2408.00016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14399v2","updated":"2024-10-07T07:49:27Z","published":"2024-09-22T11:35:59Z","title":"Beyond Persuasion: Towards Conversational Recommender System with\n  Credible Explanations","summary":"  With the aid of large language models, current conversational recommender\nsystem (CRS) has gaining strong abilities to persuade users to accept\nrecommended items. While these CRSs are highly persuasive, they can mislead\nusers by incorporating incredible information in their explanations, ultimately\ndamaging the long-term trust between users and the CRS. To address this, we\npropose a simple yet effective method, called PC-CRS, to enhance the\ncredibility of CRS's explanations during persuasion. It guides the explanation\ngeneration through our proposed credibility-aware persuasive strategies and\nthen gradually refines explanations via post-hoc self-reflection. Experimental\nresults demonstrate the efficacy of PC-CRS in promoting persuasive and credible\nexplanations. Further analysis reveals the reason behind current methods\nproducing incredible explanations and the potential of credible explanations to\nimprove recommendation accuracy.\n","authors":["Peixin Qin","Chen Huang","Yang Deng","Wenqiang Lei","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14399v2.pdf","comment":"Findings of EMNLP 2024. Our code is available at\n  https://github.com/mumen798/PC-CRS"},{"id":"http://arxiv.org/abs/2407.01449v3","updated":"2024-10-07T07:46:00Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.04808v1","updated":"2024-10-07T07:41:48Z","published":"2024-10-07T07:41:48Z","title":"LPZero: Language Model Zero-cost Proxy Search from Zero","summary":"  In spite of the outstanding performance, Neural Architecture Search (NAS) is\ncriticized for massive computation. Recently, Zero-shot NAS has emerged as a\npromising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce\ncomputational demands. Despite this, existing ZC proxies heavily rely on expert\nknowledge and incur significant trial-and-error costs. Particularly in NLP\ntasks, most existing ZC proxies fail to surpass the performance of the naive\nbaseline. To address these challenges, we introduce a novel framework,\n\\textbf{LPZero}, which is the first to automatically design ZC proxies for\nvarious tasks, achieving higher ranking consistency than human-designed\nproxies. Specifically, we model the ZC proxy as a symbolic equation and\nincorporate a unified proxy search space that encompasses existing ZC proxies,\nwhich are composed of a predefined set of mathematical symbols. To\nheuristically search for the best ZC proxy, LPZero incorporates genetic\nprogramming to find the optimal symbolic composition. We propose a\n\\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates\nunpromising proxies, thereby mitigating the risk of proxy degradation.\nExtensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's\nsuperior ranking ability and performance on downstream tasks compared to\ncurrent approaches.\n","authors":["Peijie Dong","Lujun Li","Xiang Liu","Zhenheng Tang","Xuebo Liu","Qiang Wang","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2410.04808v1.pdf","comment":"8 pages, 7 figures, 10 appendix"},{"id":"http://arxiv.org/abs/2410.04798v1","updated":"2024-10-07T07:21:49Z","published":"2024-10-07T07:21:49Z","title":"DAPE V2: Process Attention Score as Feature Map for Length Extrapolation","summary":"  The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Jing Xiong","Jiankai Sun","Jingyao Li","Minbin Huang","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.04798v1.pdf","comment":"Tech Report. arXiv admin note: text overlap with arXiv:2405.14722"},{"id":"http://arxiv.org/abs/2410.04795v1","updated":"2024-10-07T07:14:37Z","published":"2024-10-07T07:14:37Z","title":"Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models","summary":"  The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs.\n","authors":["Dahyun Kim","Sukyung Lee","Yungi Kim","Attapol Rutherford","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.04795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"},{"id":"http://arxiv.org/abs/2410.04790v1","updated":"2024-10-07T07:02:09Z","published":"2024-10-07T07:02:09Z","title":"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA","summary":"  In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods.\n","authors":["Xinyu Wang","Yanzheng Xiang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2410.04790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07284v4","updated":"2024-10-07T06:54:30Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v4.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2410.04784v1","updated":"2024-10-07T06:49:41Z","published":"2024-10-07T06:49:41Z","title":"Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge","summary":"  Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data.\n","authors":["Jiahuan Li","Yiqing Cao","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04784v1.pdf","comment":"accepted by EMNLP 2024, main conference"},{"id":"http://arxiv.org/abs/2408.01084v2","updated":"2024-10-07T06:11:46Z","published":"2024-08-02T08:03:38Z","title":"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts","summary":"  When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge the gap between\nexternal knowledge and the LLMs' parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLMs\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.\n","authors":["Youna Kim","Hyuhng Joon Kim","Cheonbok Park","Choonghyun Park","Hyunsoo Cho","Junyeob Kim","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01084v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.05356v2","updated":"2024-10-07T05:29:01Z","published":"2024-09-09T06:28:47Z","title":"IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS","summary":"  Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.\n","authors":["Ashwin Sankar","Srija Anand","Praveen Srinivasa Varadhan","Sherry Thomas","Mehak Singal","Shridhar Kumar","Deovrat Mehendale","Aditi Krishana","Giri Raju","Mitesh Khapra"],"pdf_url":"https://arxiv.org/pdf/2409.05356v2.pdf","comment":"Accepted to NeurIPS 2024 Datasets and Benchmarks track"},{"id":"http://arxiv.org/abs/2407.17467v2","updated":"2024-10-07T05:16:25Z","published":"2024-07-24T17:59:02Z","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","summary":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","authors":["Jiawei Gu","Zacc Yang","Chuanghao Ding","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17467v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.04753v1","updated":"2024-10-07T05:14:18Z","published":"2024-10-07T05:14:18Z","title":"ImProver: Agent-Based Automated Proof Optimization","summary":"  Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.\n","authors":["Riyaz Ahuja","Jeremy Avigad","Prasad Tetali","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2410.04753v1.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.04752v1","updated":"2024-10-07T05:07:48Z","published":"2024-10-07T05:07:48Z","title":"Document-level Causal Relation Extraction with Knowledge-guided Binary\n  Question Answering","summary":"  As an essential task in information extraction (IE), Event-Event Causal\nRelation Extraction (ECRE) aims to identify and classify the causal\nrelationships between event mentions in natural language texts. However,\nexisting research on ECRE has highlighted two critical challenges, including\nthe lack of document-level modeling and causal hallucinations. In this paper,\nwe propose a Knowledge-guided binary Question Answering (KnowQA) method with\nevent structures for ECRE, consisting of two stages: Event Structure\nConstruction and Binary Question Answering. We conduct extensive experiments\nunder both zero-shot and fine-tuning settings with large language models (LLMs)\non the MECI and MAVEN-ERE datasets. Experimental results demonstrate the\nusefulness of event structures on document-level ECRE and the effectiveness of\nKnowQA by achieving state-of-the-art on the MECI dataset. We observe not only\nthe effectiveness but also the high generalizability and low inconsistency of\nour method, particularly when with complete event structures after fine-tuning\nthe models.\n","authors":["Zimu Wang","Lei Xia","Wei Wang","Xinya Du"],"pdf_url":"https://arxiv.org/pdf/2410.04752v1.pdf","comment":"Accepted at Findings of EMNLP 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2410.04751v1","updated":"2024-10-07T05:07:01Z","published":"2024-10-07T05:07:01Z","title":"Intriguing Properties of Large Language and Vision Models","summary":"  Recently, large language and vision models (LLVMs) have received significant\nattention and development efforts due to their remarkable generalization\nperformance across a wide range of tasks requiring perception and cognitive\nabilities. A key factor behind their success is their simple architecture,\nwhich consists of a vision encoder, a projector, and a large language model\n(LLM). Despite their achievements in advanced reasoning tasks, their\nperformance on fundamental perception-related tasks (e.g., MMVP) remains\nsurprisingly low. This discrepancy raises the question of how LLVMs truly\nperceive images and exploit the advantages of the vision encoder. To address\nthis, we systematically investigate this question regarding several aspects:\npermutation invariance, robustness, math reasoning, alignment preserving and\nimportance, by evaluating the most common LLVM's families (i.e., LLaVA) across\n10 evaluation benchmarks. Our extensive experiments reveal several intriguing\nproperties of current LLVMs: (1) they internally process the image in a global\nmanner, even when the order of visual patch sequences is randomly permuted; (2)\nthey are sometimes able to solve math problems without fully perceiving\ndetailed numerical information; (3) the cross-modal alignment is overfitted to\ncomplex reasoning tasks, thereby, causing them to lose some of the original\nperceptual capabilities of their vision encoder; (4) the representation space\nin the lower layers (<25%) plays a crucial role in determining performance and\nenhancing visual understanding. Lastly, based on the above observations, we\nsuggest potential future directions for building better LLVMs and constructing\nmore challenging evaluation benchmarks.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Yechan Hwang","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.04751v1.pdf","comment":"Code is available in https://github.com/passing2961/IP-LLVM"},{"id":"http://arxiv.org/abs/2405.14722v3","updated":"2024-10-07T04:35:58Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2304.09797v6","updated":"2024-10-07T04:28:04Z","published":"2023-04-19T16:29:48Z","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models","summary":"  The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted extensive and comprehensive experiments on seven benchmarks. The\nresults show that PHP significantly improves accuracy while remaining highly\nefficient. For instance, with text-davinci-003, we observed a 4.2% improvement\non GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction\nin sample paths with self-consistency. With GPT-4 and PHP, we achieve\nstate-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),\nAQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).\n","authors":["Chuanyang Zheng","Zhengying Liu","Enze Xie","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2304.09797v6.pdf","comment":"Accepted to ICML AI4MATH 2024"},{"id":"http://arxiv.org/abs/2403.19270v2","updated":"2024-10-07T04:21:15Z","published":"2024-03-28T09:56:04Z","title":"sDPO: Don't Use Your Data All at Once","summary":"  As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.\n","authors":["Dahyun Kim","Yungi Kim","Wonho Song","Hyeonwoo Kim","Yunsu Kim","Sanghoon Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2403.19270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13803v3","updated":"2024-10-07T04:18:40Z","published":"2024-05-22T16:30:24Z","title":"\"I Like Sunnie More Than I Expected!\": Exploring User Expectation and\n  Perception of an Anthropomorphic LLM-based Conversational Agent for\n  Well-Being Support","summary":"  The human-computer interaction (HCI) research community has a longstanding\ninterest in exploring the mismatch between users' actual experiences and\nexpectation toward new technologies, for instance, large language models\n(LLMs). In this study, we compared users' (N = 38) initial expectations against\ntheir post-interaction perceptions of two LLM-powered mental well-being\nintervention activity recommendation systems. Both systems have a built-in LLM\nto recommend a personalized well-being intervention activity, but one system\n(Sunnie) has an anthropomorphic conversational interaction design via elements\nsuch as appearance, persona, and natural conversation. Results showed that user\nengagement was high with both systems, and both systems exceeded users'\nexpectations along the utility dimension, highlighting AI's potential to offer\nuseful intervention activity recommendations. In addition, Sunnie further\noutperformed the non-anthropomorphic baseline system in relational warmth.\nThese findings suggest that anthropomorphic conversational interaction design\nmay be particularly effective in fostering warmth in mental health support\ncontexts.\n","authors":["Siyi Wu","Julie Y. A. Cachia","Feixue Han","Bingsheng Yao","Tianyi Xie","Xuan Zhao","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13803v3.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2408.07930v3","updated":"2024-10-07T04:17:18Z","published":"2024-08-15T04:57:55Z","title":"MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and\n  Iterative Sub-SQL Refinement for Text-to-SQL","summary":"  Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider.\n","authors":["Wenxuan Xie","Gaochen Wu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07930v3.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.04739v1","updated":"2024-10-07T04:15:02Z","published":"2024-10-07T04:15:02Z","title":"TableRAG: Million-Token Table Understanding with Language Models","summary":"  Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.\n","authors":["Si-An Chen","Lesly Miculicich","Julian Martin Eisenschlos","Zifeng Wang","Zilong Wang","Yanfei Chen","Yasuhisa Fujii","Hsuan-Tien Lin","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04739v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.04734v1","updated":"2024-10-07T04:00:22Z","published":"2024-10-07T04:00:22Z","title":"TLDR: Token-Level Detective Reward Model for Large Vision Language\n  Models","summary":"  Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. Finally, we show that TLDR models can significantly speed up human\nannotation by 3 times to acquire a broader range of high-quality vision\nlanguage data.\n","authors":["Deqing Fu","Tong Xiao","Rui Wang","Wang Zhu","Pengchuan Zhang","Guan Pang","Robin Jia","Lawrence Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04734v1.pdf","comment":"Work done at Meta"},{"id":"http://arxiv.org/abs/2409.06927v2","updated":"2024-10-07T03:56:35Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v2.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.17169v3","updated":"2024-10-07T03:48:18Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v3.pdf","comment":"Accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.04731v1","updated":"2024-10-07T03:47:34Z","published":"2024-10-07T03:47:34Z","title":"Efficient transformer with reinforced position embedding for language\n  models","summary":"  In this paper, we propose an efficient transformer architecture that uses\nreinforced positional embedding to obtain superior performance with half the\nnumber of encoder decoder layers. We demonstrate that concatenating positional\nencoding with trainable token embeddings, normalizing columns in the token\nembedding matrix, and using the normalized token embedding matrix as the value\nof the attention layer improve the training and validation loss and the\ntraining time in an encoder-decoder Transformer model for a Portuguese-English\ntranslation task with 10 epochs or 12 hours of training across 10 trials. Our\nmethod, with roughly a threefold parameter reduction compared to the baseline\nmodel, yields a mean training loss of 1.21, a mean validation loss of 1.51, and\nan average training time of 1352.27 seconds per epoch, surpassing the baseline\nmodel with the same embedding dimension that employs addition of positional\nencoding and token embeddings, which achieves a mean training loss of 1.96, a\nvalidation loss of 2.18, and an average training time of 4297.79 seconds per\nepoch. Additionally, we evaluated our proposed architecture and the baseline\nacross 14 diverse translation datasets from TensorFlow. The results indicate\nthat our method consistently achieves lower or comparable training and\nvalidation losses, suggesting enhanced learning efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2410.04731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16997v2","updated":"2024-10-07T03:41:57Z","published":"2024-07-24T04:39:24Z","title":"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective","summary":"  This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.\n","authors":["Yujian Liu","Yang Zhang","Tommi Jaakkola","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2407.16997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05621v2","updated":"2024-10-07T03:41:27Z","published":"2023-12-09T17:38:39Z","title":"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching","summary":"  Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.\n","authors":["Zhenting Qi","Xiaoyu Tan","Shaojie Shi","Chao Qu","Yinghui Xu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2312.05621v2.pdf","comment":"Accepted by EMNLP 2023 (Industry Track), Oral Presentation"},{"id":"http://arxiv.org/abs/2410.04727v1","updated":"2024-10-07T03:38:27Z","published":"2024-10-07T03:38:27Z","title":"Forgetting Curve: A Reliable Method for Evaluating Memorization\n  Capability for Long-context Models","summary":"  Numerous recent works target to extend effective context length for language\nmodels and various methods, tasks and benchmarks exist to measure model's\neffective memorization length. However, through thorough investigations, we\nfind limitations for currently existing evaluations on model's memorization\ncapability. We provide an extensive survey for limitations in this work and\npropose a new method called forgetting curve to measure the memorization\ncapability of long-context models. We show that forgetting curve has the\nadvantage of being robust to the tested corpus and the experimental settings,\nof not relying on prompts and can be applied to any model size.\n  We apply our forgetting curve to a large variety of models involving both\ntransformer and RNN/SSM based architectures. Our measurement provides empirical\nevidence for the effectiveness of transformer extension techniques while raises\nquestions for the effective length of RNN/SSM based models. We also examine the\ndifference between our measurement and existing benchmarks as well as popular\nmetrics for various models. Our code and results can be found at\nhttps://github.com/1azybug/ForgettingCurve.\n","authors":["Xinyu Liu","Runsong Zhao","Pengcheng Huang","Chunyang Xiao","Bei Li","Jingang Wang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.04727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17141v3","updated":"2024-10-07T03:19:16Z","published":"2024-03-25T19:28:10Z","title":"MetaAligner: Towards Generalizable Multi-Objective Alignment of Language\n  Models","summary":"  Recent advancements in large language models (LLMs) focus on aligning to\nheterogeneous human expectations and values via multi-objective preference\nalignment. However, existing methods are dependent on the policy model\nparameters, which require high-cost repetition of their alignment algorithms\nfor each new policy model, and they cannot expand to unseen objectives due to\ntheir static alignment objectives. In this work, we propose Meta-Objective\nAligner (MetaAligner), the first policy-agnostic and generalizable method for\nmulti-objective preference alignment. MetaAligner models multi-objective\nalignment into three stages: (1) dynamic objectives reformulation algorithm\nreorganizes traditional alignment datasets to supervise the model on performing\nflexible alignment across different objectives; (2) conditional weak-to-strong\ncorrection paradigm aligns the weak outputs of fixed policy models to approach\nstrong outputs with higher preferences in the corresponding alignment\nobjectives, enabling plug-and-play inferences on any policy models, which\nsignificantly reduces training costs and facilitates alignment on close-source\npolicy models; (3) generalizable inference method flexibly adjusts target\nobjectives by updating their text descriptions in the prompts, facilitating\ngeneralizable alignment to unseen objectives. Experimental results show that\nMetaAligner achieves significant and balanced improvements in multi-objective\nalignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU\ntraining hours compared to previous alignment methods. The model also\neffectively aligns unseen objectives, marking the first step towards\ngeneralizable multi-objective preference alignment.\n","authors":["Kailai Yang","Zhiwei Liu","Qianqian Xie","Jimin Huang","Tianlin Zhang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2403.17141v3.pdf","comment":"Accepted by NeurIPS 2024 main track"},{"id":"http://arxiv.org/abs/2410.04717v1","updated":"2024-10-07T03:15:11Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04715v1","updated":"2024-10-07T03:13:06Z","published":"2024-10-07T03:13:06Z","title":"Rule-based Data Selection for Large Language Models","summary":"  The quality of training data significantly impacts the performance of large\nlanguage models (LLMs). There are increasing studies using LLMs to rate and\nselect data based on several human-crafted metrics (rules). However, these\nconventional rule-based approaches often depend too heavily on human\nheuristics, lack effective metrics for assessing rules, and exhibit limited\nadaptability to new tasks. In our study, we introduce an innovative rule-based\nframework that utilizes the orthogonality of score vectors associated with\nrules as a novel metric for rule evaluations. Our approach includes an\nautomated pipeline that first uses LLMs to generate a diverse set of rules,\nencompassing various rating dimensions to evaluate data quality. Then it rates\na batch of data based on these rules and uses the determinantal point process\n(DPP) from random matrix theory to select the most orthogonal score vectors,\nthereby identifying a set of independent rules. These rules are subsequently\nused to evaluate all data, selecting samples with the highest average scores\nfor downstream tasks such as LLM training. We verify the effectiveness of our\nmethod through two experimental setups: 1) comparisons with ground truth\nratings and 2) benchmarking LLMs trained with the chosen data. Our\ncomprehensive experiments cover a range of scenarios, including general\npre-training and domain-specific fine-tuning in areas such as IMDB, Medical,\nMath, and Code. The outcomes demonstrate that our DPP-based rule rating method\nconsistently outperforms other approaches, including rule-free rating, uniform\nsampling, importance resampling, and QuRating, in terms of both rating\nprecision and model performance.\n","authors":["Xiaomin Li","Mingye Gao","Zhiwei Zhang","Chang Yue","Hong Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12327v3","updated":"2024-10-07T03:08:12Z","published":"2024-07-17T05:53:20Z","title":"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models","summary":"  Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\n\\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.\n","authors":["Ayush Kaushal","Tejas Vaidhya","Arnab Kumar Mondal","Tejas Pandey","Aaryan Bhagat","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2407.12327v3.pdf","comment":"42 pages, 21 figures, and 13 tables"},{"id":"http://arxiv.org/abs/2405.15984v3","updated":"2024-10-07T03:07:58Z","published":"2024-05-24T23:56:36Z","title":"Evaluating and Safeguarding the Adversarial Robustness of\n  Retrieval-Based In-Context Learning","summary":"  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl\n","authors":["Simon Yu","Jie He","Pasquale Minervini","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2405.15984v3.pdf","comment":"COLM 2024, 30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05328v2","updated":"2024-10-07T03:06:34Z","published":"2024-06-08T02:59:52Z","title":"FacLens: Transferable Probe for Foreseeing Non-Factuality in Large\n  Language Models","summary":"  Despite advancements in large language models (LLMs), non-factual responses\nremain prevalent. Unlike extensive studies on post-hoc detection of such\nresponses, this work studies non-factuality prediction (NFP), aiming to predict\nwhether an LLM will generate a non-factual response to a question before the\ngeneration process. Previous efforts on NFP have demonstrated LLMs' awareness\nof their internal knowledge, but they still face challenges in efficiency and\ntransferability. In this work, we propose a lightweight NFP model named\nFactuality Lens (FacLens), which effectively probes hidden representations of\nquestions for the NFP task. Besides, we discover that hidden question\nrepresentations sourced from different LLMs exhibit similar NFP patterns, which\nenables the transferability of FacLens across LLMs to reduce development costs.\nExtensive experiments highlight FacLens's superiority in both effectiveness and\nefficiency.\n","authors":["Yanling Wang","Haoyang Li","Hao Zou","Jing Zhang","Xinlei He","Qi Li","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2406.05328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v2","updated":"2024-10-07T03:01:01Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xiaolei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.20267v4","updated":"2024-10-07T02:53:44Z","published":"2024-05-30T17:19:19Z","title":"Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and\n  Committee Discussions","summary":"  As LLMs continuously evolve, there is an urgent need for a reliable\nevaluation method that delivers trustworthy results promptly. Currently, static\nbenchmarks suffer from inflexibility and unreliability, leading users to prefer\nhuman voting platforms like Chatbot Arena. However, human evaluations require\nsignificant manual effort. To address this, we propose the Auto-Arena, an\ninnovative framework that automates the entire evaluation process using\nLLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM\ncandidates engage in a multi-round peer battle based on individual questions,\naiming at revealing their true performance differences. Finally, a committee of\nLLM judges collaboratively discusses and decides the winner, reducing bias and\nenhancing fairness. During the peer battles, we observe intriguing scenarios\nwhere the LLM candidates display competitive behaviors and even learn from the\nopponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena\nshows a 92.14% correlation with human preferences, surpassing all previous\nexpert-annotated benchmarks without any manual efforts. As a result, Auto-Arena\noffers a promising alternative to current human evaluation platforms for\nevaluating LLMs automatically.\n","authors":["Ruochen Zhao","Wenxuan Zhang","Yew Ken Chia","Weiwen Xu","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2405.20267v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04707v1","updated":"2024-10-07T02:52:30Z","published":"2024-10-07T02:52:30Z","title":"Learning How Hard to Think: Input-Adaptive Allocation of LM Computation","summary":"  Computationally intensive decoding procedures--including search, reranking,\nand self-critique--can improve the quality of language model (LM) outputs in\nproblems spanning code generation, numerical reasoning, and dialog. Existing\nwork typically applies the same decoding procedure for every input to an LM.\nBut not all inputs require the same amount of computation to process. Can we\nallocate decoding computation adaptively, using more resources to answer\nquestions whose answers will be harder to compute? We present an approach that\npredicts the distribution of rewards given an input and computation budget,\nthen allocates additional computation to inputs for which it is predicted to be\nmost useful. We apply this approach in two decoding procedures: first, an\nadaptive best-of-k procedure that dynamically selects the number of samples to\ngenerate as input to a reranker; second, a routing procedure that dynamically\nresponds to a query using a decoding procedure that is expensive but accurate,\nor one that is cheaper but less capable. Across a suite of programming,\nmathematics, and dialog tasks, we show that accurate computation-allocation\nprocedures can be learned, and reduce computation by up to 50% at no cost to\nresponse quality, or improve quality by up to 10% at a fixed computational\nbudget.\n","authors":["Mehul Damani","Idan Shenfeld","Andi Peng","Andreea Bobu","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.04707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04704v1","updated":"2024-10-07T02:48:18Z","published":"2024-10-07T02:48:18Z","title":"Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model","summary":"  Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.\n","authors":["Kai Lia","Masato Akagia","Yongwei Lib","Masashi Unokia"],"pdf_url":"https://arxiv.org/pdf/2410.04704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00943v2","updated":"2024-10-07T02:47:36Z","published":"2024-04-01T06:03:39Z","title":"Evalverse: Unified and Accessible Library for Large Language Model\n  Evaluation","summary":"  This paper introduces Evalverse, a novel library that streamlines the\nevaluation of Large Language Models (LLMs) by unifying disparate evaluation\ntools into a single, user-friendly framework. Evalverse enables individuals\nwith limited knowledge of artificial intelligence to easily request LLM\nevaluations and receive detailed reports, facilitated by an integration with\ncommunication platforms like Slack. Thus, Evalverse serves as a powerful tool\nfor the comprehensive assessment of LLMs, offering both researchers and\npractitioners a centralized and easily accessible evaluation framework.\nFinally, we also provide a demo video for Evalverse, showcasing its\ncapabilities and implementation in a two-minute format.\n","authors":["Jihoo Kim","Wonho Song","Dahyun Kim","Yunsu Kim","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.00943v2.pdf","comment":"Accepted to EMNLP 2024 Demo Track"},{"id":"http://arxiv.org/abs/2410.04699v1","updated":"2024-10-07T02:30:18Z","published":"2024-10-07T02:30:18Z","title":"The LLM Effect: Are Humans Truly Using LLMs, or Are They Being\n  Influenced By Them Instead?","summary":"  Large Language Models (LLMs) have shown capabilities close to human\nperformance in various analytical tasks, leading researchers to use them for\ntime and labor-intensive analyses. However, their capability to handle highly\nspecialized and open-ended tasks in domains like policy studies remains in\nquestion. This paper investigates the efficiency and accuracy of LLMs in\nspecialized tasks through a structured user study focusing on Human-LLM\npartnership. The study, conducted in two stages-Topic Discovery and Topic\nAssignment-integrates LLMs with expert annotators to observe the impact of LLM\nsuggestions on what is usually human-only analysis. Results indicate that\nLLM-generated topic lists have significant overlap with human generated topic\nlists, with minor hiccups in missing document-specific topics. However, LLM\nsuggestions may significantly improve task completion speed, but at the same\ntime introduce anchoring bias, potentially affecting the depth and nuance of\nthe analysis, raising a critical question about the trade-off between increased\nefficiency and the risk of biased analysis.\n","authors":["Alexander S. Choi","Syeda Sabrina Akter","JP Singh","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.04699v1.pdf","comment":"Accepted to EMNLP Main 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2410.04698v1","updated":"2024-10-07T02:30:07Z","published":"2024-10-07T02:30:07Z","title":"MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning\n  in LLMs","summary":"  Recent large language models (LLMs) have demonstrated versatile capabilities\nin long-context scenarios. Although some recent benchmarks have been developed\nto evaluate the long-context capabilities of LLMs, there is a lack of\nbenchmarks evaluating the mathematical reasoning abilities of LLMs over long\ncontexts, which is crucial for LLMs' application in real-world scenarios. In\nthis paper, we introduce MathHay, an automated benchmark designed to assess the\nlong-context mathematical reasoning capabilities of LLMs. Unlike previous\nbenchmarks like Needle in a Haystack, which focus primarily on information\nretrieval within long texts, MathHay demands models with both\ninformation-seeking and complex mathematical reasoning abilities. We conduct\nextensive experiments on MathHay to assess the long-context mathematical\nreasoning abilities of eight top-performing LLMs. Even the best-performing\nmodel, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over\nlong contexts, achieving only 51.26% accuracy at 128K tokens. This highlights\nthe significant room for improvement on the MathHay benchmark.\n","authors":["Lei Wang","Shan Dong","Yuhui Xu","Hanze Dong","Yalu Wang","Amrita Saha","Ee-Peng Lim","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.04698v1.pdf","comment":"Work-in-Progress"},{"id":"http://arxiv.org/abs/2401.15884v3","updated":"2024-10-07T02:19:21Z","published":"2024-01-29T04:36:39Z","title":"Corrective Retrieval Augmented Generation","summary":"  Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.\n","authors":["Shi-Qi Yan","Jia-Chen Gu","Yun Zhu","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2401.15884v3.pdf","comment":"Update results, add more analysis, and fix typos"},{"id":"http://arxiv.org/abs/2410.04691v1","updated":"2024-10-07T02:12:22Z","published":"2024-10-07T02:12:22Z","title":"Deeper Insights Without Updates: The Power of In-Context Learning Over\n  Fine-Tuning","summary":"  Fine-tuning and in-context learning (ICL) are two prevalent methods in\nimbuing large language models with task-specific knowledge. It is commonly\nbelieved that fine-tuning can surpass ICL given sufficient training samples as\nit allows the model to adjust its internal parameters based on the data.\nHowever, this paper presents a counterintuitive finding: For tasks with\nimplicit patterns, ICL captures these patterns significantly better than\nfine-tuning. We developed several datasets featuring implicit patterns, such as\nsequences determining answers through parity or identifying reducible terms in\ncalculations. We then evaluated the models' understanding of these patterns\nunder both fine-tuning and ICL across models ranging from 0.5B to 7B\nparameters. The results indicate that models employing ICL can quickly grasp\ndeep patterns and significantly improve accuracy. In contrast, fine-tuning,\ndespite utilizing thousands of times more training samples than ICL, achieved\nonly limited improvements. We also proposed circuit shift theory from a\nmechanistic interpretability's view to explain why ICL wins.\n","authors":["Qingyu Yin","Xuzheng He","Luoao Deng","Chak Tou Leong","Fan Wang","Yanzhao Yan","Xiaoyu Shen","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04691v1.pdf","comment":"EMNLP'24 Findings"},{"id":"http://arxiv.org/abs/2406.08464v2","updated":"2024-10-07T01:45:38Z","published":"2024-06-12T17:52:30Z","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing","summary":"  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n","authors":["Zhangchen Xu","Fengqing Jiang","Luyao Niu","Yuntian Deng","Radha Poovendran","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2406.08464v2.pdf","comment":"Link: https://magpie-align.github.io/"},{"id":"http://arxiv.org/abs/2405.16406v3","updated":"2024-10-07T01:27:59Z","published":"2024-05-26T02:15:49Z","title":"SpinQuant: LLM quantization with learned rotations","summary":"  Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\n","authors":["Zechun Liu","Changsheng Zhao","Igor Fedorov","Bilge Soran","Dhruv Choudhary","Raghuraman Krishnamoorthi","Vikas Chandra","Yuandong Tian","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2405.16406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12832v3","updated":"2024-10-07T01:26:23Z","published":"2024-09-19T15:07:35Z","title":"FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists","summary":"  Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.\n","authors":["Tenghao Huang","Donghee Lee","John Sweeney","Jiatong Shi","Emily Steliotes","Matthew Lange","Jonathan May","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12832v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04663v1","updated":"2024-10-07T00:22:07Z","published":"2024-10-07T00:22:07Z","title":"Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates","summary":"  This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.\n","authors":["Chaithanya Bandi","Hari Bandi","Abir Harrasse"],"pdf_url":"https://arxiv.org/pdf/2410.04663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08760v3","updated":"2024-10-07T00:16:54Z","published":"2024-04-12T18:36:20Z","title":"The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models","summary":"  We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}\n","authors":["Siyang Liu","Trish Maturi","Bowen Yi","Siqi Shen","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2404.08760v3.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2410.04657v1","updated":"2024-10-07T00:09:50Z","published":"2024-10-07T00:09:50Z","title":"Contrastive Learning to Improve Retrieval for Real-world Fact Checking","summary":"  Recent work on fact-checking addresses a realistic setting where models\nincorporate evidence retrieved from the web to decide the veracity of claims. A\nbottleneck in this pipeline is in retrieving relevant evidence: traditional\nmethods may surface documents directly related to a claim, but fact-checking\ncomplex claims requires more inferences. For instance, a document about how a\nvaccine was developed is relevant to addressing claims about what it might\ncontain, even if it does not address them directly. We present Contrastive\nFact-Checking Reranker (CFR), an improved retriever for this setting. By\nleveraging the AVeriTeC dataset, which annotates subquestions for claims with\nhuman written answers from evidence documents, we fine-tune Contriever with a\ncontrastive objective based on multiple training signals, including\ndistillation from GPT-4, evaluating subquestion answers, and gold labels in the\ndataset. We evaluate our model on both retrieval and end-to-end veracity\njudgments about claims. On the AVeriTeC dataset, we find a 6\\% improvement in\nveracity classification accuracy. We also show our gains can be transferred to\nFEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to\nmake inferences.\n","authors":["Aniruddh Sriram","Fangyuan Xu","Eunsol Choi","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2410.04657v1.pdf","comment":"EMNLP 2024 FEVER Workshop"}]},"2024-10-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2211.11548v2","updated":"2024-10-06T23:54:24Z","published":"2022-09-17T05:34:32Z","title":"Survey of Query-based Text Summarization","summary":"  Query-based text summarization is an important real world problem that\nrequires to condense the prolix text data into a summary under the guidance of\nthe query information provided by users. The topic has been studied for a long\ntime and there are many existing interesting research related to query-based\ntext summarization. Yet much of the work is not systematically surveyed. This\nsurvey aims at summarizing some interesting work in query-based text\nsummarization methods as well as related generic text summarization methods.\nNot all taxonomies in this paper exist the related work to the best of our\nknowledge and some analysis will be presented.\n","authors":["Hang Yu","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2211.11548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07064v3","updated":"2024-10-06T23:53:34Z","published":"2023-11-13T04:08:49Z","title":"Prompts have evil twins","summary":"  We discover that many natural-language prompts can be replaced by\ncorresponding prompts that are unintelligible to humans but that provably\nelicit similar behavior in language models. We call these prompts \"evil twins\"\nbecause they are obfuscated and uninterpretable (evil), but at the same time\nmimic the functionality of the original natural-language prompts (twins).\nRemarkably, evil twins transfer between models. We find these prompts by\nsolving a maximum-likelihood problem which has applications of independent\ninterest.\n","authors":["Rimon Melamed","Lucas H. McCabe","Tanay Wakhare","Yejin Kim","H. Howie Huang","Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2311.07064v3.pdf","comment":"EMNLP 2024 Main, camera-ready"},{"id":"http://arxiv.org/abs/2409.18025v2","updated":"2024-10-06T23:30:44Z","published":"2024-09-26T16:32:19Z","title":"An Adversarial Perspective on Machine Unlearning for AI Safety","summary":"  Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.\n","authors":["Jakub Łucki","Boyi Wei","Yangsibo Huang","Peter Henderson","Florian Tramèr","Javier Rando"],"pdf_url":"https://arxiv.org/pdf/2409.18025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11823v2","updated":"2024-10-06T22:42:47Z","published":"2024-06-17T17:57:30Z","title":"On Efficient Language and Vision Assistants for Visually-Situated\n  Natural Language Understanding: What Matters in Reading and Reasoning","summary":"  Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva.\n","authors":["Geewook Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.11823v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2408.08926v2","updated":"2024-10-06T22:19:54Z","published":"2024-08-15T17:23:10Z","title":"Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks\n  of Language Models","summary":"  Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have the potential to cause\nreal-world impact. Policymakers, model providers, and other researchers in the\nAI and cybersecurity communities are interested in quantifying the capabilities\nof such agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute bash commands and\nobserve outputs. Since many tasks are beyond the capabilities of existing LM\nagents, we introduce subtasks for each task, which break down a task into\nintermediary steps for a more detailed evaluation. To evaluate agent\ncapabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o,\nOpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,\nGemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask\nguidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and\nClaude 3 Opus successfully solved complete tasks that took human teams up to 11\nminutes to solve. In comparison, the most difficult task took human teams 24\nhours and 54 minutes to solve. All code and data are publicly available at\nhttps://cybench.github.io\n","authors":["Andy K. Zhang","Neil Perry","Riya Dulepet","Joey Ji","Justin W. Lin","Eliot Jones","Celeste Menders","Gashon Hussein","Samantha Liu","Donovan Jasper","Pura Peetathawatchai","Ari Glenn","Vikram Sivashankar","Daniel Zamoshchin","Leo Glikbarg","Derek Askaryar","Mike Yang","Teddy Zhang","Rishi Alluri","Nathan Tran","Rinnara Sangpisit","Polycarpos Yiorkadjis","Kenny Osele","Gautham Raghupathi","Dan Boneh","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2408.08926v2.pdf","comment":"78 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.12074v2","updated":"2024-10-06T22:17:03Z","published":"2024-06-17T20:20:47Z","title":"COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities","summary":"  Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.\n","authors":["Zihao He","Minh Duc Chu","Rebecca Dorn","Siyi Guo","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.12074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18400v4","updated":"2024-10-06T22:13:16Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v4.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.04633v1","updated":"2024-10-06T21:33:51Z","published":"2024-10-06T21:33:51Z","title":"A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for\n  Speech Emotion Recognition","summary":"  Best-performing speech models are trained on large amounts of data in the\nlanguage they are meant to work for. However, most languages have sparse data,\nmaking training models challenging. This shortage of data is even more\nprevalent in speech emotion recognition. Our work explores the model's\nperformance in limited data, specifically for speech emotion recognition.\nMeta-learning specializes in improving the few-shot learning. As a result, we\nemploy meta-learning techniques on speech emotion recognition tasks, accent\nrecognition, and person identification. To this end, we propose a series of\nimprovements over the multistage meta-learning method. Unlike other works\nfocusing on smaller models due to the high computational cost of meta-learning\nalgorithms, we take a more practical approach. We incorporate a large\npre-trained backbone and a prototypical network, making our methods more\nfeasible and applicable. Our most notable contribution is an improved\nfine-tuning technique during meta-testing that significantly boosts the\nperformance on out-of-distribution datasets. This result, together with\nincremental improvements from several other works, helped us achieve accuracy\nscores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition\ndatasets not included in the training or validation splits in the context of\n4-way 5-shot learning.\n","authors":["David-Gabriel Ion","Răzvan-Alexandru Smădu","Dumitru-Clementin Cercel","Florin Pop","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2410.04633v1.pdf","comment":"16 pages, 1 figure, Accepted by WISE 2024"}]},"2024-10-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.07176v1","updated":"2024-10-09T17:59:58Z","published":"2024-10-09T17:59:58Z","title":"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models","summary":"  Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.\n","authors":["Fei Wang","Xingchen Wan","Ruoxi Sun","Jiefeng Chen","Sercan Ö. Arık"],"pdf_url":"https://arxiv.org/pdf/2410.07176v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07173v1","updated":"2024-10-09T17:59:33Z","published":"2024-10-09T17:59:33Z","title":"Do better language models have crisper vision?","summary":"  How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07170v1","updated":"2024-10-09T17:59:06Z","published":"2024-10-09T17:59:06Z","title":"One Initialization to Rule them All: Fine-tuning via Explained Variance\n  Adaptation","summary":"  Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.\n","authors":["Fabian Paischer","Lukas Hauzenberger","Thomas Schmied","Benedikt Alkin","Marc Peter Deisenroth","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2410.07170v1.pdf","comment":"10 pages + references and appendix, code available at\n  https://github.com/ml-jku/EVA"},{"id":"http://arxiv.org/abs/2410.07167v1","updated":"2024-10-09T17:59:04Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v1.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2410.07168v1","updated":"2024-10-09T17:59:04Z","published":"2024-10-09T17:59:04Z","title":"Sylber: Syllabic Embedding Representation of Speech from Raw Audio","summary":"  Syllables are compositional units of spoken language that play a crucial role\nin human speech perception and production. However, current neural speech\nrepresentations lack structure, resulting in dense token sequences that are\ncostly to process. To bridge this gap, we propose a new model, Sylber, that\nproduces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised model that regresses features on\nsyllabic segments distilled from a teacher model which is an exponential moving\naverage of the model in training. This results in a highly structured\nrepresentation of speech features, offering three key benefits: 1) a fast,\nlinear-time syllable segmentation algorithm, 2) efficient syllabic tokenization\nwith an average of 4.27 tokens per second, and 3) syllabic units better suited\nfor lexical and syntactic understanding. We also train token-to-speech\ngenerative models with our syllabic units and show that fully intelligible\nspeech can be reconstructed from these tokens. Lastly, we observe that\ncategorical perception, a linguistic phenomenon of speech perception, emerges\nnaturally in our model, making the embedding space more categorical and sparse\nthan previous self-supervised learning approaches. Together, we present a novel\nself-supervised approach for representing speech as syllables, with significant\npotential for efficient speech tokenization and spoken language modeling.\n","authors":["Cheol Jun Cho","Nicholas Lee","Akshat Gupta","Dhruv Agarwal","Ethan Chen","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2410.07168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07166v1","updated":"2024-10-09T17:59:00Z","published":"2024-10-09T17:59:00Z","title":"Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making","summary":"  We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.\n","authors":["Manling Li","Shiyu Zhao","Qineng Wang","Kangrui Wang","Yu Zhou","Sanjana Srivastava","Cem Gokmen","Tony Lee","Li Erran Li","Ruohan Zhang","Weiyu Liu","Percy Liang","Li Fei-Fei","Jiayuan Mao","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.07166v1.pdf","comment":"Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track"},{"id":"http://arxiv.org/abs/2410.07163v1","updated":"2024-10-09T17:58:12Z","published":"2024-10-09T17:58:12Z","title":"Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning","summary":"  In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.\n","authors":["Chongyu Fan","Jiancheng Liu","Licong Lin","Jinghan Jia","Ruiqi Zhang","Song Mei","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07157v1","updated":"2024-10-09T17:56:15Z","published":"2024-10-09T17:56:15Z","title":"InstructG2I: Synthesizing Images from Multimodal Attributed Graphs","summary":"  In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.\n","authors":["Bowen Jin","Ziqi Pang","Bingjun Guo","Yu-Xiong Wang","Jiaxuan You","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2410.07157v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.07147v1","updated":"2024-10-09T17:54:41Z","published":"2024-10-09T17:54:41Z","title":"Taking a turn for the better: Conversation redirection throughout the\n  course of mental-health therapy","summary":"  Mental-health therapy involves a complex conversation flow in which patients\nand therapists continuously negotiate what should be talked about next. For\nexample, therapists might try to shift the conversation's direction to keep the\ntherapeutic process on track and avoid stagnation, or patients might push the\ndiscussion towards issues they want to focus on.\n  How do such patient and therapist redirections relate to the development and\nquality of their relationship? To answer this question, we introduce a\nprobabilistic measure of the extent to which a certain utterance immediately\nredirects the flow of the conversation, accounting for both the intention and\nthe actual realization of such a change. We apply this new measure to\ncharacterize the development of patient-therapist relationships over multiple\nsessions in a very large, widely-used online therapy platform. Our analysis\nreveals that (1) patient control of the conversation's direction generally\nincreases relative to that of the therapist as their relationship progresses;\nand (2) patients who have less control in the first few sessions are\nsignificantly more likely to eventually express dissatisfaction with their\ntherapist and terminate the relationship.\n","authors":["Vivian Nguyen","Sang Min Jung","Lillian Lee","Thomas D. Hull","Cristian Danescu-Niculescu-Mizil"],"pdf_url":"https://arxiv.org/pdf/2410.07147v1.pdf","comment":"To appear in the Proceedings of EMNLP (Findings) 2024. Code available\n  at https://convokit.cornell.edu"},{"id":"http://arxiv.org/abs/2410.07145v1","updated":"2024-10-09T17:54:28Z","published":"2024-10-09T17:54:28Z","title":"Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling","summary":"  One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.\n","authors":["Yingfa Chen","Xinrong Zhang","Shengding Hu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07145v1.pdf","comment":"21 pages, 18 figures"},{"id":"http://arxiv.org/abs/2410.07137v1","updated":"2024-10-09T17:53:06Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07129v1","updated":"2024-10-09T17:51:55Z","published":"2024-10-09T17:51:55Z","title":"Mental Disorders Detection in the Era of Large Language Models","summary":"  This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.\n","authors":["Gleb Kuzmin","Petr Strepetov","Maksim Stankevich","Ivan Smirnov","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2410.07129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12877v2","updated":"2024-10-09T17:51:44Z","published":"2024-07-16T08:25:26Z","title":"ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models","summary":"  Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .\n","authors":["Yaswanth Narsupalli","Abhranil Chandra","Sreevatsa Muppirala","Manish Gupta","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2407.12877v2.pdf","comment":"Paper Under Review"},{"id":"http://arxiv.org/abs/2410.07118v1","updated":"2024-10-09T17:48:40Z","published":"2024-10-09T17:48:40Z","title":"Exploring the Readiness of Prominent Small Language Models for the\n  Democratization of Financial Literacy","summary":"  The use of small language models (SLMs), herein defined as models with less\nthan three billion parameters, is increasing across various domains and\napplications. Due to their ability to run on more accessible hardware and\npreserve user privacy, SLMs possess the potential to democratize access to\nlanguage models for individuals of different socioeconomic status and with\ndifferent privacy preferences. This study assesses several state-of-the-art\nSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama\nproject) for use in the financial domain to support the development of\nfinancial literacy LMs. Democratizing access to quality financial information\nfor those who are financially under educated is greatly needed in society,\nparticularly as new financial markets and products emerge and participation in\nfinancial markets increases due to ease of access. We are the first to examine\nthe use of open-source SLMs to democratize access to financial question\nanswering capabilities for individuals and students. To this end, we provide an\nanalysis of the memory usage, inference time, similarity comparisons to\nground-truth answers, and output readability of prominent SLMs to determine\nwhich models are most accessible and capable of supporting access to financial\ninformation. We analyze zero-shot and few-shot learning variants of the models.\nThe results suggest that some off-the-shelf SLMs merit further exploration and\nfine-tuning to prepare them for individual use, while others may have limits to\ntheir democratization.\n","authors":["Tagore Rao Kosireddy","Jeffrey D. Wall","Evan Lucas"],"pdf_url":"https://arxiv.org/pdf/2410.07118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v1","updated":"2024-10-09T17:45:47Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicolò Penzo","Massimo Stefan","Roberto Dessì","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12108v2","updated":"2024-10-09T17:45:07Z","published":"2024-07-16T18:28:40Z","title":"Private prediction for large-scale synthetic text generation","summary":"  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n","authors":["Kareem Amin","Alex Bie","Weiwei Kong","Alexey Kurakin","Natalia Ponomareva","Umar Syed","Andreas Terzis","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2407.12108v2.pdf","comment":"20 pages; updated figure + some new experiments from EMNLP 2024\n  findings camera-ready"},{"id":"http://arxiv.org/abs/2410.07103v1","updated":"2024-10-09T17:41:53Z","published":"2024-10-09T17:41:53Z","title":"Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context","summary":"  Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.\n","authors":["Sangwon Yu","Ik-hwan Kim","Jongyoon Song","Saehyung Lee","Junsung Park","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.07103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00260v2","updated":"2024-10-09T17:39:59Z","published":"2024-09-30T22:15:58Z","title":"DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining","summary":"  Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.\n","authors":["Vinayak Arannil","Neha Narwal","Sourav Sanjukta Bhabesh","Sai Nikhil Thirandas","Darren Yow-Bang Wang","Graham Horwood","Alex Anto Chirayath","Gouri Pandeshwar"],"pdf_url":"https://arxiv.org/pdf/2410.00260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06112v3","updated":"2024-10-09T17:38:22Z","published":"2024-01-11T18:46:12Z","title":"Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed\n  Embeddings","summary":"  Word embedding is one of the most important components in natural language\nprocessing, but interpreting high-dimensional embeddings remains a challenging\nproblem. To address this problem, Independent Component Analysis (ICA) is\nidentified as an effective solution. ICA-transformed word embeddings reveal\ninterpretable semantic axes; however, the order of these axes are arbitrary. In\nthis study, we focus on this property and propose a novel method, Axis Tour,\nwhich optimizes the order of the axes. Inspired by Word Tour, a one-dimensional\nword embedding method, we aim to improve the clarity of the word embedding\nspace by maximizing the semantic continuity of the axes. Furthermore, we show\nthrough experiments on downstream tasks that Axis Tour yields better or\ncomparable low-dimensional embeddings compared to both PCA and ICA.\n","authors":["Hiroaki Yamagiwa","Yusuke Takase","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.06112v3.pdf","comment":"EMNLP 2024 Findings (short)"},{"id":"http://arxiv.org/abs/2410.07095v1","updated":"2024-10-09T17:34:27Z","published":"2024-10-09T17:34:27Z","title":"MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering","summary":"  We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.\n","authors":["Jun Shern Chan","Neil Chowdhury","Oliver Jaffe","James Aung","Dane Sherburn","Evan Mays","Giulio Starace","Kevin Liu","Leon Maksin","Tejal Patwardhan","Lilian Weng","Aleksander Mądry"],"pdf_url":"https://arxiv.org/pdf/2410.07095v1.pdf","comment":"10 pages. Plus 17 pages appendix. 8 figures. Equal contribution by\n  first seven authors. Authors randomized. Work by Neil Chowdhury done while at\n  OpenAI"},{"id":"http://arxiv.org/abs/2410.07094v1","updated":"2024-10-09T17:34:14Z","published":"2024-10-09T17:34:14Z","title":"An Approach for Auto Generation of Labeling Functions for Software\n  Engineering Chatbots","summary":"  Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are the\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend and respond to user queries. Before deploying NLUs, there is a need\nto train them with labeled data. However, acquiring such labeled data for SE\nchatbots is challenging due to the scarcity of high-quality datasets. This\nchallenge arises because training SE chatbots requires specialized vocabulary\nand phrases not found in typical language datasets. Consequently, chatbot\ndevelopers often resort to manually annotating user queries to gather the data\nnecessary for training effective chatbots, a process that is both\ntime-consuming and resource-intensive. Previous studies propose approaches to\nsupport chatbot practitioners in annotating users' posed queries. However,\nthese approaches require human intervention to generate rules, called labeling\nfunctions (LFs), that identify and categorize user queries based on specific\npatterns in the data. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate the effectiveness of our approach by applying it to the queries of\nfour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)\nand measure the performance improvement gained from training the NLU on the\nqueries labeled by the generated LFs. We find that the generated LFs\neffectively label data with AUC scores of up to 85.3%, and NLU's performance\nimprovement of up to 27.2% across the studied datasets. Furthermore, our\nresults show that the number of LFs used to generate LFs affects the labeling\nperformance. We believe that our approach can save time and resources in\nlabeling users' queries, allowing practitioners to focus on core chatbot\nfunctionalities.\n","authors":["Ebube Alor","Ahmad Abdellatif","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2410.07094v1.pdf","comment":"Submitted to IEEE Transactions on Software Engineering for review"},{"id":"http://arxiv.org/abs/2410.07083v1","updated":"2024-10-09T17:24:28Z","published":"2024-10-09T17:24:28Z","title":"Stanceformer: Target-Aware Transformer for Stance Detection","summary":"  The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}\n","authors":["Krishna Garg","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2410.07083v1.pdf","comment":"16 pages, 2 figures, 14 tables including Appendix"},{"id":"http://arxiv.org/abs/2410.07076v1","updated":"2024-10-09T17:19:58Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v1.pdf","comment":"Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git"},{"id":"http://arxiv.org/abs/2410.07073v1","updated":"2024-10-09T17:16:22Z","published":"2024-10-09T17:16:22Z","title":"Pixtral 12B","summary":"  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.\n","authors":["Pravesh Agrawal","Szymon Antoniak","Emma Bou Hanna","Devendra Chaplot","Jessica Chudnovsky","Saurabh Garg","Theophile Gervet","Soham Ghosh","Amélie Héliou","Paul Jacob","Albert Q. Jiang","Timothée Lacroix","Guillaume Lample","Diego Las Casas","Thibaut Lavril","Teven Le Scao","Andy Lo","William Marshall","Louis Martin","Arthur Mensch","Pavankumar Muddireddy","Valera Nemychnikova","Marie Pellat","Patrick Von Platen","Nikhil Raghuraman","Baptiste Rozière","Alexandre Sablayrolles","Lucile Saulnier","Romain Sauvestre","Wendy Shang","Roman Soletskyi","Lawrence Stewart","Pierre Stock","Joachim Studnia","Sandeep Subramanian","Sagar Vaze","Thomas Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06809v3","updated":"2024-10-09T17:16:15Z","published":"2024-04-10T07:56:26Z","title":"Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation","summary":"  The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications.\n","authors":["Ruotong Pan","Boxi Cao","Hongyu Lin","Xianpei Han","Jia Zheng","Sirui Wang","Xunliang Cai","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2404.06809v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and\n  models are available at https://github.com/panruotong/CAG"},{"id":"http://arxiv.org/abs/2410.07069v1","updated":"2024-10-09T17:14:50Z","published":"2024-10-09T17:14:50Z","title":"ReIFE: Re-evaluating Instruction-Following Evaluation","summary":"  The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.\n","authors":["Yixin Liu","Kejian Shi","Alexander R. Fabbri","Yilun Zhao","Peifeng Wang","Chien-Sheng Wu","Shafiq Joty","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2410.07069v1.pdf","comment":"GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE"},{"id":"http://arxiv.org/abs/2408.11252v3","updated":"2024-10-09T17:12:50Z","published":"2024-08-21T00:17:59Z","title":"Counterfactuals As a Means for Evaluating Faithfulness of Attribution\n  Methods in Autoregressive Language Models","summary":"  Despite the widespread adoption of autoregressive language models,\nexplainability evaluation research has predominantly focused on span infilling\nand masked language models. Evaluating the faithfulness of an explanation\nmethod -- how accurately it explains the inner workings and decision-making of\nthe model -- is challenging because it is difficult to separate the model from\nits explanation. Most faithfulness evaluation techniques corrupt or remove\ninput tokens deemed important by a particular attribution (feature importance)\nmethod and observe the resulting change in the model's output. However, for\nautoregressive language models, this approach creates out-of-distribution\ninputs due to their next-token prediction training objective. In this study, we\npropose a technique that leverages counterfactual generation to evaluate the\nfaithfulness of attribution methods for autoregressive language models. Our\ntechnique generates fluent, in-distribution counterfactuals, making the\nevaluation protocol more reliable.\n","authors":["Sepehr Kamahi","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2408.11252v3.pdf","comment":"Accepted to BlackboxNLP @ EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.07064v1","updated":"2024-10-09T17:06:57Z","published":"2024-10-09T17:06:57Z","title":"Data Selection via Optimal Control for Language Models","summary":"  This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which mitigates the quick exhaustion of\navailable web-crawled corpora. Our code, data, and model checkpoints can be\nfound in https://github.com/microsoft/LMOps/tree/main/data_selection.\n","authors":["Yuxian Gu","Li Dong","Hongning Wang","Yaru Hao","Qingxiu Dong","Furu Wei","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16570v5","updated":"2024-10-09T16:52:19Z","published":"2024-08-29T14:37:05Z","title":"Predictability maximization and the origins of word order harmony","summary":"  We address the linguistic problem of the sequential arrangement of a head and\nits dependents from an information theoretic perspective. In particular, we\nconsider the optimal placement of a head that maximizes the predictability of\nthe sequence. We assume that dependents are statistically independent given a\nhead, in line with the open-choice principle and the core assumptions of\ndependency grammar. We demonstrate the optimality of harmonic order, i.e.,\nplacing the head last maximizes the predictability of the head whereas placing\nthe head first maximizes the predictability of dependents. We also show that\npostponing the head is the optimal strategy to maximize its predictability\nwhile bringing it forward is the optimal strategy to maximize the\npredictability of dependents. We unravel the advantages of the strategy of\nmaximizing the predictability of the head over maximizing the predictability of\ndependents. Our findings shed light on the placements of the head adopted by\nreal languages or emerging in different kinds of experiments.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2408.16570v5.pdf","comment":"Minor corrections; references added"},{"id":"http://arxiv.org/abs/2410.07054v1","updated":"2024-10-09T16:51:21Z","published":"2024-10-09T16:51:21Z","title":"Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing","summary":"  Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.\n","authors":["Weichuan Wang","Zhaoyi Li","Defu Lian","Chen Ma","Linqi Song","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2410.07054v1.pdf","comment":"20 pages, EMNLP'2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.07053v1","updated":"2024-10-09T16:51:10Z","published":"2024-10-09T16:51:10Z","title":"Robots in the Middle: Evaluating LLMs in Dispute Resolution","summary":"  Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms.\n","authors":["Jinzhe Tan","Hannes Westermann","Nikhil Reddy Pottanigari","Jaromír Šavelka","Sébastien Meeùs","Mia Godet","Karim Benyekhlef"],"pdf_url":"https://arxiv.org/pdf/2410.07053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07035v1","updated":"2024-10-09T16:15:36Z","published":"2024-10-09T16:15:36Z","title":"PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness","summary":"  Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.\n","authors":["Zekun Wang","Feiyu Duan","Yibo Zhang","Wangchunshu Zhou","Ke Xu","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2410.07035v1.pdf","comment":"39 pages. CP-Bench and LenCtrl-Bench are available in\n  https://huggingface.co/datasets/ZenMoore/CP-Bench and\n  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench"},{"id":"http://arxiv.org/abs/2410.07030v1","updated":"2024-10-09T16:13:19Z","published":"2024-10-09T16:13:19Z","title":"Clean Evaluations on Contaminated Visual Language Models","summary":"  How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.\n","authors":["Hongyuan Lu","Shujie Miao","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2410.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07025v1","updated":"2024-10-09T16:07:11Z","published":"2024-10-09T16:07:11Z","title":"Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback","summary":"  Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology.\n","authors":["Dennis Hein","Zhihong Chen","Sophie Ostmeier","Justin Xu","Maya Varma","Eduardo Pontes Reis","Arne Edward Michalson","Christian Bluethgen","Hyun Joo Shin","Curtis Langlotz","Akshay S Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2410.07025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02554v2","updated":"2024-10-09T16:07:04Z","published":"2022-08-04T09:53:22Z","title":"Vocabulary Transfer for Medical Texts","summary":"  Working within specific NLP subdomains presents significant challenges,\nprimarily due to a persistent deficit of data. Stringent privacy concerns and\nlimited data accessibility often drive this shortage. Additionally, the medical\ndomain demands high accuracy, where even marginal improvements in model\nperformance can have profound impacts. In this study, we investigate the\npotential of vocabulary transfer to enhance model performance in biomedical NLP\ntasks. Specifically, we focus on vocabulary extension, a technique that\ninvolves expanding the target vocabulary to incorporate domain-specific\nbiomedical terms. Our findings demonstrate that vocabulary extension, leads to\nmeasurable improvements in both downstream model performance and inference\ntime.\n","authors":["Priyanka Singh","Vladislav D. Mosin","Ivan P. Yamshchikov"],"pdf_url":"https://arxiv.org/pdf/2208.02554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12746v5","updated":"2024-10-09T16:04:39Z","published":"2024-06-18T16:06:38Z","title":"Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA","summary":"  Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke\n","authors":["Miaoyu Li","Haoxin Li","Zilin Du","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2406.12746v5.pdf","comment":"Accepted to Findings of EMNLP2024"},{"id":"http://arxiv.org/abs/2410.07009v1","updated":"2024-10-09T15:52:48Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation","summary":"  The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Hätty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07002v1","updated":"2024-10-09T15:45:52Z","published":"2024-10-09T15:45:52Z","title":"CursorCore: Assist Programming through Aligning Anything","summary":"  Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.\n","authors":["Hao Jiang","Qi Liu","Rui Li","Shengyu Ye","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v3","updated":"2024-10-09T15:44:36Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v3.pdf","comment":"Accepted by Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.13053v3","updated":"2024-10-09T15:33:10Z","published":"2024-05-19T20:46:07Z","title":"MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models","summary":"  The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching.\n","authors":["Jingwei Xu","Junyu Lai","Yunpeng Huang"],"pdf_url":"https://arxiv.org/pdf/2405.13053v3.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2405.12109v2","updated":"2024-10-09T15:25:25Z","published":"2024-05-20T15:25:18Z","title":"Linguistic Structure from a Bottleneck on Sequential Information\n  Processing","summary":"  Human language is a unique form of communication in the natural world,\ndistinguished by its structured nature. Most fundamentally, it is systematic,\nmeaning that signals can be broken down into component parts that are\nindividually meaningful -- roughly, words -- which are combined in a regular\nway to form sentences. Furthermore, the way in which these parts are combined\nmaintains a kind of locality: words are usually concatenated together, and they\nform contiguous phrases, keeping related parts of sentences close to each\nother. We address the challenge of understanding how these basic properties of\nlanguage arise from broader principles of efficient communication under\ninformation processing constraints. Here we show that natural-language-like\nsystematicity arises in codes that are constrained by predictive information, a\nmeasure of the amount of information that must be extracted from the past of a\nsequence in order to predict its future. In simulations, we show that such\ncodes approximately factorize their source distributions, and then express the\nresulting factors systematically and locally. Next, in a series of\ncross-linguistic corpus studies, we show that human languages are structured to\nhave low predictive information at the levels of phonology, morphology, syntax,\nand semantics. Our result suggests that human language performs a sequential,\ndiscrete form of Independent Components Analysis on the statistical\ndistribution over meanings that need to be expressed. It establishes a link\nbetween the statistical and algebraic structure of human language, and\nreinforces the idea that the structure of human language is shaped by\ncommunication under cognitive constraints.\n","authors":["Richard Futrell","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.12109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12176v2","updated":"2024-10-09T15:23:44Z","published":"2024-07-16T21:03:14Z","title":"GPT-4V Cannot Generate Radiology Reports Yet","summary":"  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n","authors":["Yuyang Jiang","Chacha Chen","Dang Nguyen","Benjamin M. Mervak","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2407.12176v2.pdf","comment":"24 pages, 3 figures, code:\n  https://github.com/YuyangJ0/GPT-4V-evaluation-radiology-report"},{"id":"http://arxiv.org/abs/2410.06981v1","updated":"2024-10-09T15:18:57Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06973v1","updated":"2024-10-09T15:11:13Z","published":"2024-10-09T15:11:13Z","title":"Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara","summary":"  In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.\n","authors":["Azree Nazri","Olalekan Agbolade","Faisal Aziz"],"pdf_url":"https://arxiv.org/pdf/2410.06973v1.pdf","comment":"20 pages, 5 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.06965v1","updated":"2024-10-09T15:02:34Z","published":"2024-10-09T15:02:34Z","title":"Uncovering Factor Level Preferences to Improve Human-Model Alignment","summary":"  Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.\n","authors":["Juhyun Oh","Eunsu Kim","Jiseon Kim","Wenda Xu","Inha Cha","William Yang Wang","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2410.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19919v2","updated":"2024-10-09T14:57:48Z","published":"2024-09-30T03:48:54Z","title":"Understanding Higher-Order Correlations Among Semantic Components in\n  Embeddings","summary":"  Independent Component Analysis (ICA) offers interpretable semantic components\nof embeddings. While ICA theory assumes that embeddings can be linearly\ndecomposed into independent components, real-world data often do not satisfy\nthis assumption. Consequently, non-independencies remain between the estimated\ncomponents, which ICA cannot eliminate. We quantified these non-independencies\nusing higher-order correlations and demonstrated that when the higher-order\ncorrelation between two components is large, it indicates a strong semantic\nassociation between them, along with many words sharing common meanings with\nboth components. The entire structure of non-independencies was visualized\nusing a maximum spanning tree of semantic components. These findings provide\ndeeper insights into embeddings through ICA.\n","authors":["Momose Oyama","Hiroaki Yamagiwa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2409.19919v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06961v1","updated":"2024-10-09T14:57:31Z","published":"2024-10-09T14:57:31Z","title":"Self-Boosting Large Language Models with Synthetic Preference Data","summary":"  Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.\n","authors":["Qingxiu Dong","Li Dong","Xingxing Zhang","Zhifang Sui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.06961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06949v1","updated":"2024-10-09T14:45:45Z","published":"2024-10-09T14:45:45Z","title":"Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06949v1.pdf","comment":"26 pages, 7 figures. Submitted ICLR 2025"},{"id":"http://arxiv.org/abs/2410.06944v1","updated":"2024-10-09T14:38:49Z","published":"2024-10-09T14:38:49Z","title":"CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on\n  Relatively Free Word Ordered and Morphologically Rich Low Resource Languages","summary":"  Neural dependency parsing has achieved remarkable performance for low\nresource morphologically rich languages. It has also been well-studied that\nmorphologically rich languages exhibit relatively free word order. This prompts\na fundamental investigation: Is there a way to enhance dependency parsing\nperformance, making the model robust to word order variations utilizing the\nrelatively free word order nature of morphologically rich languages? In this\nwork, we examine the robustness of graph-based parsing architectures on 7\nrelatively free word order languages. We focus on scrutinizing essential\nmodifications such as data augmentation and the removal of position encoding\nrequired to adapt these architectures accordingly. To this end, we propose a\ncontrastive self-supervised learning method to make the model robust to word\norder variations. Furthermore, our proposed modification demonstrates a\nsubstantial average gain of 3.03/2.95 points in 7 relatively free word order\nlanguages, as measured by the UAS/LAS Score metric when compared to the best\nperforming baseline.\n","authors":["Pretam Ray","Jivnesh Sandhan","Amrith Krishna","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.06944v1.pdf","comment":"Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.20271v2","updated":"2024-10-09T14:30:08Z","published":"2024-07-25T07:09:35Z","title":"Learn while Unlearn: An Iterative Unlearning Framework for Generative\n  Language Models","summary":"  Recent advancements in machine learning, particularly in Natural Language\nProcessing (NLP), have led to the development of sophisticated models trained\non extensive datasets, yet raising concerns about the potential leakage of\nsensitive information. In response, regulatory measures such as the European\nUnion's General Data Protection Regulation (GDPR) have driven increasing\ninterest in Machine Unlearning techniques, which enable models to selectively\nforget specific data entries. Early approaches primarily relied on\npre-processing methods, while more recent research has shifted towards\ntraining-based unlearning techniques. Despite their effectiveness, most\nexisting methods require access to the original training data, which is often\ninaccessible. Additionally, directly applying unlearning techniques bear the\ncost of undermining the model's expressive capabilities. To address these\nchallenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,\nwhich consists of three core components: A Knowledge Unlearning Induction\nmodule designed to remove specific knowledge through an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically assess the unlearning extent on specific\ndata pieces and make iterative update. Experimental results demonstrate the\nefficacy of our ICU method in unlearning sensitive information while\nmaintaining the model's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications.\n","authors":["Haoyu Tang","Ye Liu","Xukai Liu","Kai Zhang","Yanghai Zhang","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.20271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01899v2","updated":"2024-10-09T14:25:00Z","published":"2024-07-02T02:50:15Z","title":"Scope-enhanced Compositional Semantic Parsing for DRT","summary":"  Discourse Representation Theory (DRT) distinguishes itself from other\nsemantic representation frameworks by its ability to model complex semantic and\ndiscourse phenomena through structural nesting and variable binding. While\nseq2seq models hold the state of the art on DRT parsing, their accuracy\ndegrades with the complexity of the sentence, and they sometimes struggle to\nproduce well-formed DRT representations. We introduce the AMS parser, a\ncompositional, neurosymbolic semantic parser for DRT. It rests on a novel\nmechanism for predicting quantifier scope. We show that the AMS parser reliably\nproduces well-formed outputs and performs well on DRT parsing, especially on\ncomplex sentences.\n","authors":["Xiulin Yang","Jonas Groschwitz","Alexander Koller","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2407.01899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06916v1","updated":"2024-10-09T14:15:30Z","published":"2024-10-09T14:15:30Z","title":"SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration","summary":"  Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text.\n","authors":["Heming Xia","Yongqi Li","Jun Zhang","Cunxiao Du","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06913v1","updated":"2024-10-09T14:12:51Z","published":"2024-10-09T14:12:51Z","title":"Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning","summary":"  Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github.\n","authors":["Runchuan Zhu","Zhipeng Ma","Jiang Wu","Junyuan Gao","Jiaqi Wang","Dahua Lin","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2410.06913v1.pdf","comment":"Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He"},{"id":"http://arxiv.org/abs/2410.02492v2","updated":"2024-10-09T14:07:15Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v2.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2410.06898v1","updated":"2024-10-09T13:59:34Z","published":"2024-10-09T13:59:34Z","title":"Generative Model for Less-Resourced Language with 1 billion parameters","summary":"  Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.\n","authors":["Domen Vreš","Martin Božič","Aljaž Potočnik","Tomaž Martinčič","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.06898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06886v1","updated":"2024-10-09T13:47:50Z","published":"2024-10-09T13:47:50Z","title":"FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding","summary":"  The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications.\n","authors":["Jingyang Deng","Zhengyang Shen","Boyang Wang","Lixin Su","Suqi Cheng","Ying Nie","Junfeng Wang","Dawei Yin","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.06886v1.pdf","comment":"Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024), this is the full version of the paper including technical\n  appendices. This final version features enhanced formatting and corrections\n  to errors present in other online versions. We regret any inconvenience this\n  may have caused our readers"},{"id":"http://arxiv.org/abs/2409.06927v3","updated":"2024-10-09T13:39:27Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v3.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2305.10652v4","updated":"2024-10-09T13:06:48Z","published":"2023-05-18T02:19:05Z","title":"Speech Separation based on Contrastive Learning and Deep Modularization","summary":"  The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.\n","authors":["Peter Ochieng"],"pdf_url":"https://arxiv.org/pdf/2305.10652v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2212.00369"},{"id":"http://arxiv.org/abs/2410.06846v1","updated":"2024-10-09T13:06:43Z","published":"2024-10-09T13:06:43Z","title":"Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity","summary":"  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2410.06846v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.06845v1","updated":"2024-10-09T13:06:40Z","published":"2024-10-09T13:06:40Z","title":"MentalArena: Self-play Training of Language Models for Diagnosis and\n  Treatment of Mental Health Disorders","summary":"  Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main\n","authors":["Cheng Li","May Fung","Qingyun Wang","Chi Han","Manling Li","Jindong Wang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.06845v1.pdf","comment":"Technical Report; 27 pages"},{"id":"http://arxiv.org/abs/2405.14722v4","updated":"2024-10-09T12:48:03Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.02966v3","updated":"2024-10-09T12:46:40Z","published":"2024-03-05T13:43:58Z","title":"Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering","summary":"  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.\n","authors":["Sungho Ko","Hyunjin Cho","Hyungjoo Chae","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2403.02966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04798v2","updated":"2024-10-09T12:37:08Z","published":"2024-10-07T07:21:49Z","title":"DAPE V2: Process Attention Score as Feature Map for Length Extrapolation","summary":"  The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Jing Xiong","Jiankai Sun","Jingyao Li","Minbin Huang","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.04798v2.pdf","comment":"Tech Report. arXiv admin note: text overlap with arXiv:2405.14722"},{"id":"http://arxiv.org/abs/2405.17264v2","updated":"2024-10-09T12:34:19Z","published":"2024-05-27T15:22:58Z","title":"On the Noise Robustness of In-Context Learning for Text Generation","summary":"  Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations.\n","authors":["Hongfu Gao","Feipeng Zhang","Wenyu Jiang","Jun Shu","Feng Zheng","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2405.17264v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.18708v4","updated":"2024-10-09T12:29:38Z","published":"2024-09-27T12:54:13Z","title":"Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity","summary":"  We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2409.18708v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19759v2","updated":"2024-10-09T12:20:33Z","published":"2024-06-28T08:59:24Z","title":"Breaking the Script Barrier in Multilingual Pre-Trained Language Models\n  with Transliteration-Based Post-Training Alignment","summary":"  Multilingual pre-trained models (mPLMs) have shown impressive performance on\ncross-lingual transfer tasks. However, the transfer performance is often\nhindered when a low-resource target language is written in a different script\nthan the high-resource source language, even though the two languages may be\nrelated or share parts of their vocabularies. Inspired by recent work that uses\ntransliteration to address this problem, our paper proposes a\ntransliteration-based post-pretraining alignment (PPA) method aiming to improve\nthe cross-lingual alignment between languages using diverse scripts. We select\ntwo areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and\n$\\textbf{South+East Asian Languages}$, wherein the languages are mutually\ninfluenced but use different scripts. We apply our method to these language\ngroups and conduct extensive experiments on a spectrum of downstream tasks. The\nresults show that after PPA, models consistently outperform the original model\n(up to 50% for some tasks) in English-centric transfer. In addition, when we\nuse languages other than English as sources in transfer, our method obtains\neven larger improvements. We will make our code and models publicly available\nat \\url{https://github.com/cisnlp/Transliteration-PPA}.\n","authors":["Orgest Xhelili","Yihong Liu","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.19759v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2405.19846v5","updated":"2024-10-09T12:14:22Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18231v2","updated":"2024-10-09T12:11:15Z","published":"2024-04-28T15:56:41Z","title":"From Persona to Personalization: A Survey on Role-Playing Language\n  Agents","summary":"  Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.\n","authors":["Jiangjie Chen","Xintao Wang","Rui Xu","Siyu Yuan","Yikai Zhang","Wei Shi","Jian Xie","Shuang Li","Ruihan Yang","Tinghui Zhu","Aili Chen","Nianqi Li","Lida Chen","Caiyu Hu","Siye Wu","Scott Ren","Ziquan Fu","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.18231v2.pdf","comment":"Accepted to TMLR 2024"},{"id":"http://arxiv.org/abs/2410.06809v1","updated":"2024-10-09T12:09:30Z","published":"2024-10-09T12:09:30Z","title":"Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level","summary":"  Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.\n","authors":["Xinyi Zeng","Yuying Shang","Yutao Zhu","Jiawei Chen","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06809v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.03278v2","updated":"2024-10-09T12:07:08Z","published":"2024-10-04T09:50:45Z","title":"What do Large Language Models Need for Machine Translation Evaluation?","summary":"  Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.\n","authors":["Shenbin Qian","Archchana Sindhujan","Minnie Kabra","Diptesh Kanojia","Constantin Orăsan","Tharindu Ranasinghe","Frédéric Blain"],"pdf_url":"https://arxiv.org/pdf/2410.03278v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.17774v2","updated":"2024-10-09T11:59:34Z","published":"2024-09-26T12:11:28Z","title":"Faithfulness and the Notion of Adversarial Sensitivity in NLP\n  Explanations","summary":"  Faithfulness is arguably the most critical metric to assess the reliability\nof explainable AI. In NLP, current methods for faithfulness evaluation are\nfraught with discrepancies and biases, often failing to capture the true\nreasoning of models. We introduce Adversarial Sensitivity as a novel approach\nto faithfulness evaluation, focusing on the explainer's response when the model\nis under adversarial attack. Our method accounts for the faithfulness of\nexplainers by capturing sensitivity to adversarial input changes. This work\naddresses significant limitations in existing evaluation techniques, and\nfurthermore, quantifies faithfulness from a crucial yet underexplored paradigm.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2409.17774v2.pdf","comment":"Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP"},{"id":"http://arxiv.org/abs/2410.06802v1","updated":"2024-10-09T11:58:40Z","published":"2024-10-09T11:58:40Z","title":"Seg2Act: Global Context-aware Action Generation for Document Logical\n  Structuring","summary":"  Document logical structuring aims to extract the underlying hierarchical\nstructure of documents, which is crucial for document intelligence. Traditional\napproaches often fall short in handling the complexity and the variability of\nlengthy documents. To address these issues, we introduce Seg2Act, an\nend-to-end, generation-based method for document logical structuring,\nrevisiting logical structure extraction as an action generation task.\nSpecifically, given the text segments of a document, Seg2Act iteratively\ngenerates the action sequence via a global context-aware generative model, and\nsimultaneously updates its global context and current logical structure based\non the generated actions. Experiments on ChCatExt and HierDoc datasets\ndemonstrate the superior performance of Seg2Act in both supervised and transfer\nlearning settings.\n","authors":["Zichao Li","Shaojie He","Meng Liao","Xuanang Chen","Yaojie Lu","Hongyu Lin","Yanxiong Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.06802v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2311.08481v2","updated":"2024-10-09T11:54:38Z","published":"2023-11-14T19:15:55Z","title":"Functionality learning through specification instructions","summary":"  Test suites assess natural language processing models' performance on\nspecific functionalities: cases of interest involving model robustness,\nfairness, or particular linguistic capabilities. This paper introduces\nspecification instructions: text descriptions specifying fine-grained\ntask-specific behaviors. For each functionality in a suite, we generate an\ninstruction that describes it. We combine the specification instructions to\ncreate specification-augmented prompts, which we feed to language models\npre-trained on natural instruction data.\n  We conduct experiments to measure how optimizing for some functionalities may\nnegatively impact functionalities that are not covered by the specification\nset. Our analyses across four tasks and models of diverse sizes and families\nshow that smaller models struggle to follow specification instructions.\nHowever, larger models (>~3B params.) can benefit from specifications and --\nsurprisingly -- even generalize certain desirable behaviors across\nfunctionalities.\n","authors":["Pedro Henrique Luz de Araujo","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08481v2.pdf","comment":"36 pages, 8 figures. Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.06795v1","updated":"2024-10-09T11:46:32Z","published":"2024-10-09T11:46:32Z","title":"From Pixels to Tokens: Revisiting Object Hallucinations in Large\n  Vision-Language Models","summary":"  Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.\n","authors":["Yuying Shang","Xinyi Zeng","Yutao Zhu","Xiao Yang","Zhengwei Fang","Jingyuan Zhang","Jiawei Chen","Zinan Liu","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05949v6","updated":"2024-10-09T11:46:24Z","published":"2024-01-11T14:38:19Z","title":"Universal Vulnerabilities in Large Language Models: Backdoor Attacks for\n  In-context Learning","summary":"  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.\n","authors":["Shuai Zhao","Meihuizi Jia","Luu Anh Tuan","Fengjun Pan","Jinming Wen"],"pdf_url":"https://arxiv.org/pdf/2401.05949v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06917v3","updated":"2024-10-09T11:17:46Z","published":"2024-07-09T14:52:52Z","title":"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models","summary":"  Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.\n","authors":["Zara Siddique","Liam D. Turner","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2407.06917v3.pdf","comment":"Accepted to EMNLP Main 2024"},{"id":"http://arxiv.org/abs/2410.06765v1","updated":"2024-10-09T10:53:18Z","published":"2024-10-09T10:53:18Z","title":"To Preserve or To Compress: An In-Depth Study of Connector Selection in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have garnered\nsignificant attention from both industry and academia. However, there is still\nconsiderable debate on constructing MLLM architectures, particularly regarding\nthe selection of appropriate connectors for perception tasks of varying\ngranularities. This paper systematically investigates the impact of connectors\non MLLM performance. Specifically, we classify connectors into\nfeature-preserving and feature-compressing types. Utilizing a unified\nclassification standard, we categorize sub-tasks from three comprehensive\nbenchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained\nperception, fine-grained perception, and reasoning, and evaluate the\nperformance. Our findings reveal that feature-preserving connectors excel in\n\\emph{fine-grained perception} tasks due to their ability to retain detailed\nvisual information. In contrast, feature-compressing connectors, while less\neffective in fine-grained perception tasks, offer significant speed advantages\nand perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning}\ntasks. These insights are crucial for guiding MLLM architecture design and\nadvancing the optimization of MLLM architectures.\n","authors":["Junyan Lin","Haoran Chen","Dawei Zhu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06765v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.03553v2","updated":"2024-10-09T10:49:08Z","published":"2024-10-04T16:02:50Z","title":"Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding","summary":"  Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.\n","authors":["Wei Wu","Chao Wang","Liyi Chen","Mingze Yin","Yiheng Zhu","Kun Fu","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06741v1","updated":"2024-10-09T10:20:32Z","published":"2024-10-09T10:20:32Z","title":"CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models","summary":"  Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.\n","authors":["Zi Gong","Hang Yu","Cong Liao","Bingchang Liu","Chaoyu Chen","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2410.06741v1.pdf","comment":"15 pages, main conference of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06735v1","updated":"2024-10-09T10:13:13Z","published":"2024-10-09T10:13:13Z","title":"Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?","summary":"  Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.\n","authors":["Fumiya Uchiyama","Takeshi Kojima","Andrew Gambardella","Qi Cao","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2410.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06733v1","updated":"2024-10-09T10:09:11Z","published":"2024-10-09T10:09:11Z","title":"Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles","summary":"  While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.\n","authors":["Qi Chen","Bowen Zhang","Gang Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.06733v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06722v1","updated":"2024-10-09T09:45:01Z","published":"2024-10-09T09:45:01Z","title":"Scaling Laws for Mixed quantization in Large Language Models","summary":"  Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.\n","authors":["Zeyu Cao","Cheng Zhang","Pedro Gimenes","Jianqiao Lu","Jianyi Cheng","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06718v1","updated":"2024-10-09T09:41:34Z","published":"2024-10-09T09:41:34Z","title":"MatMamba: A Matryoshka State Space Model","summary":"  State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}\n","authors":["Abhinav Shukla","Sai Vemprala","Aditya Kusupati","Ashish Kapoor"],"pdf_url":"https://arxiv.org/pdf/2410.06718v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06716v1","updated":"2024-10-09T09:39:55Z","published":"2024-10-09T09:39:55Z","title":"Guaranteed Generation from Large Language Models","summary":"  As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.\n","authors":["Minbeom Kim","Thibaut Thonet","Jos Rozen","Hwaran Lee","Kyomin Jung","Marc Dymetman"],"pdf_url":"https://arxiv.org/pdf/2410.06716v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.19874v2","updated":"2024-10-09T09:36:49Z","published":"2024-06-28T12:28:52Z","title":"Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood","summary":"  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n","authors":["Yang Xu","Yu Wang","Hao An","Zhichen Liu","Yongyuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19874v2.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.06707v1","updated":"2024-10-09T09:20:24Z","published":"2024-10-09T09:20:24Z","title":"Calibrating Verbalized Probabilities for Large Language Models","summary":"  Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments.\n","authors":["Cheng Wang","Gyuri Szarvas","Georges Balazs","Pavel Danchenko","Patrick Ernst"],"pdf_url":"https://arxiv.org/pdf/2410.06707v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.06704v1","updated":"2024-10-09T09:16:25Z","published":"2024-10-09T09:16:25Z","title":"PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs","summary":"  In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.\n","authors":["Krishna Kanth Nakka","Ahmed Frikha","Ricardo Mendes","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.06704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00403v2","updated":"2024-10-09T09:14:17Z","published":"2024-03-30T15:59:17Z","title":"UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion\n  Cause","summary":"  Multimodal emotion recognition in conversation (MERC) and multimodal\nemotion-cause pair extraction (MECPE) have recently garnered significant\nattention. Emotions are the expression of affect or feelings; responses to\nspecific events, or situations -- known as emotion causes. Both collectively\nexplain the causality between human emotion and intents. However, existing\nworks treat emotion recognition and emotion cause extraction as two individual\nproblems, ignoring their natural causality. In this paper, we propose a Unified\nMultimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC)\nto explore the causality between emotion and emotion cause. Concretely, UniMEEC\nreformulates the MERC and MECPE tasks as mask prediction problems and unifies\nthem with a causal prompt template. To differentiate the modal effects, UniMEEC\nproposes a multimodal causal prompt to probe the pre-trained knowledge\nspecified to modality and implements cross-task and cross-modality interactions\nunder task-oriented settings. Experiment results on four public benchmark\ndatasets verify the model performance on MERC and MECPE tasks and achieve\nconsistent improvements compared with the previous state-of-the-art methods.\n","authors":["Guimin Hu","Zhihong Zhu","Daniel Hershcovich","Lijie Hu","Hasti Seifi","Jiayuan Xie"],"pdf_url":"https://arxiv.org/pdf/2404.00403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17011v2","updated":"2024-10-09T08:58:40Z","published":"2024-07-24T05:26:52Z","title":"Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism","summary":"  Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively.\n","authors":["Anhao Zhao","Fanghua Ye","Jinlan Fu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2407.17011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02655v2","updated":"2024-10-09T08:51:28Z","published":"2024-04-03T11:36:12Z","title":"Calibrating the Confidence of Large Language Models by Eliciting\n  Fidelity","summary":"  Large language models optimized with techniques like RLHF have achieved good\nalignment in being helpful and harmless. However, post-alignment, these\nlanguage models often exhibit overconfidence, where the expressed confidence\ndoes not accurately calibrate with their correctness rate. In this paper, we\ndecompose the language model confidence into the \\textit{Uncertainty} about the\nquestion and the \\textit{Fidelity} to the answer generated by language models.\nThen, we propose a plug-and-play method to estimate the confidence of language\nmodels. Our method has shown good calibration performance by conducting\nexperiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two\nnovel metrics, IPR and CE, to evaluate the calibration of the model, and we\nhave conducted a detailed discussion on \\textit{Truly Well-Calibrated\nConfidence}. Our method could serve as a strong baseline, and we hope that this\nwork will provide some insights into the model confidence calibration.\n","authors":["Mozhi Zhang","Mianqiu Huang","Rundong Shi","Linsen Guo","Chong Peng","Peng Yan","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.02655v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06682v1","updated":"2024-10-09T08:44:47Z","published":"2024-10-09T08:44:47Z","title":"Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zujun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06672v1","updated":"2024-10-09T08:28:53Z","published":"2024-10-09T08:28:53Z","title":"Towards Universality: Studying Mechanistic Similarity Across Language\n  Model Architectures","summary":"  The hypothesis of Universality in interpretability suggests that different\nneural networks may converge to implement similar algorithms on similar tasks.\nIn this work, we investigate two mainstream architectures for language\nmodeling, namely Transformers and Mambas, to explore the extent of their\nmechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate\ninterpretable features from these models and show that most features are\nsimilar in these two models. We also validate the correlation between feature\nsimilarity and Universality. We then delve into the circuit-level analysis of\nMamba models and find that the induction circuits in Mamba are structurally\nanalogous to those in Transformers. We also identify a nuanced difference we\ncall \\emph{Off-by-One motif}: The information of one token is written into the\nSSM state in its next position. Whilst interaction between tokens in\nTransformers does not exhibit such trend.\n","authors":["Junxuan Wang","Xuyang Ge","Wentao Shu","Qiong Tang","Yunhua Zhou","Zhengfu He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.06672v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.06667v1","updated":"2024-10-09T08:23:22Z","published":"2024-10-09T08:23:22Z","title":"Large Language Models as Code Executors: An Exploratory Study","summary":"  The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.\n","authors":["Chenyang Lyu","Lecheng Yan","Rui Xing","Wenxi Li","Younes Samih","Tianbo Ji","Longyue Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19013v2","updated":"2024-10-09T08:16:44Z","published":"2024-09-23T23:43:43Z","title":"Improving Academic Skills Assessment with NLP and Ensemble Learning","summary":"  This study addresses the critical challenges of assessing foundational\nacademic skills by leveraging advancements in natural language processing\n(NLP). Traditional assessment methods often struggle to provide timely and\ncomprehensive feedback on key cognitive and linguistic aspects, such as\ncoherence, syntax, and analytical reasoning. Our approach integrates multiple\nstate-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,\nwithin an ensemble learning framework. These models are combined through\nstacking techniques using LightGBM and Ridge regression to enhance predictive\naccuracy. The methodology involves detailed data preprocessing, feature\nextraction, and pseudo-label learning to optimize model performance. By\nincorporating sophisticated NLP techniques and ensemble learning, this study\nsignificantly improves the accuracy and efficiency of assessments, offering a\nrobust solution that surpasses traditional methods and opens new avenues for\neducational technology research focused on enhancing core academic\ncompetencies.\n","authors":["Zhengpei Cheng","Yingyi Wu","Danyang Zhang","Jiacheng Hu","Yujian Long"],"pdf_url":"https://arxiv.org/pdf/2409.19013v2.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05413v3","updated":"2024-10-09T07:53:10Z","published":"2024-07-07T15:37:13Z","title":"SBoRA: Low-Rank Adaptation with Regional Weight Updates","summary":"  This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA reduces the number of trainable parameters by half or doubles\nthe rank with the similar number of trainable parameters as LoRA, while\nimproving learning performance. By utilizing orthogonal standard basis vectors\nto initialize one of the low-rank matrices (either $\\mathbf{A}$ or\n$\\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient\nfine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only\none of the matrices is updated, leading to a sparse update matrix\n$\\mathrm{\\Delta} \\mathbf{W}$ with predominantly zero rows or columns.\nConsequently, most of the fine-tuned model's weights\n$(\\mathbf{W}_0+\\mathrm{\\Delta} \\mathbf{W})$ remain unchanged from the\npre-trained weights, akin to the modular organization of the human brain, which\nefficiently adapts to new tasks. Our empirical results demonstrate the\nsuperiority of SBoRA-FA over LoRA in various fine-tuning tasks, including\ncommonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the\neffectiveness of QSBoRA on quantized LLaMA models of varying scales,\nhighlighting its potential for efficient adaptation to new tasks. Code is\navailable at https://github.com/cityuhkai/SBoRA\n","authors":["Lai-Man Po","Yuyang Liu","Haoxuan Wu","Tianqi Zhang","Wing-Yin Yu","Zhuohan Wang","Zeyu Jiang","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2407.05413v3.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.06638v1","updated":"2024-10-09T07:43:38Z","published":"2024-10-09T07:43:38Z","title":"Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing","summary":"  Large Language Models (LLMs) have exhibited strong mathematical reasoning and\ncomputational prowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle errors, such\nas miscalculations or incorrect substitutions, limit the models' full\nmathematical potential. Existing studies to improve mathematical ability\ntypically involve distilling reasoning skills from stronger LLMs or applying\npreference learning to step-wise response pairs. Although these methods\nleverage samples of varying granularity to mitigate reasoning errors, they\noverlook the frequently occurring subtle errors. A major reason is that sampled\npreference pairs involve differences unrelated to the errors, which may\ndistract the model from focusing on subtle errors. In this work, we propose a\nnovel preference learning framework called eRror-Injected Self-Editing (RISE),\nwhich injects predefined subtle errors into partial tokens of correct solutions\nto construct hard pairs for error mitigation. In detail, RISE uses the model\nitself to edit a small number of tokens in the solution, injecting designed\nsubtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective to focus on predefined errors and their tokens, without\nrequiring fine-grained sampling or preference annotation. Extensive experiments\nvalidate the effectiveness of RISE, with preference learning on\nQwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on\nMATH.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Chak Tou Leong","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16563v2","updated":"2024-10-09T07:39:29Z","published":"2024-04-25T12:24:37Z","title":"Evaluating Large Language Models on Time Series Feature Understanding: A\n  Comprehensive Taxonomy and Benchmark","summary":"  Large Language Models (LLMs) offer the potential for automatic time series\nanalysis and reporting, which is a critical task across many domains, spanning\nhealthcare, finance, climate, energy, and many more. In this paper, we propose\na framework for rigorously evaluating the capabilities of LLMs on time series\nunderstanding, encompassing both univariate and multivariate forms. We\nintroduce a comprehensive taxonomy of time series features, a critical\nframework that delineates various characteristics inherent in time series data.\nLeveraging this taxonomy, we have systematically designed and synthesized a\ndiverse dataset of time series, embodying the different outlined features, each\naccompanied by textual descriptions. This dataset acts as a solid foundation\nfor assessing the proficiency of LLMs in comprehending time series. Our\nexperiments shed light on the strengths and limitations of state-of-the-art\nLLMs in time series understanding, revealing which features these models\nreadily comprehend effectively and where they falter. In addition, we uncover\nthe sensitivity of LLMs to factors including the formatting of the data, the\nposition of points queried within a series and the overall time series length.\n","authors":["Elizabeth Fons","Rachneet Kaur","Soham Palande","Zhen Zeng","Tucker Balch","Manuela Veloso","Svitlana Vyetrenko"],"pdf_url":"https://arxiv.org/pdf/2404.16563v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06634v1","updated":"2024-10-09T07:35:46Z","published":"2024-10-09T07:35:46Z","title":"Tree of Problems: Improving structured problem solving with\n  compositionality","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems.\n","authors":["Armel Zebaze","Benoît Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2410.06634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02599v2","updated":"2024-10-09T07:31:18Z","published":"2024-08-05T16:21:17Z","title":"Progressively Label Enhancement for Large Language Model Alignment","summary":"  Large Language Models (LLM) alignment aims to prevent models from producing\ncontent that misaligns with human expectations, which can lead to ethical and\nlegal concerns. In the last few years, Reinforcement Learning from Human\nFeedback (RLHF) has been the most prominent method for achieving alignment. Due\nto challenges in stability and scalability with RLHF stages, which arise from\nthe complex interactions between multiple models, researchers are exploring\nalternative methods to achieve effects comparable to those of RLHF. However,\nthese methods often rely on large high-quality datasets. Despite some methods\nconsidering the generation of additional data to expand datasets, they often\ntreat model training and data generation as separate and static processes,\noverlooking the fact that these processes are highly interdependent, leading to\ninefficient utilization of the generated data. To deal with this problem, we\npropose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a\nframework that dynamically adjusts the model's training process based on the\nevolving quality of the generated data. Specifically, we prompt the model to\ngenerate responses for both the original query and the query guided by a set of\ncarefully designed principles, and then utilize a dynamic threshold to\ndetermine the appropriate training approach for both responses based on their\ncorresponding reward scores. Experimental results demonstrate the effectiveness\nof PLE compared to existing LLM alignment methods.\n","authors":["Biao Liu","Ning Xu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2408.02599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06625v1","updated":"2024-10-09T07:21:43Z","published":"2024-10-09T07:21:43Z","title":"ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time","summary":"  Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.\n","authors":["Yi Ding","Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06625v1.pdf","comment":"27pages"},{"id":"http://arxiv.org/abs/2410.06617v1","updated":"2024-10-09T07:14:45Z","published":"2024-10-09T07:14:45Z","title":"Learning Evolving Tools for Large Language Models","summary":"  Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning.\n","authors":["Guoxin Chen","Zhong Zhang","Xin Cong","Fangda Guo","Yesai Wu","Yankai Lin","Wenzheng Feng","Yasheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06617v1.pdf","comment":"Ongoning Work"},{"id":"http://arxiv.org/abs/2410.06615v1","updated":"2024-10-09T07:12:24Z","published":"2024-10-09T07:12:24Z","title":"$β$-calibration of Language Model Confidence Scores for Generative\n  QA","summary":"  To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is on average\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce $\\beta$-calibration,\nwhich ensures calibration holds across different question-and-answer groups. We\nthen propose discretized posthoc calibration schemes for achieving\n$\\beta$-calibration.\n","authors":["Putra Manggala","Atalanti Mastakouri","Elke Kirschbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2410.06615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11911v3","updated":"2024-10-09T06:59:31Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework inspired by cognitive load theory to measure the complexity of ToM\ntasks. We quantify a problem's complexity as the number of states necessary to\nsolve it correctly. Our complexity measure also accounts for spurious states of\na ToM problem designed to make it apparently harder. We use our method to\nassess the complexity of five widely adopted ToM benchmarks. On top of this\nframework, we design a prompting technique that augments the information\navailable to a model with a description of how the environment changes with the\nagents' interactions. We name this technique Discrete World Models (DWM) and\nshow how it elicits superior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v3.pdf","comment":"Accepted EMNLP 2024, Website\n  https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2410.06606v1","updated":"2024-10-09T06:58:09Z","published":"2024-10-09T06:58:09Z","title":"Dissecting Fine-Tuning Unlearning in Large Language Models","summary":"  Fine-tuning-based unlearning methods prevail for preventing targeted harmful,\nsensitive, or copyrighted information within large language models while\npreserving overall capabilities. However, the true effectiveness of these\nmethods is unclear. In this paper, we delve into the limitations of\nfine-tuning-based unlearning through activation patching and parameter\nrestoration experiments. Our findings reveal that these methods alter the\nmodel's knowledge retrieval process, rather than genuinely erasing the\nproblematic knowledge embedded in the model parameters. Furthermore, behavioral\ntests demonstrate that the unlearning mechanisms inevitably impact the global\nbehavior of the models, affecting unrelated knowledge or capabilities. Our work\nadvocates the development of more resilient unlearning techniques for truly\nerasing knowledge. Our code is released at\nhttps://github.com/yihuaihong/Dissecting-FT-Unlearning.\n","authors":["Yihuai Hong","Yuelin Zou","Lijie Hu","Ziqian Zeng","Di Wang","Haiqin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.06606v1.pdf","comment":"Accepted in EMNLP 2024 Main (Short paper)"},{"id":"http://arxiv.org/abs/2402.10645v3","updated":"2024-10-09T06:54:29Z","published":"2024-02-16T12:46:16Z","title":"Can Separators Improve Chain-of-Thought Prompting?","summary":"  Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large Language Models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce COT-SEP, a method that strategically employs separators\nat the end of each exemplar in CoT prompting. These separators are designed to\nhelp the LLMs understand their thought processes better while reasoning.\nInterestingly, it turns out that COT-SEP significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.\n","authors":["Yoonjeong Park","Hyunjin Kim","Chanyeol Choi","Junseong Kim","Jy-yong Sohn"],"pdf_url":"https://arxiv.org/pdf/2402.10645v3.pdf","comment":"IEEE FLLM 2024"},{"id":"http://arxiv.org/abs/2409.11365v2","updated":"2024-10-09T06:39:28Z","published":"2024-09-17T17:14:41Z","title":"CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration","summary":"  The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks.\n","authors":["Jiahui Gao","Renjie Pi","Tianyang Han","Han Wu","Lanqing Hong","Lingpeng Kong","Xin Jiang","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2409.11365v2.pdf","comment":"10 pages, COLM-2024"},{"id":"http://arxiv.org/abs/2410.06577v1","updated":"2024-10-09T06:22:36Z","published":"2024-10-09T06:22:36Z","title":"Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient\n  Attentions","summary":"  Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints\nwill be available soon.\n","authors":["Zhihao He","Hang Yu","Zi Gong","Shizhan Liu","Jianguo Li","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2410.06577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07471v2","updated":"2024-10-09T06:21:47Z","published":"2024-08-14T11:29:47Z","title":"Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization","summary":"  Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework for\nBridging and Modeling Correlations in pairwise data, named BMC. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nthrough targeted modifications, synthesizing a pseudo-winning response by\nimproving the losing response with the winning response as a reference.\nSecondly, we identify that DPO alone is insufficient to model these\ncorrelations and capture nuanced variations. Therefore, we propose learning\ntoken-level correlations by dynamically leveraging the policy model's\nconfidence during training. Comprehensive experiments on QA, math, and\ninstruction-following tasks demonstrate the effectiveness of our approach,\nsignificantly surpassing competitive baselines, including DPO. Additionally,\nour in-depth quantitative analysis reveals the reasons behind our method's\nsuperior performance over DPO and showcases its versatility to other DPO\nvariants. We release our repository at https://github.com/YJiangcm/BMC.\n","authors":["Yuxin Jiang","Bo Huang","Yufei Wang","Xingshan Zeng","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.07471v2.pdf","comment":"19 pages, 8 figures, 10 tables, working in progress"},{"id":"http://arxiv.org/abs/2405.06001v3","updated":"2024-10-09T06:09:41Z","published":"2024-05-09T11:49:05Z","title":"LLMC: Benchmarking Large Language Model Quantization with a Versatile\n  Compression Toolkit","summary":"  Recent advancements in large language models (LLMs) are propelling us toward\nartificial general intelligence with their remarkable emergent abilities and\nreasoning capabilities. However, the substantial computational and memory\nrequirements limit the widespread adoption. Quantization, a key compression\ntechnique, can effectively mitigate these demands by compressing and\naccelerating LLMs, albeit with potential risks to accuracy. Numerous studies\nhave aimed to minimize the accuracy loss associated with quantization. However,\ntheir quantization configurations vary from each other and cannot be fairly\ncompared. In this paper, we present LLMC, a plug-and-play compression toolkit,\nto fairly and systematically explore the impact of quantization. LLMC\nintegrates dozens of algorithms, models, and hardwares, offering high\nextensibility from integer to floating-point quantization, from LLM to\nvision-language (VLM) model, from fixed-bit to mixed precision, and from\nquantization to sparsification. Powered by this versatile toolkit, our\nbenchmark covers three key aspects: calibration data, algorithms (three\nstrategies), and data formats, providing novel insights and detailed analyses\nfor further research and practical guidance for users. Our toolkit is available\nat https://github.com/ModelTC/llmc.\n","authors":["Ruihao Gong","Yang Yong","Shiqiao Gu","Yushi Huang","Chengtao Lv","Yunchen Zhang","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2405.06001v3.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.06566v1","updated":"2024-10-09T06:00:05Z","published":"2024-10-09T06:00:05Z","title":"Detecting Bias and Enhancing Diagnostic Accuracy in Large Language\n  Models for Healthcare","summary":"  Biased AI-generated medical advice and misdiagnoses can jeopardize patient\nsafety, making the integrity of AI in healthcare more critical than ever. As\nLarge Language Models (LLMs) take on a growing role in medical decision-making,\naddressing their biases and enhancing their accuracy is key to delivering safe,\nreliable care. This study addresses these challenges head-on by introducing new\nresources designed to promote ethical and precise AI in healthcare. We present\ntwo datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate\nand mitigate biases in health-related LLM outputs, and DiseaseMatcher, with\n32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing\nsymptom-based diagnostic accuracy. Using these datasets, we developed the\nEthiClinician, a fine-tuned model built on the ChatDoctor framework, which\noutperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing\nand correcting hidden biases in existing models for healthcare, our work sets a\nnew benchmark for safer, more reliable patient outcomes.\n","authors":["Pardis Sadat Zahraei","Zahra Shakeri"],"pdf_url":"https://arxiv.org/pdf/2410.06566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11470v2","updated":"2024-10-09T05:59:07Z","published":"2024-07-16T08:08:48Z","title":"Beyond Correctness: Benchmarking Multi-dimensional Code Generation for\n  Large Language Models","summary":"  In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\ncurrent benchmarks primarily assess the accuracy of LLM-generated code, while\nneglecting other critical dimensions that also significantly impact code\nquality in real-world development. Moreover, relying exclusively on correctness\nas the guiding metric renders LLMs susceptible to data contamination.\nTherefore, this paper proposes the RACE benchmark, which comprehensively\nevaluates the quality of code generated by LLMs across 4 dimensions:\nReadability, mAintainability, Correctness, and Efficiency. Specifically,\nconsidering the demand-dependent nature of dimensions beyond correctness, we\ndesign various types of user requirements for each dimension to assess the\nmodel's ability to generate correct code that also meets user demands. We\nanalyze 28 representative LLMs based on RACE and find that: 1) current\ncorrectness-centric benchmarks fail to capture the multifaceted requirements of\ncode in real-world scenarios, while RACE provides a comprehensive evaluation\nthat reveals the defects of LLMs across multiple dimensions; 2) the RACE\nbenchmark serves as an effective tool for resisting the risk of data\ncontamination; 3) even the most advanced code LLMs still encounter significant\nchallenges in customized requirements involving complex instructions; 4) most\nLLMs exhibit an inherent preference for specific coding style. These findings\nhighlight the need for a multidimensional evaluation of code LLMs, emphasizing\nmetrics beyond correctness for real-world applications. Future efforts should\naim to develop novel learning algorithms to enhance code generation under\nvaried constraints and improve coverage and usability for diverse user needs.\n","authors":["Jiasheng Zheng","Boxi Cao","Zhengzhao Ma","Ruotong Pan","Hongyu Lin","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2407.11470v2.pdf","comment":"We release benchmark at https://github.com/jszheng21/RACE and\n  leaderboard at https://huggingface.co/spaces/jszheng/RACE_leaderboard"},{"id":"http://arxiv.org/abs/2406.14282v2","updated":"2024-10-09T05:56:07Z","published":"2024-06-20T13:07:38Z","title":"Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs","summary":"  Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.\n","authors":["Junjie Wang","Mingyang Chen","Binbin Hu","Dan Yang","Ziqi Liu","Yue Shen","Peng Wei","Zhiqiang Zhang","Jinjie Gu","Jun Zhou","Jeff Z. Pan","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14282v2.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.06555v1","updated":"2024-10-09T05:17:38Z","published":"2024-10-09T05:17:38Z","title":"ING-VP: MLLMs cannot Play Easy Vision-based Games Yet","summary":"  As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps://github.com/Thisisus7/ING-VP.git.\n","authors":["Haoran Zhang","Hangyu Guo","Shuyue Guo","Meng Cao","Wenhao Huang","Jiaheng Liu","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06555v1.pdf","comment":"49 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.06554v1","updated":"2024-10-09T05:17:08Z","published":"2024-10-09T05:17:08Z","title":"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models","summary":"  Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at\n[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).\n","authors":["Yanjun Chen","Dawei Zhu","Yirong Sun","Xinghao Chen","Wei Zhang","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06554v1.pdf","comment":"10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06550v1","updated":"2024-10-09T05:15:13Z","published":"2024-10-09T05:15:13Z","title":"Investigating Cost-Efficiency of LLM-Generated Training Data for\n  Conversational Semantic Frame Analysis","summary":"  Recent studies have demonstrated that few-shot learning allows LLMs to\ngenerate training data for supervised models at a low cost. However, the\nquality of LLM-generated data may not entirely match that of human-labeled\ndata. This raises a crucial question: how should one balance the trade-off\nbetween the higher quality but more expensive human data and the lower quality\nyet substantially cheaper LLM-generated data? In this paper, we synthesized\ntraining data for conversational semantic frame analysis using GPT-4 and\nexamined how to allocate budgets optimally to achieve the best performance. Our\nexperiments, conducted across various budget levels, reveal that optimal\ncost-efficiency is achieved by combining both human and LLM-generated data\nacross a wide range of budget levels. Notably, as the budget decreases, a\nhigher proportion of LLM-generated data becomes more preferable.\n","authors":["Shiho Matta","Yin Jou Huang","Fei Cheng","Hirokazu Kiyomaru","Yugo Murawaki"],"pdf_url":"https://arxiv.org/pdf/2410.06550v1.pdf","comment":"12 pages including 4 pages of references and appendix. 7 figures"},{"id":"http://arxiv.org/abs/2410.06547v1","updated":"2024-10-09T04:53:38Z","published":"2024-10-09T04:53:38Z","title":"TuringQ: Benchmarking AI Comprehension in Theory of Computation","summary":"  We present TuringQ, the first benchmark designed to evaluate the reasoning\ncapabilities of large language models (LLMs) in the theory of computation.\nTuringQ consists of 4,006 undergraduate and graduate-level question-answer\npairs, categorized into four difficulty levels and covering seven core\ntheoretical areas. We evaluate several open-source LLMs, as well as GPT-4,\nusing Chain of Thought prompting and expert human assessment. Additionally, we\npropose an automated LLM-based evaluation system that demonstrates competitive\naccuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on\nTuringQ shows measurable improvements in reasoning ability and out-of-domain\ntasks such as algebra. TuringQ serves as both a benchmark and a resource for\nenhancing LLM performance in complex computational reasoning tasks. Our\nanalysis offers insights into LLM capabilities and advances in AI comprehension\nof theoretical computer science.\n","authors":["Pardis Sadat Zahraei","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2410.06547v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.17274v2","updated":"2024-10-09T04:40:24Z","published":"2024-06-25T04:41:17Z","title":"Can We Trust the Performance Evaluation of Uncertainty Estimation\n  Methods in Text Summarization?","summary":"  Text summarization, a key natural language generation (NLG) task, is vital in\nvarious domains. However, the high cost of inaccurate summaries in\nrisk-critical applications, particularly those involving human-in-the-loop\ndecision-making, raises concerns about the reliability of uncertainty\nestimation on text summarization (UE-TS) evaluation methods. This concern stems\nfrom the dependency of uncertainty model metrics on diverse and potentially\nconflicting NLG metrics. To address this issue, we introduce a comprehensive\nUE-TS benchmark incorporating 31 NLG metrics across four dimensions. The\nbenchmark evaluates the uncertainty estimation capabilities of two large\nlanguage models and one pre-trained language model on three datasets, with\nhuman-annotation analysis incorporated where applicable. We also assess the\nperformance of 14 common uncertainty estimation methods within this benchmark.\nOur findings emphasize the importance of considering multiple uncorrelated NLG\nmetrics and diverse uncertainty estimation methods to ensure reliable and\nefficient evaluation of UE-TS techniques. Our code and data are available\nhttps://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.\n","authors":["Jianfeng He","Runing Yang","Linlin Yu","Changbin Li","Ruoxi Jia","Feng Chen","Ming Jin","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2406.17274v2.pdf","comment":"62 pages, 41 figures, 11 tables"},{"id":"http://arxiv.org/abs/2409.15084v2","updated":"2024-10-09T04:37:29Z","published":"2024-09-20T14:25:08Z","title":"Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist\n  with Tertiary Memory","summary":"  Mental health issues, particularly depressive disorders, present significant\nchallenges in contemporary society, necessitating the development of effective\nautomated diagnostic methods. This paper introduces the Agent Mental Clinic\n(AMC), a self-improving conversational agent system designed to enhance\ndepression diagnosis through simulated dialogues between patient and\npsychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we\ndesign a psychiatrist agent consisting of a tertiary memory structure, a\ndialogue control and reflect plugin that acts as ``supervisor'' and a memory\nsampling module, fully leveraging the skills reflected by the psychiatrist\nagent, achieving great accuracy on depression risk and suicide risk diagnosis\nvia conversation. Experiment results on datasets collected in real-life\nscenarios demonstrate that the system, simulating the procedure of training\npsychiatrists, can be a promising optimization method for aligning LLMs with\nreal-life distribution in specific domains without modifying the weights of\nLLMs, even when only a few representative labeled cases are available.\n","authors":["Kunyao Lan","Bingrui Jin","Zichen Zhu","Siyuan Chen","Shu Zhang","Kenny Q. Zhu","Mengyue Wu"],"pdf_url":"https://arxiv.org/pdf/2409.15084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06541v1","updated":"2024-10-09T04:35:22Z","published":"2024-10-09T04:35:22Z","title":"Chip-Tuning: Classify Before Language Models Say","summary":"  The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility.\n","authors":["Fangwei Zhu","Dian Li","Jiajun Huang","Gang Liu","Hui Wang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2410.06541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19916v2","updated":"2024-10-09T04:26:37Z","published":"2024-09-30T03:37:10Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming","summary":"  Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.\n","authors":["Tianyang Wang","Ziqian Bi","Keyu Chen","Jiawei Xu","Qian Niu","Junyu Liu","Benji Peng","Ming Li","Sen Zhang","Xuanhe Pan","Jinlang Wang","Pohsun Feng","Caitlyn Heqi Yin","Yizhu Wen","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2409.19916v2.pdf","comment":"47pages"},{"id":"http://arxiv.org/abs/2410.06524v1","updated":"2024-10-09T03:53:26Z","published":"2024-10-09T03:53:26Z","title":"Do great minds think alike? Investigating Human-AI Complementarity in\n  Question Answering with CAIMIRA","summary":"  Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.\n","authors":["Maharshi Gor","Hal Daumé III","Tianyi Zhou","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2410.06524v1.pdf","comment":"To appear at EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2403.09559v3","updated":"2024-10-09T03:51:45Z","published":"2024-03-14T16:47:25Z","title":"Less is More: High-value Data Selection for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building large vision language\nmodels~(LVLMs), which can greatly improve the task generalization and solving\ncapabilities by learning a mixture of instruction data from diverse visual\ntasks. Previous work mostly collects multiple existing visual instruction\ndatasets via heuristic ways for training (even more than a million\ninstructions), which may introduce data redundancy and enlarge the training\ncost. To investigate this issue, we conduct a series of empirical studies,\nwhich reveal a significant redundancy within the visual instruction datasets,\nand show that greatly reducing the amount of instructions from several tasks\neven do not affect the performance. Based on the findings, we propose a\nhigh-value data selection approach TIVE, to eliminate redundancy within the\nvisual instruction data and reduce the training cost. In TIVE, we first\nestimate the instance influence score on its corresponding task, and the task\ndifficulty score, based on the gradient-based influence functions. Then, we\nleverage the two kinds of scores to determine the task proportion within the\nselected visual instruction subset, and select high-value instances for each\ntask, respectively. Experiments on various LVLMs show that our approach using\nonly about 15% data can achieve comparable average performance to the full-data\nfine-tuned model across eight benchmarks, even surpassing it on four of the\nbenchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.02952v2","updated":"2024-10-09T03:46:46Z","published":"2024-10-03T19:52:37Z","title":"Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications","summary":"  We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.\n","authors":["Oren Sultan","Alex Khasin","Guy Shiran","Asnat Greenstein-Messica","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2410.02952v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.05785v4","updated":"2024-10-09T03:43:34Z","published":"2024-02-08T16:23:29Z","title":"Limits of Transformer Language Models on Learning to Compose Algorithms","summary":"  We analyze the capabilities of Transformer language models in learning\ncompositional discrete tasks. To this end, we evaluate training LLaMA models\nand prompting GPT-4 and Gemini on four tasks demanding to learn a composition\nof several discrete sub-tasks. On both training LLaMA models from scratch and\nprompting on GPT-4 and Gemini, we measure how well these models can reuse\nprimitives observable in the sub-tasks to learn the composition task. Our\nresults indicate that compositional learning in state-of-the-art Transformer\nlanguage models is highly sample inefficient: LLaMA requires more data samples\nthan relearning all sub-tasks from scratch to learn the compositional task;\nin-context prompting with few samples is unreliable and fails at executing the\nsub-tasks or correcting the errors in multi-round code generation. Further, by\nleveraging complexity theory, we support these findings with a theoretical\nanalysis focused on the sample inefficiency of gradient descent in memorizing\nfeedforward models.\n","authors":["Jonathan Thomm","Aleksandar Terzic","Giacomo Camposampiero","Michael Hersche","Bernhard Schölkopf","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2402.05785v4.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06520v1","updated":"2024-10-09T03:42:40Z","published":"2024-10-09T03:42:40Z","title":"A Novel LLM-based Two-stage Summarization Approach for Long Dialogues","summary":"  Long document summarization poses a significant challenge in natural language\nprocessing due to input lengths that exceed the capacity of most\nstate-of-the-art pre-trained language models. This study proposes a\nhierarchical framework that segments and condenses information from long\ndocuments, subsequently fine-tuning the processed text with an abstractive\nsummarization model. Unsupervised topic segmentation methods identify\nsemantically appropriate breakpoints. The condensation stage utilizes an\nunsupervised generation model to generate condensed data, and our current\nexperiments employ ChatGPT(v3.5). The summarization stage fine-tunes the\nabstractive summarization model on the condensed data to generate the final\nresults. This framework enables long documents to be processed on models even\nwhen the document length exceeds the model's maximum input size. The exclusion\nof the entire document from the summarization model reduces the time and\ncomputational resources required for training, making the framework suitable\nfor contexts with constrained local computational resources.\n","authors":["Yuan-Jhe Yin","Bo-Yu Chen","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10900v2","updated":"2024-10-09T03:42:22Z","published":"2024-06-16T11:44:43Z","title":"AutoHallusion: Automatic Generation of Hallucination Benchmarks for\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) are prone to hallucinations, where\ncertain contextual cues in an image can trigger the language module to produce\noverconfident and incorrect reasoning about abnormal or hypothetical objects.\nWhile some benchmarks have been developed to investigate LVLM hallucinations,\nthey often rely on hand-crafted corner cases whose failure patterns may not\ngeneralize well. Additionally, fine-tuning on these examples could undermine\ntheir validity. To address this, we aim to scale up the number of cases through\nan automated approach, reducing human bias in crafting such corner cases. This\nmotivates the development of AutoHallusion, the first automated benchmark\ngeneration approach that employs several key strategies to create a diverse\nrange of hallucination examples. Our generated visual-question pairs pose\nsignificant challenges to LVLMs, requiring them to overcome contextual biases\nand distractions to arrive at correct answers. AutoHallusion enables us to\ncreate new benchmarks at the minimum cost and thus overcomes the fragility of\nhand-crafted benchmarks. It also reveals common failure patterns and reasons,\nproviding key insights to detect, avoid, or control hallucinations.\nComprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro\nVision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of\nhallucination induction on synthetic and real-world datasets of AutoHallusion,\npaving the way for a long battle against hallucinations. The codebase and data\ncan be accessed at https://github.com/wuxiyang1996/AutoHallusion.\n","authors":["Xiyang Wu","Tianrui Guan","Dianqi Li","Shuaiyi Huang","Xiaoyu Liu","Xijun Wang","Ruiqi Xian","Abhinav Shrivastava","Furong Huang","Jordan Lee Boyd-Graber","Tianyi Zhou","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07175v3","updated":"2024-10-09T03:41:43Z","published":"2024-03-11T21:33:05Z","title":"Rebuilding ROME : Resolving Model Collapse during Sequential Model\n  Editing","summary":"  Recent work using Rank-One Model Editing (ROME), a popular model editing\nmethod, has shown that there are certain facts that the algorithm is unable to\nedit without breaking the model. Such edits have previously been called\ndisabling edits. These disabling edits cause immediate model collapse and\nlimits the use of ROME for sequential editing. In this paper, we show that\ndisabling edits are an artifact of irregularities in the implementation of\nROME. With this paper, we provide a more stable implementation ROME, which we\ncall r-ROME and show that model collapse is no longer observed when making\nlarge scale sequential edits with r-ROME, while further improving\ngeneralization and locality of model editing compared to the original\nimplementation of ROME. We also provide a detailed mathematical explanation of\nthe reason behind disabling edits.\n","authors":["Akshat Gupta","Sidharth Baskaran","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.07175v3.pdf","comment":"EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.06519v1","updated":"2024-10-09T03:40:22Z","published":"2024-10-09T03:40:22Z","title":"SEGMENT+: Long Text Processing with Short-Context Language Models","summary":"  There is a growing interest in expanding the input capacity of language\nmodels (LMs) across various domains. However, simply increasing the context\nwindow does not guarantee robust performance across diverse long-input\nprocessing tasks, such as understanding extensive documents and extracting\ndetailed information from lengthy and noisy data. In response, we introduce\nSEGMENT+, a general framework that enables LMs to handle extended inputs within\nlimited context windows efficiently. SEGMENT+ utilizes structured notes and a\nfiltering module to manage information flow, resulting in a system that is both\ncontrollable and interpretable. Our extensive experiments across various model\nsizes, focusing on long-document question-answering and Needle-in-a-Haystack\ntasks, demonstrate the effectiveness of SEGMENT+ in improving performance.\n","authors":["Wei Shi","Shuang Li","Kerun Yu","Jinglei Chen","Zujie Liang","Xinhui Wu","Yuxi Qian","Feng Wei","Bo Zheng","Jiaqing Liang","Jiangjie Chen","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.06519v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.01506v2","updated":"2024-10-09T03:39:11Z","published":"2024-06-03T16:34:01Z","title":"The Geometry of Categorical and Hierarchical Concepts in Large Language\n  Models","summary":"  The linear representation hypothesis is the informal idea that semantic\nconcepts are encoded as linear directions in the representation spaces of large\nlanguage models (LLMs). Previous work has shown how to make this notion precise\nfor representing binary concepts that have natural contrasts (e.g., {male,\nfemale}) as directions in representation space. However, many natural concepts\ndo not have natural contrasts (e.g., whether the output is about an animal). In\nthis work, we show how to extend the formalization of the linear representation\nhypothesis to represent features (e.g., is_animal) as vectors. This allows us\nto immediately formalize the representation of categorical concepts as\npolytopes in the representation space. Further, we use the formalization to\nprove a relationship between the hierarchical structure of concepts and the\ngeometry of their representations. We validate these theoretical results on the\nGemma and LLaMA-3 large language models, estimating representations for 900+\nhierarchically related concepts using data from WordNet.\n","authors":["Kiho Park","Yo Joong Choe","Yibo Jiang","Victor Veitch"],"pdf_url":"https://arxiv.org/pdf/2406.01506v2.pdf","comment":"Best Paper Award at the ICML 2024 Workshop on Mechanistic\n  Interpretability. Code is available at\n  https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations"},{"id":"http://arxiv.org/abs/2403.14236v5","updated":"2024-10-09T03:37:30Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v5.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.06511v1","updated":"2024-10-09T03:26:11Z","published":"2024-10-09T03:26:11Z","title":"TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training","summary":"  The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.\n","authors":["Wanchao Liang","Tianyu Liu","Less Wright","Will Constable","Andrew Gu","Chien-Chin Huang","Iris Zhang","Wei Feng","Howard Huang","Junjie Wang","Sanket Purandare","Gokul Nadathur","Stratos Idreos"],"pdf_url":"https://arxiv.org/pdf/2410.06511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06508v1","updated":"2024-10-09T03:20:02Z","published":"2024-10-09T03:20:02Z","title":"Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge\n  with Curriculum Preference Learning","summary":"  Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique\nfor enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO\nhave enabled LLMs to distill high-quality behaviors from MCTS, improving their\nreasoning performance. However, existing distillation methods underutilize the\nrich trajectory information generated by MCTS, limiting the potential for\nimprovements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel\npairwise training framework that enables LLMs to self-improve through MCTS\nbehavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via\ntwo key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from\nchild nodes sharing the same parent in the search tree, providing step-level\ninformation for more effective MCTS behavior distillation. (2) AlphaLLM-CPL\nintroduces curriculum preference learning, dynamically adjusting the training\nsequence of trajectory pairs in each offline training epoch to prioritize\ncritical learning steps and mitigate overfitting. Experimental results on\nmathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly\noutperforms previous MCTS behavior distillation methods, substantially boosting\nthe reasoning capabilities of LLMs.\n","authors":["Xiyao Wang","Linfeng Song","Ye Tian","Dian Yu","Baolin Peng","Haitao Mi","Furong Huang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.06508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06496v1","updated":"2024-10-09T02:49:56Z","published":"2024-10-09T02:49:56Z","title":"On the Similarity of Circuits across Languages: a Case Study on the\n  Subject-verb Agreement Task","summary":"  Several algorithms implemented by language models have recently been\nsuccessfully reversed-engineered. However, these findings have been\nconcentrated on specific tasks and models, leaving it unclear how universal\ncircuits are across different settings. In this paper, we study the circuits\nimplemented by Gemma 2B for solving the subject-verb agreement task across two\ndifferent languages, English and Spanish. We discover that both circuits are\nhighly consistent, being mainly driven by a particular attention head writing a\n`subject number' signal to the last residual stream, which is read by a small\nset of neurons in the final MLPs. Notably, this subject number signal is\nrepresented as a direction in the residual stream space, and is\nlanguage-independent. We demonstrate that this direction has a causal effect on\nthe model predictions, effectively flipping the Spanish predicted verb number\nby intervening with the direction found in English. Finally, we present\nevidence of similar behavior in other models within the Gemma 1 and Gemma 2\nfamilies.\n","authors":["Javier Ferrando","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2410.06496v1.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2305.19118v4","updated":"2024-10-09T02:41:21Z","published":"2023-05-30T15:25:45Z","title":"Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate","summary":"  Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Code is available at\nhttps://github.com/Skytliang/Multi-Agents-Debate.\n","authors":["Tian Liang","Zhiwei He","Wenxiang Jiao","Xing Wang","Yan Wang","Rui Wang","Yujiu Yang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2305.19118v4.pdf","comment":"EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2410.06479v1","updated":"2024-10-09T02:14:39Z","published":"2024-10-09T02:14:39Z","title":"LLM Compression with Neural Architecture Search","summary":"  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup.\n","authors":["Rhea Sanjay Sukthanker","Benedikt Staffler","Frank Hutter","Aaron Klein"],"pdf_url":"https://arxiv.org/pdf/2410.06479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07840v3","updated":"2024-10-09T02:03:26Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose Decompose and Compare Consistency (DeCC)\nfor reliability measurement. By comparing the consistency between the direct\nanswer generated using the VLM's internal reasoning process, and the indirect\nanswers obtained by decomposing the question into sub-questions and reasoning\nover the sub-answers produced by the VLM, DeCC measures the reliability of\nVLM's direct answer. Experiments across six vision-language tasks with three\nVLMs show DeCC's reliability estimation achieves better correlation with task\naccuracy compared to the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.06458v1","updated":"2024-10-09T01:25:10Z","published":"2024-10-09T01:25:10Z","title":"LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for\n  Enhanced Following of Instructions with Multiple Constraints","summary":"  Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.\n","authors":["Thomas Palmeira Ferraz","Kartik Mehta","Yu-Hsiang Lin","Haw-Shiuan Chang","Shereen Oraby","Sijia Liu","Vivek Subramanian","Tagyoung Chung","Mohit Bansal","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2410.06458v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.10443v4","updated":"2024-10-09T01:12:19Z","published":"2024-05-16T21:07:42Z","title":"Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation","summary":"  Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.\n","authors":["Matthew Raffel","Victor Agostinelli","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10443v4.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06441v1","updated":"2024-10-09T00:49:08Z","published":"2024-10-09T00:49:08Z","title":"Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and\n  Performance of SGD for Fine-Tuning Language Models","summary":"  Fine-tuning language models (LMs) with the Adam optimizer often demands\nexcessive memory, limiting accessibility. The \"in-place\" version of Stochastic\nGradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO)\nhave been proposed to address this. However, IP-SGD still requires substantial\nmemory, and MeZO suffers from slow convergence and degraded final performance\ndue to its zeroth-order nature. This paper introduces Addax, a novel method\nthat improves both memory efficiency and performance of IP-SGD by integrating\nit with MeZO. Specifically, Addax computes zeroth- or first-order gradients of\ndata points in the minibatch based on their memory consumption, combining these\ngradient estimates to update directions. By computing zeroth-order gradients\nfor data points that require more memory and first-order gradients for others,\nAddax overcomes the slow convergence of MeZO and the excessive memory\nrequirement of IP-SGD. Additionally, the zeroth-order gradient acts as a\nregularizer for the first-order gradient, further enhancing the model's final\nperformance. Theoretically, we establish the convergence of Addax under mild\nassumptions, demonstrating faster convergence and less restrictive\nhyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks\nshow that Addax consistently outperforms MeZO regarding accuracy and\nconvergence speed while having a comparable memory footprint. When fine-tuning\nOPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1\nscore by 14% and runs 15x faster while using memory similar to MeZO. In our\nexperiments on the larger OPT-30B model, on average, Addax outperforms MeZO in\nterms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU.\nMoreover, Addax surpasses the performance of standard fine-tuning approaches,\nsuch as IP-SGD and Adam, in most tasks with significantly less memory\nrequirement.\n","authors":["Zeman Li","Xinwei Zhang","Peilin Zhong","Yuan Deng","Meisam Razaviyayn","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2410.06441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12327v4","updated":"2024-10-09T00:22:46Z","published":"2024-07-17T05:53:20Z","title":"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models","summary":"  Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\n\\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.\n","authors":["Ayush Kaushal","Tejas Vaidhya","Arnab Kumar Mondal","Tejas Pandey","Aaryan Bhagat","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2407.12327v4.pdf","comment":"42 pages, 21 figures, and 13 tables"}]},"2024-10-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.06428v1","updated":"2024-10-08T23:49:31Z","published":"2024-10-08T23:49:31Z","title":"Stress Detection on Code-Mixed Texts in Dravidian Languages using\n  Machine Learning","summary":"  Stress is a common feeling in daily life, but it can affect mental well-being\nin some situations, the development of robust detection models is imperative.\nThis study introduces a methodical approach to the stress identification in\ncode-mixed texts for Dravidian languages. The challenge encompassed two\ndatasets, targeting Tamil and Telugu languages respectively. This proposal\nunderscores the importance of using uncleaned text as a benchmark to refine\nfuture classification methodologies, incorporating diverse preprocessing\ntechniques. Random Forest algorithm was used, featuring three textual\nrepresentations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams\nof characters. The approach achieved a good performance for both linguistic\ncategories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,\noverpassing results achieved with different complex techniques such as FastText\nand Transformer models. The results underscore the value of uncleaned data for\nmental state detection and the challenges classifying code-mixed texts for\nstress, indicating the potential for improved performance through cleaning\ndata, other preprocessing techniques, or more complex models.\n","authors":["L. Ramos","M. Shahiki-Tash","Z. Ahani","A. Eponon","O. Kolesnikova","H. Calvo"],"pdf_url":"https://arxiv.org/pdf/2410.06428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06427v1","updated":"2024-10-08T23:46:56Z","published":"2024-10-08T23:46:56Z","title":"NLP Case Study on Predicting the Before and After of the Ukraine-Russia\n  and Hamas-Israel Conflicts","summary":"  We propose a method to predict toxicity and other textual attributes through\nthe use of natural language processing (NLP) techniques for two recent events:\nthe Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis\nfor exploration in future conflicts with hopes to mitigate risk through the\nanalysis of social media before and after a conflict begins. Our work compiles\nseveral datasets from Twitter and Reddit for both conflicts in a before and\nafter separation with an aim of predicting a future state of social media for\navoidance. More specifically, we show that: (1) there is a noticeable\ndifference in social media discussion leading up to and following a conflict\nand (2) social media discourse on platforms like Twitter and Reddit is useful\nin identifying future conflicts before they arise. Our results show that\nthrough the use of advanced NLP techniques (both supervised and unsupervised)\ntoxicity and other attributes about language before and after a conflict is\npredictable with a low error of nearly 1.2 percent for both conflicts.\n","authors":["Jordan Miner","John E. Ortega"],"pdf_url":"https://arxiv.org/pdf/2410.06427v1.pdf","comment":"The clusters created using topic modeling can be viewed at\n  https://naturallang.com/conflict/conflict.html"},{"id":"http://arxiv.org/abs/2410.06420v1","updated":"2024-10-08T23:14:24Z","published":"2024-10-08T23:14:24Z","title":"ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language\n  Models in Hospital Environments","summary":"  The global shortage of healthcare workers has demanded the development of\nsmart healthcare assistants, which can help monitor and alert healthcare\nworkers when necessary. We examine the healthcare knowledge of existing Large\nVision Language Models (LVLMs) via the Visual Question Answering (VQA) task in\nhospital settings through expert annotated open-ended questions. We introduce\nthe Emergency Room Visual Question Answering (ERVQA) dataset, consisting of\n<image, question, answer> triplets covering diverse emergency room scenarios, a\nseminal benchmark for LVLMs. By developing a detailed error taxonomy and\nanalyzing answer trends, we reveal the nuanced nature of the task. We benchmark\nstate-of-the-art open-source and closed LVLMs using traditional and adapted VQA\nmetrics: Entailment Score and CLIPScore Confidence. Analyzing errors across\nmodels, we infer trends based on properties like decoder type, model size, and\nin-context examples. Our findings suggest the ERVQA dataset presents a highly\ncomplex task, highlighting the need for specialized, domain-specific solutions.\n","authors":["Sourjyadip Ray","Kushal Gupta","Soumi Kundu","Payal Arvind Kasat","Somak Aditya","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.06420v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.18988v3","updated":"2024-10-08T22:18:59Z","published":"2024-04-29T17:36:58Z","title":"Markovian Transformers for Informative Language Modeling","summary":"  Chain-of-Thought (CoT) reasoning holds great promise for explaining the\noutputs of language models, but recent studies have highlighted significant\nchallenges in its practical application for interpretability. We propose to\naddress this issue via two key components: a technique to factor next-token\nprediction through intermediate CoT text, ensuring the CoT is causally\nload-bearing, and a reinforcement learning approach to train CoT to predict\nfuture tokens independently of other context. This results in \"Markovian\"\nlanguage models, where CoT serves as a fixed-size state for future token\nprediction. Our approach optimizes for \"informativeness\" -- the improvement in\nnext-token predictions using a trained CoT compared to a baseline. We\ndemonstrate our method's effectiveness using Proximal Policy Optimization (PPO)\non arithmetic problems and achieve an 11% performance boost on the GSM8K\nbenchmark using Mistral 7B Inst V2. The increased sensitivity of model\nperformance to CoT perturbations provides strong evidence of CoT reliance. This\nwork advances the development of more transparent and interpretable language\nmodels, potentially enabling their extension to arbitrarily long contexts and\nenhancing AI reasoning capabilities across various domains.\n","authors":["Scott Viteri","Max Lamparth","Peter Chatain","Clark Barrett"],"pdf_url":"https://arxiv.org/pdf/2404.18988v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.06396v1","updated":"2024-10-08T21:59:31Z","published":"2024-10-08T21:59:31Z","title":"MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks","summary":"  Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce MLissard, a\nmultilingual benchmark designed to evaluate models' abilities to process and\ngenerate texts of varied lengths and offers a mechanism for controlling\nsequence complexity.\n  Our evaluation of open-source and proprietary models show a consistent\ndecline in performance across all models and languages as the complexity of the\nsequence increases. Surprisingly, the use of in-context examples in languages\nother than English helps increase extrapolation performance significantly. The\ndatasets and code are available at https://github.com/unicamp-dl/Lissard\n","authors":["Mirelle Bueno","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2410.06396v1.pdf","comment":"GenBench Workshop by EMNLP 2024: Camera-ready version"},{"id":"http://arxiv.org/abs/2410.06392v1","updated":"2024-10-08T21:53:07Z","published":"2024-10-08T21:53:07Z","title":"Counterfactual Causal Inference in Natural Language with Large Language\n  Models","summary":"  Causal structure discovery methods are commonly applied to structured data\nwhere the causal variables are known and where statistical testing can be used\nto assess the causal relationships. By contrast, recovering a causal structure\nfrom unstructured natural language data such as news articles contains numerous\nchallenges due to the absence of known variables or counterfactual data to\nestimate the causal links. Large Language Models (LLMs) have shown promising\nresults in this direction but also exhibit limitations. This work investigates\nLLM's abilities to build causal graphs from text documents and perform\ncounterfactual causal inference. We propose an end-to-end causal structure\ndiscovery and causal inference method from natural language: we first use an\nLLM to extract the instantiated causal variables from text data and build a\ncausal graph. We merge causal graphs from multiple data sources to represent\nthe most exhaustive set of causes possible. We then conduct counterfactual\ninference on the estimated graph. The causal graph conditioning allows\nreduction of LLM biases and better represents the causal estimands. We use our\nmethod to show that the limitations of LLMs in counterfactual causal reasoning\ncome from prediction errors and propose directions to mitigate them. We\ndemonstrate the applicability of our method on real-world news articles.\n","authors":["Gaël Gendron","Jože M. Rožanec","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2410.06392v1.pdf","comment":"22 pages, 10 pages for the main paper, 12 pages for the references\n  and appendix, 5 figures"},{"id":"http://arxiv.org/abs/2410.06384v1","updated":"2024-10-08T21:31:42Z","published":"2024-10-08T21:31:42Z","title":"Validation of the Scientific Literature via Chemputation Augmented by\n  Large Language Models","summary":"  Chemputation is the process of programming chemical robots to do experiments\nusing a universal symbolic language, but the literature can be error prone and\nhard to read due to ambiguities. Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains, including natural language\nprocessing, robotic control, and more recently, chemistry. Despite significant\nadvancements in standardizing the reporting and collection of synthetic\nchemistry data, the automatic reproduction of reported syntheses remains a\nlabour-intensive task. In this work, we introduce an LLM-based chemical\nresearch agent workflow designed for the automatic validation of synthetic\nliterature procedures. Our workflow can autonomously extract synthetic\nprocedures and analytical data from extensive documents, translate these\nprocedures into universal XDL code, simulate the execution of the procedure in\na hardware-specific setup, and ultimately execute the procedure on an\nXDL-controlled robotic system for synthetic chemistry. This demonstrates the\npotential of LLM-based workflows for autonomous chemical synthesis with\nChemputers. Due to the abstraction of XDL this approach is safe, secure, and\nscalable since hallucinations will not be chemputable and the XDL can be both\nverified and encrypted. Unlike previous efforts, which either addressed only a\nlimited portion of the workflow, relied on inflexible hard-coded rules, or\nlacked validation in physical systems, our approach provides four realistic\nexamples of syntheses directly executed from synthetic literature. We\nanticipate that our workflow will significantly enhance automation in\nrobotically driven synthetic chemistry research, streamline data extraction,\nimprove the reproducibility, scalability, and safety of synthetic and\nexperimental chemistry.\n","authors":["Sebastian Pagel","Michael Jirasek","Leroy Cronin"],"pdf_url":"https://arxiv.org/pdf/2410.06384v1.pdf","comment":"22 pages, 7 figures, 34 references"},{"id":"http://arxiv.org/abs/2410.06370v1","updated":"2024-10-08T21:08:13Z","published":"2024-10-08T21:08:13Z","title":"HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting\n  Humanitarian Aid","summary":"  Humanitarian organizations can enhance their effectiveness by analyzing data\nto discover trends, gather aggregated insights, manage their security risks,\nsupport decision-making, and inform advocacy and funding proposals. However,\ndata about violent incidents with direct impact and relevance for humanitarian\naid operations is not readily available. An automatic data collection and\nNLP-backed classification framework aligned with humanitarian perspectives can\nhelp bridge this gap. In this paper, we present HumVI - a dataset comprising\nnews articles in three languages (English, French, Arabic) containing instances\nof different types of violent incidents categorized by the humanitarian sector\nthey impact, e.g., aid security, education, food security, health, and\nprotection. Reliable labels were obtained for the dataset by partnering with a\ndata-backed humanitarian organization, Insecurity Insight. We provide multiple\nbenchmarks for the dataset, employing various deep learning architectures and\ntechniques, including data augmentation and mask loss, to address different\ntask-related challenges, e.g., domain expansion. The dataset is publicly\navailable at https://github.com/dataminr-ai/humvi-dataset.\n","authors":["Hemank Lamba","Anton Abilov","Ke Zhang","Elizabeth M. Olson","Henry k. Dambanemuya","João c. Bárcia","David S. Batista","Christina Wille","Aoife Cahill","Joel Tetreault","Alex Jaimes"],"pdf_url":"https://arxiv.org/pdf/2410.06370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04617v2","updated":"2024-10-08T20:40:59Z","published":"2024-09-06T21:00:57Z","title":"Sparse Rewards Can Self-Train Dialogue Agents","summary":"  Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training\n","authors":["Barrett Martin Lattimer","Varun Gangal","Ryan McDonald","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.04617v2.pdf","comment":"Minor but nontrivial changes likely"},{"id":"http://arxiv.org/abs/2402.01920v2","updated":"2024-10-08T20:32:15Z","published":"2024-02-02T21:45:24Z","title":"Preference Poisoning Attacks on Reward Model Learning","summary":"  Learning reward models from pairwise comparisons is a fundamental component\nin a number of domains, including autonomous control, conversational agents,\nand recommendation systems, as part of a broad goal of aligning automated\ndecisions with user preferences. These approaches entail collecting preference\ninformation from people, with feedback often provided anonymously. Since\npreferences are subjective, there is no gold standard to compare against; yet,\nreliance of high-impact systems on preference learning creates a strong\nmotivation for malicious actors to skew data collected in this fashion to their\nends. We investigate the nature and extent of this vulnerability by considering\nan attacker who can flip a small subset of preference comparisons to either\npromote or demote a target outcome. We propose two classes of algorithmic\napproaches for these attacks: a gradient-based framework, and several variants\nof rank-by-distance methods. Next, we evaluate the efficacy of best attacks in\nboth these classes in successfully achieving malicious goals on datasets from\nthree domains: autonomous control, recommendation system, and textual\nprompt-response preference learning. We find that the best attacks are often\nhighly successful, achieving in the most extreme case 100\\% success rate with\nonly 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary\nsignificantly across domains. In addition, we observe that the simpler and more\nscalable rank-by-distance approaches are often competitive with, and on\noccasion significantly outperform, gradient-based methods. Finally, we show\nthat state-of-the-art defenses against other classes of poisoning attacks\nexhibit limited efficacy in our setting.\n","authors":["Junlin Wu","Jiongxiao Wang","Chaowei Xiao","Chenguang Wang","Ning Zhang","Yevgeniy Vorobeychik"],"pdf_url":"https://arxiv.org/pdf/2402.01920v2.pdf","comment":null}]}}